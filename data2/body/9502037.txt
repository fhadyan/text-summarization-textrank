  Recent years have seen a resurgence of interest in probabilistic techniques for automatic language analysis.
 In particular, there has arisen a distinct paradigm of processing on the basis of pre-analyzed data which has taken the name Data-Oriented Parsing.
   ``Data Oriented Parsing (DOP) is a model where no abstract rules, but language experiences in the form of an analyzed corpus, constitute the basis for language processing.''   There is not space here to present full justification for adopting such an approach or to detail the advantages that it offers.
 The main claim it makes is that effective language processing requires a consideration of both the structural and statistical aspects of language, whereas traditional competence grammars rely only on the former, and standard statistical techniques such as n-gram models only on the latter.
 DOP attempts to combine these two traditions and produce ``performance grammars'', which:   ``...
 should not only contain information on the structural possibilities of the general language system, but also on details of actual language use in a language community...''   This approach entails however that a corpus has first to be pre-analyzed (ie.
 hand-parsed), and the question immediately arises as to the formalism to be used for this.
 There is no lack of competing competence grammars available, but also no reason to expect that such grammars should be suited to a DOP approach, designed as they were to characterize the nature of linguistic competence rather than performance.
   The next section sets out some of the properties that we might require from such a ``performance grammar'' and offers a formalism which attempts to satisfy these requirements.
   Given that we are attempting to construct a formalism that will do justice to both the statistical and structural aspects of language, the features that we would wish to maximize will include the following:   1.
 The formalism should be easy to use with probabilistic processing techniques, ideally having a close correspondence to a simple probabilistic model such as a Markov process.
 2.
 The formalism should be fine-grained, ie.
 responsive to the behaviour of individual words (as n-gram models are).
 This suggests a radically lexicalist approach (cf.
 Karttunen, 1990) in which all rules are encoded in the lexicon, there being no phrase structure rules which do not introduce lexical items.
 3.
 It should be capable of capturing fully the linguistic intuitions of language users.
 In other words, using the formalism one should be able to characterize the structural regularities of language with at least the sophistication of modern competence grammars.
 4.
 As it is to be used with real data, the formalism should be able to characterize the wide range of syntactic structures found in actual language use, including those normally excluded by competence grammars as belonging to the ``periphery'' of the language or as being ``ungrammatical''.
 Ideally every interpretable utterance should have one and only one analysis for any interpretation of it.
 Considering the first of these points, namely a close relation to a simple probabilistic model, a good place to start the search might be with a right-branching finite-state grammar.
 In this class of grammars every rule has the form A   a B (A,B   {non-terminals}, a   {terminals}) and all trees have the simple structure :   Or: equivalent vertical alignment, henceforth to be used in this paper, on the right)   In probabilistic terms, a finite-state grammar corresponds to a first-order Markov process, where given a sequence of states Si, Sj,...
 drawn from a finite set of possible states {S0,..,Sn} the probability of a particular state occurring depends solely on the identity of the previous state.
 In the finite-state grammar each word is associated with a transition between two categories, in the tree above `a' with the transition A   B and so on.
 To calculate the probability that a string of words x1, x2, x3,...
 xn has the parse represented by the string of category-states S1, S2, S3,...Sn, we simply take the product of the probability of each transition: ie.
   In addition to satisfying our first criterion, a finite-state grammar also fulfills the requirement that the formalism be radically lexicalist, as by definition every rule introduces a lexical item.
   If a finite-state grammar is chosen however, the third criterion, that of linguistic adequacy, seems to present an insurmountable stumbling block.
 How can such a simple formalism, in which syntax is reduced to a string of category-states, hope to capture even the basic hierarchical structure, the familiar ``tree structure'', of linguistic expressions?   Indeed, if the non-terminals are viewed as atomic categories then there is no way this can be done.
 If however, in line with most current theories, categories are taken to be bundles of features and crucially if one of these features has the value of a stack of categories, then this hierarchical structure can indeed be represented.
   Using the notation A [B] to represent a state of basic category A carrying a category B on its stack, the hierarchical structure of the sentence:   (1) The man gave the dog a bone.
   can be represented as:   \t\tThe \t\tS \t\t[ ] man \t\tN \t\t[VP] gave \t\tVP \t\t[ ] (1a)\t\tthe \t\tNP \t\t[NP] dog \t\tN \t\t[NP] a \t\tNP \t\t[ ] bone \t\tN \t\t[ ]   Intuitively, syntactic links between non-adjacent words, impossible in a standard finite-state grammar, are here established by passing categories along on the stack ``through'' the state of intervening words.
 That such a formalism can fully capture basic linguistic structures is confirmed by the proof in Aho (1968) that an indexed grammar (ie.
 one where categories are supplemented with a stack of unbounded length, as above), if restricted to right linear trees (also as above), is equivalent to a context-free grammar.
   A perusal of the state transitions associated with individual words in (1a) reveals an obvious relationship to the ``types'' of categorial grammar.
 Using   to represent a list of categories (possibly null), we arrive at the following transitions (with their corresponding categorial types alongside).
   The ditransitive verb `gave' is   VP [   ]   NP [NP,   ] (VP/NP)/NP   Determiners in complement position are both:   NP [   ]   N [   ] NP/N   Determiner in subject position is `type-raised' to:   S [   ]   N [VP,   ] (S/VP)/N   The common nouns are all:   N [   ]   N   In fact as no intermediate constituents are formed in the analysis, an even closer parallel is to a dependency syntax where only rightward pointing arrows are allowed, of which the formalism as presented above is a notational variant.
 This lack of intermediate constituents has the added benefit that no ``spurious ambiguities'' can arise.
   Knowing now that the addition of a stack-valued feature suffices to capture the basic hierarchical structure of language, additional features can be used to deal with other syntactic relations.
 For example, following the example of GPSG, unbounded dependencies can be captured using ``slashed'' categories.
 If we represent a ``slashed'' category X with the lower case x, and use the notation A(b) for a category A carrying a feature b, then the topicalized sentence:   (2) This bone the man gave the puppy.
   will have the analysis:   \t\tThis \t\tS \t\t[ ] bone \t\tN \t\t[S(np)] the \t\tS(np) \t\t[ ] (2a)\t\tman \t\tN \t\t[VP(np)] gave \t\tVP(np) \t\t[ ] the \t\tNP \t\t[ ] puppy \t\tN \t\t[ ]   Although there is no space in this paper to go into greater detail, further constructions involving unbounded dependency and complement control phenomena can be captured in similar ways.
   The criterion that remains to be satisfied is that of width of coverage: can the formalism cope with the many ``peripheral'' structures found in real written and spoken texts? As it stands the formalism is weakly equivalent to a context-free grammar and as such will have problems dealing with phenomena like discontinuous constituents, non-constituent coordination and gapping.
 Fortunately if extensions are made to the formalism, necessarily taking it outside weak equivalence to a context-free grammar, natural and general analyses present themselves for such constructions.
 Two of these will now be sketched.
   Consider the pair of sentences (3) and (4), identical in interpretation, but the latter containing a discontinuous noun phrase and the former not:   (3) I saw a dog which had no nose yesterday.
   (4) I saw a dog yesterday which had no nose.
   which have the respective analyses: \t\tI \t\tS \t\t[ ] saw \t\tVP \t\t[ ] a \t\tNP \t\t[NP(t)]\t\t`t' = dog \t\tN \t\t[NP(t)]\t\t`time adjunct' (3a)\t\twhich \t\tS(rel) \t\t[NP(t)] \t\t `rel' = had \t\tVP \t\t[NP(t)] \t\t `relative' no \t\tNP \t\t[NP(t)] nose \t\tN \t\t[NP(t)] yesterday \t\tNP(t) \t\t[ ]   \t\tI \t\tS \t\t[ ] saw \t\tVP \t\t[ ] a \t\tNP \t\t[NP(t)] dog \t\tN \t\t[NP(t)] (4a)\t\tyesterday \t\tNP(t) \t\t[S(rel)] which \t\tS(rel) \t\t[ ] had \t\tVP \t\t[ ] no \t\tNP \t\t[ ] nose \t\tN \t\t[ ]   The only transition in (4a) that differs from that of the corresponding word in the `core' variant (3a) is that of `dog' which has the respective transitions:   N [NP(t)]   S(rel) [NP(t)] (in 3a)   N [NP(t)]   NP(t) [S(rel)] (in 4a)   Both nouns introduce a relative clause modifier S(rel), the difference being that in the discontinuous variant a category has been taken off the stack at the same time as the modifier has been placed on the stack.
 It has been assumed so far that we are using a right-linear indexed grammar, but such a rule is expressly disallowed in an indexed grammar and so allowing transitions of this kind ends the formalism`s weak equivalence to the context-free grammars.
   Of course, having allowed such crossed dependencies, there is nothing in the formalism itself that will disallow a similar analysis for a discontinuity unacceptable in English such as:   (5) I saw a yesterday dog.
   This does not present a problem, however, as in DOP it is information in the parsed corpus which determines the structures that are possible.
 There is no need to explicitly rule out (5), as the transition NP [   ]   [N] will be vanishingly rare in any corpus of even the most garbled speech, while the transition N [   ]   [S(rel)] is commonly met with in both written and spoken English.
   The analysis of standard coordination is shown in (6):   \t\tFido \t\tS \t\t[ ] gnawed \t\tVP \t\t[ ] a \t\tNP \t\t[VP(+)] (6)\t\tbone \t\tN \t\t[VP(+)] and \t\tVP(+) \t\t[ ] barked \t\tVP \t\t[ ]   Instead of a typical transition for `gnawed' of VP   NP, we have a transition introducing a coordinated VP: VP   NP [VP(+)]   In general for any transition X   Y , where X is a category and Y a list of categories (possibly empty), there will be a transition introducing coordination: X   Y [X(+)]   Non-constituent coordinations such as (7) present serious problems for phrase-structure approaches:   (7) Fido had a bone yesterday and biscuit today.
   However if we generalize the schema already obtained for standard coordination by allowing X to be not only a single category, but a list of categories, it is found to suffice for non-constituent coordination as well.
   \t\tFido \t\tS \t\t[ ] had \t\tVP \t\t[ ] a \t\tNP \t\t[NP(t)] (7a)\t\tbone \t\tN \t\t[NP(t)] yesterday \t\tNP(t) \t\t[N(+) [NP(t)]] and \t\tN(+) \t\t[NP(t)] biscuit \t\tN \t\t[NP(t)] today \t\tNP(t) \t\t[ ]   In this analysis instead of a regular transition for `bone' of: N [NP(t)]   NP(t) [ ]   there is instead a transition introducing coordination: N [NP(t)]   NP(t) [N(+) [NP(t)]]   Allowing categories on the stack to themselves have non-empty stacks moves the formalism one step further from being an indexed grammar.
 This is the final incarnation of the formalism, being the State-Transition Grammar of the title.
   Similar schemas are being investigated to characterize gapping constructions.
   It should be noted that an indefinite amount of centre-embedding can be described, but only at the expense of unlimited growth in the length of states:   \t\tThe \t\tS \t\t[ ] fly \t\tN \t\t[VP] the \t\tS(np) \t\t[VP] dog \t\tN \t\t[VP(np),VP] (8)\t\tthe \t\tS(np) \t\t[VP(np),VP] cat \t\tN \t\t[VP(np),VP(np),VP] scratched \t\tVP(np) \t\t[VP(np),VP] swallowed \t\tVP(np) \t\t[VP] died \t\tVP \t\t[ ]   This contrasts with unlimited right-recursion where there is no growth in state length:   \t\tI \t\tS \t\t[ ] saw \t\tVP \t\t[ ] the \t\tNP \t\t[ ] cat \t\tN \t\t[ ] (9)\t\tthat \t\tS(rel) \t\t[ ] scratched \t\tVP \t\t[ ] the \t\tNP \t\t[ ] dog \t\tN \t\t[ ] that \t\tS(rel) \t\t[ ] \t\t...
 \t\t...
 \t\t As the model is to be trained from real data, transitions involving long states as in (8) will have an ever smaller and eventually effectively nil probability.
 Therefore, when tuned to any particular language corpus the resulting grammar will be effectively finite-state.
   Assuming that we now have a corpus parsed with the state-transition grammar, how can this information be used to parse fresh text?   Firstly, for each word type in the corpus we can collect the transitions with which it occurs and calculate its probability distribution over all possible transitions (an infinite number of which will be zero).
 To make this concrete, there are five tokens of the word `dog' in the examples thus far, and so `dog' will have the transition probability distribution: N [VP(np),VP]   N [NP]   NP [ ]\t\t0.2 N [NP(t)]   S(rel) [NP(t)]\t\t0.2 N [NP(t)]   NP(t) [S(rel)]\t\t0.2 N [VP(np),VP]   S(np) [VP(np),VP]\t\t0.2 N [ ]   S(rel) [ ]\t\t0.2   To find the most probable parse for a sentence, we simply find the path from word to word which maximizes the product of the state transitions (as we have a first order Markov process).
   However this simple-minded approach, although easy to implement, in other ways leaves much to be desired.
 The probability distributions are far too ``gappy'' and even if a huge amount of data were collected, the chances that they would provide the desired path for a sentence of any reasonable length are slim.
 The process of generalizing or smoothing the transition probabilities is therefore seen to be indispensable.
   Although far from exhausting the possible methods for smoothing, the following three are those used in the implementation described at the end of the paper.
   1.
 Factor out elements on the stack which are merely carried over from state to state (which was done earlier in looking at the correspondence of state transitions to categorial types).
 The previous transitions for `dog' then become:   aaaaaaN [   ]   S(rel)\t\tN [   ]   [ ]\t\t0.2 \t\tN [   ]   [S(rel)]\t\t0.2 \t\tN [   ]   S(np) [   ]\t\t0.2 \t\tN [   ]   S(rel) [   ]\t\t0.4   2.
 Factor out other features which are merely passed from state to state.
 For instance in the example sentences, `the' has the generalized transitions:   \t\tS [   ]   N [VP,   ] \t\tS(np) [   ]   N [VP(np),   ]   which can be further generalized to the single transition:   aaS(   ) [   ]   N\t\tS(   ) [   ]   N [VP(   ),   ]\t\t   = set of features   3.
 Establish word paradigms, ie.
 classes of words which occur with similar transitions.
 The probability distribution for individual words can then be smoothed by suitably blending in the paradigmatic distribution.
 These paradigms will correspond to a great extent to the word classes of rule-based grammars.
 The advantage would be retained however that the system is still fine-grained enough to reflect the idiosyncratic patterns of individual words and could override this paradigmatic information if sufficient data were available.
   Words hitherto unknown to the system can be treated as being extreme examples of words lacking sufficient transition data and they might then be given a transition distribution blended from the open class word paradigms.
   Although essential for effective processing, the smoothing operations may give rise to new problems.
 For example, factoring out items on the stack, as in (1), removes from the model the disinclination for long states inherent in the original corpus.
 To recapture this discarded aspect of the language, it would be sufficient to introduce into the model a probabilistic penalty based on state length.
 This penalty may easily be calculated according to the lengths of states in the parsed corpus.
   Not only would this allow the modelling of the restriction on centre-embedding, but it would also allow many other ``processing'' phenomena to be accurately characterized.
 Taking as an example ``heavy-NP shift'', suppose that the corpus contained two distinct transitions for the word `threw', with the particle `out' both before and after the object.
   threw \t\tVP   NP, X(out) \t\tprob: p1 VP   X(out), NP \t\tprob: p2   Even if p1 were considerably greater than p2, the cumulative negative effect of the longer states in (10) would eventually lead to the model giving the sentence with the shifted NP (11) a higher probability.
   \t\tI \t\tS \t\t[ ] threw \t\tVP \t\t[ ] the \t\tNP \t\t[X(out)] bacon \t\tN \t\t[X(out)] (10)\t\tthat \t\tS(rel) \t\t[X(out)] Fido \t\tS(np) \t\t[X(out)] had \t\tVP(np) \t\t[X(out)] chewed \t\tVP(np) \t\t[X(out)] out \t\tX(out) \t\t[ ]   \t\tI \t\tS \t\t[ ] threw \t\tVP \t\t[ ] out \t\tX(out) \t\t[NP] the \t\tNP \t\t[ ] (11)\t\tbacon \t\tN \t\t[ ] that \t\tS(rel) \t\t[ ] Fido \t\tS(np) \t\t[ ] had \t\tVP(np) \t\t[ ] chewed \t\tVP(np) \t\t[ ]   One strength of n-gram models is that they can capture a certain amount of lexical preference information.
 For example, in a bigram model trained on sufficient data the probability of the bigram `dog barked' could be expected to be significantly higher than `cat barked', and this slice of ``world knowledge'' is something our model lacks.
   It would not be difficult to make a small extension to the present model to capture such information, namely by introducing an additional feature containing the ''lexical value'' of the head of a phrase.
 Abandoning the shorthand `VP' and representing a subject explicitly as a ``slashed'' NP, a sentence with added lexical head features would appear as:   \t\tThe \t\tS \t\t[ ] dog \t\tN(dog) \t\t[S(np(dog))] which \t\tS(rel,np(dog)) \t\t[S(np(dog))] (12)\t\tchased \t\tS(np(dog)) \t\t[S(np(dog))] the \t\tNP(cat) \t\t[S(np(dog))] cat \t\tN(cat) \t\t[S(np(dog))] barked \t\tS(np(dog)) \t\t[ ]   In contrast to n-grams, where this sentence would cloud somewhat the ``world knowledge'', containing as it does the bigram `cat barked', the added structure of our model allows the lexical preference to be captured no matter how far the head noun is from the head verb.
 From (12) the world knowledge of the system would be reinforced by the two stereotypical transitions:   `chased' S(np(dog))   NP(cat) `barked' S(np(dog))   [ ]   16,000+ running words from section N of the Brown corpus (texts N01-N08) were hand-parsed using the state-transition grammar.
 The actual formalism used was much fuller than the rather schematic one given above, including many additional features such as case, tense, person and number.
 Transition probabilities were generalized in the ways discussed in the previous section.
   100 sentences of less than 15 words were chosen randomly from other texts in section N of the Brown corpus (N09-N14) and fed to the parser without alteration.
 Unknown words in the input, of which there were obviously many, were assigned to one of seven orthographic classes and given appropriate transitions calculated from the corpus.
   27 were parsed correctly, ie.
 exactly the same as the hand parse or differing in only relatively insignificant ways which the model could not hope to know.
 23 were parsed wrongly, ie.
 the analysis differed from the hand parse in some non-trivial way.
 50 were not parsed at all, ie.
 one or more of the transitions necessary to find a parse path was lacking, even after generalizing the transitions.
   Although the results at present are extremely modest, it should be borne in mind both that the amount of data the system has to work on is very small and that the smoothing of transition probabilities is still far from optimal.
 The present target is to achieve such a level of performance that the corpus can be extended by hand-correction of the parser output, rather than hand-parsing from scratch.
 Not only will this hopefully save a certain amount of drudgery, it should also help to minimize errors and maintain consistency.
   A more distant goal is to ascertain whether the performance of the model can improve after parsing new texts and processing the data therein even without hand-correction of the parses, and if so what the limits are to such ``self-improvement''.
 