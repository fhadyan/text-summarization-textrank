 The efficiency of LR(k) parsing techniques   is very attractive from the perspective of natural language processing applications.  This has stimulated the computational linguistics community to develop extensions of these techniques to general context-free grammar parsing. The best-known example is generalized LR parsing, also known asTomita's algorithm, described by and further investigated by, for example, and . Despite appearances, the graph-structured stacks used to describe Tomita's algorithm differ very little from parse tables, or in other words,generalized LR parsing is one of theso calledtabular parsing algorithms, among which also the  CYK algorithm   and Earley's algorithm  can be found. (Tabular parsing is also known as chart parsing.)In this paper we investigate the extension of LR parsing to general context-free grammars from a more general viewpoint: tabularalgorithms can often be described by the compositionof two constructions. One example is given by and ;the construction of pushdown automata from grammars and thesimulation of these automata by means of tabulation yield differenttabular algorithms for different such constructions.Another example, on which our presentation is based, was firstsuggested by : a grammar is first transformedand then a standard tabular algorithm along with some filtering condition is appliedusing the transformed grammar. In our case, the transformationand the subsequent application of the tabular algorithm result in a new formof tabular LR parsing.Our method is more efficient than Tomita's algorithm in two respects.First, reduce operations are implemented in an efficient way,by splitting them into several, more primitive, operations(a similar idea has been proposed by  for Tomita's algorithm). Second, several paths in the computation that must be simulated separately by Tomita's algorithm are collapsed into a single computation path, using state minimization techniques. Experiments on practical grammars have indicated that there is a significant gain in efficiency, with regard to both space and time requirements.Our grammar transformation produces a so called cover for the input grammar, which together with the filtering condition fully captures the specification of the method, abstracting away from algorithmic details such as data structures and control flow. Since this cover can be easily precomputed,  implementing our LR parser simply amounts torunning the standard tabular algorithm.This is very attractive from an application-oriented perspective,since many actual systems for natural language processing are based on these kinds of parsing algorithm.The remainder of this paper is organized as follows. In Section  some preliminaries are discussed.  We review the notion of LR automaton in Section   and introduce the notion of 2LR automaton in Section .  Then we specify our tabular LR method in Section ,  and provide an analysis of the algorithm in Section .  Finally, some empirical results are given in Section ,  and further discussion of our method is provided in Section .   Throughout this paper we use standard formal language notation.We assume that the reader is familiar with context-free grammar parsing theory .   A context-free grammar ()is a 4-tuple ,where and are two finite disjoint sets of terminal and nonterminal symbols, respectively, is the start symbol, and P is a finite set of rules. Each rule has the formwith and ,where denotes .The size of G, written ,is definedas ;by  wemean the length of a string of symbols .We generally use symbols to range over ,symbols to range over ,symbols X, Y, Z to range over ,symbols to range over ,and symbols to range over .We write to denote the empty string.A is said to be in binary form if for all of its rules.(The binary form does not limitthe (weak) generative capacity of context-free  grammars .) For technical reasons, we sometimes use the augmented grammar associated with G, defined as ,where ,and are fresh symbols, ,and .A pushdown automaton ()is a 5-tuple,where ,and T are finite sets of input symbols, stack symbolsand transitions, respectively;is the initial stack symbol andis the final stack symbol.  Each transition has the form ,where ,,,and or z=a.We generally use symbols to range over ,andthe symbol to range over .Consider a fixed input string .A configuration of the automaton is a pair consistingof a stack and the remaining input w,which is a suffix of the input string v.The rightmost symbol of represents the top of the stack.The initial configuration has the form,where the stack is formed by the initialstack symbol.  The final configuration has the form,where the stack is formed by the finalstack symbol stacked upon the initial stack symbol. The application of a transition is described as follows.If the top-most symbols of the stack are ,then these symbols may be replaced by ,provided that either ,or z=a and a isthe first symbol of the remaining input. Furthermore, if z=a then a isremoved from the remaining input. Formally, for a fixed we define the binary relation on configurations as the least relation satisfyingif there is a transition,andif there is a transition.The recognition of a certain input v is obtained if starting from theinitial configuration for that input we can reach the finalconfiguration by repeated application of transitions, or,formally, if ,where denotes the reflexive and transitive closure of.By a computation of a PDA we mean a sequence... ,.A is called deterministic if for all possible configurations at most one transition is applicable. A is said to be in binary formif, for all transitions ,we have .Let be a .We recall the notion of LR automaton, which isa particular kind of .We make use of the augmented grammar  introduced in Section . Let .We introduce the function from to and the function from to .For any ,is the smallest set such that1.;and2.and together imply .We then define   We construct a finite set as the smallest collection of sets satisfying the conditions:1.;and2.for every and ,we have,provided .Two elements from deserve special attention:,and ,which is defined to bethe unique set in containing ;in other words, .For ,an A-redex is a string ,,of elements from ,satisfying the following conditions: 1.,for some ;and2.,        for .Note that in such an A-redex, ,and qk, for .The LR automaton associated with G is now introduced.  Transitions in (i) above are called shift, transitions in (ii) are called reduce. The automata defined in the previous sectionare deterministic only for a subset of the s,  called the LR(0) grammars , and behave nondeterministically in the general case.When designing tabular methods that simulate nondeterministic computations of ,two main difficulties are encountered: A reduce transition in is an elementary operation that removes from the stack a number of elements bounded by the size of the underlying grammar.  Consequently, the time requirement of tabularsimulation of computations can be onerous, for reasons pointed out by  and .The set can be exponential in the size  of the grammar .   If in such a case the computations of touch upon each state, then time and space requirements of tabular simulation are obviously onerous. The first issue above is solved here by recasting in binary form. This is done by considering each reduce transition as a sequence of ``pop'' operations which affect at most two stack symbols at a time. (See also , , and ,and for LR parsing specifically  and .)The following definition introduces this new kind of automaton.Transitions in (i) above are again called shift, transitionsin (ii) are called initiate, those in (iii) are called gathering, and transitions in (iv) are called goto.The role of a reduce step in is taken over in by an initiate step, a number of gathering steps, and a goto step.Observe that these steps involve the new stack symbolsthat are distinguishable from possible stack symbols.We now turn to the second above-mentioned problem, regarding the size of set .The problem is in part solved here as follows. The number of states in is considerably reduced by identifying two states if they become identicalafter items from have beensimplified to only the suffix of the right-hand side .This is reminiscent of techniques of state minimization for finite automata ,  as they have been applied before to LR parsing, e.g., by  and .Let be the augmented grammar associated with a G, and let .We define variants of the and functions from theprevious section as follows.For any set ,is the smallest collection of sets such that1.;and2.andtogether imply .Also, we define   We now construct a finite set as the smallest set satisfying the conditions:1.;and 2.for everyand ,we have,provided .As stack symbols, we take the elements from and a subsetof elements from :In a stack symbol of the form (X, q),the X serves to record the grammar symbolthat has been recognized last, cf. the symbols that formerly were foundimmediately before the dots. The 2LR automaton associated with G can now be introduced.   Note that in the case of a reduce/reduce conflict with two grammarrules sharing some suffix in the right-hand side, the gathering stepsof will treat both rules simultaneously,until the parts of the right-hand sides are reached where the two rules differ.(See  for a similar sharing of computation forcommon suffixes.)An interesting fact is that the automaton is very similarto the automaton constructed for a grammar transformedby the transformation given by .This section presents a tabular LR parser, which is the main result of this paper. The parser is derived from the 2LR automata introduced in the previous section.  Following the general approach presented by , we simulate computations of these devices using a tabular method, a grammar transformation and a filteringfunction.We make use of a tabular parsing algorithm which is basically an asynchronous version of the CYK algorithm, as presented by , extended to productions of the forms and and with a left-to-right filtering condition. The algorithm uses a parse table consisting in a 0-indexed square array U. The indices represent positions in the input string.We define Ui to be .Computation of the entries of U is moderated by a filtering process.This process makes use of a function fromto ,specific to a certaincontext-free grammar.We have a certain nonterminal which is initiallyinserted in U0,0 in order to start the recognition process.We are now ready to give a formal specification of the tabular algorithm.  The string has been accepted when .We now specify a grammar transformation, based on the definition of .  Observe that there is a direct, one-to-one correspondence between transitions of and productions of .The accompanying function is defined as follows(q,q',q'' range over the stack elements):The above definition implies that only the tabular equivalents of theshift, initiate and goto transitions are subject to actual filtering; the simulationof the gathering transitions does not depend on elements in .Finally, the distinguished nonterminal from the cover used to initialize the table is .Thus we start with .The 2LR cover introduces spurious ambiguity: where some grammarG would allow a certain number of parses to be found for a certaininput, the grammar in general allows more parses.This problem is in part solved by the filtering function .The remaining spurious ambiguity is avoided by a particularway of constructing the parse trees, described in what follows. After Algorithm  has recognized a given input, the set of all parse trees can be computed as where the function ,which determines sets of either parse trees or lists of parse treesfor entries in U, is recursively defined by:1.is the set .This set contains a singleparse tree consisting of a single node labelled a.2.is the set .This set consists ofan empty list of trees.3.is the union of the sets ,where ,,and there is at least oneandin ,for some q.For each such k, select one such q. We define.Each is a list oftrees, with head t and tail .4.is the union of the sets ,where is such that in .We define .The function constructs a tree from a fresh root node labelledA and the trees in list as immediate subtrees.We emphasize that in the third clause above, one should not consider more than one q for given k in order to prevent spurious ambiguity.(In fact, for fixed X,i,k and for different q such that ,yields the exact same set of trees.)With this proviso, the degree of ambiguity, i.e. the number of parsesfound by the algorithm for any input, is reduced to exactly that of the source grammar.A practical implementation would construct the parse trees on-the-fly,attaching them to the table entries, allowing packing and sharingof subtrees (cf. the literature on parse forests  ,).  Our algorithm actually only needs one (packed) subtree for several with fixed X,i,k but different q.The resulting parse forests would then be optimallycompact, contrary to some other LR-based tabular algorithms, as pointedout by ,  and . In this section, we investigate how the steps performed  by Algorithm  (applied to the 2LR cover) relate to those performed by ,for the same input.We define a subrelation of as:if and only if,for some ,where for all k, .Informally, we have if configurationcan be reached from without thebottom-most part of the intermediate stacks being affected byany of the transitions;furthermore, at least one element is pushed on top of .The following characterization relates  the automaton  and Algorithm  applied to the 2LR cover.Symbol is eventually added to Ui,j if and only if for some :In words, q is found in entry Ui,j if and only if, at input position j, the automaton would push some element q on top of some lower-part of the stack that remains unaffected while the input from i to j is being read.The above characterization, whose proof is not reported here,is the justification for calling the resulting algorithm tabular LR parsing. In particular, fora grammar for which is deterministic, i.e. for anLR(0) grammar, the number of steps performed by andthe number of steps performed by the above algorithm are exactly the same. In the caseof grammars which are not LR(0), the tabular LR algorithmis more efficient than for example a backtrack realisation of .For determining the order of the time complexity ofour algorithm, we look at the mostexpensive step, whichis the computation of an element from two elements and ,through.In a straightforward realisation of the algorithm,this step can be applied times (once for each i,k,j and each transition), each step takinga constant amount of time.We conclude that the time complexity of our algorithmis .As far as space requirements are concerned, each set Ui,jor Ui contains at most elements.(One may assume an auxiliary table storing each Ui.)This results in a space complexity .The entries in the table represent single stack elements, as opposed topairs of stack elements following  and . This has been investigated before by  and . We have performed some experiments with Algorithm  applied to and ,for 4 practical context-free grammars.For a cover was used analogous to the one in  Definition ; the filtering function remains the same.The first grammar generates a subset of the programming language ALGOL 68 . The second and third  grammars generate a fragment of Dutch, and are referred to  as the CORRie grammar  and the Deltra  grammar ,  respectively. These grammars were stripped of their argumentsin order to convert them into context-free grammars.  The fourth grammar, referred to as the Alvey grammar ,  generates a fragment of English and wasautomatically generated from a unification-based grammar.The test sentenceshave been obtained by automatic generation from the grammars, using the Grammar Workbench , which uses a random generator to select rules; therefore these sentences do not necessarily represent input typical of the applications for which the grammars were written.  Table  summarizes the test material.  Our implementation is merely a prototype, which means thatabsolute duration of the parsing process is little indicative ofthe actual efficiency of more sophisticated implementations.Therefore, our measurements have been restricted to implementation-independentquantities, viz. the number of elements storedin the parse table and the number of elementary stepsperformed by the algorithm. In apractical implementation, such quantities will strongly influence the space and time complexity, although they do not represent the onlydetermining factors. Furthermore, all optimizationsof the time and space efficiency have been left out of consideration. Table  presents the costs of parsing the  test sentences.The first and third columns give the number of entries stored in tableU, the second and fourth columns give the number of elementary steps that wereperformed. An elementary step consists of the derivation of one elementin or from one or two other elements. The elements that are used in the filtering process are countedindividually. We give an example for the case of .Suppose we derive an element froman element ,warranted bytwo elements ,,through ,in the presence ofand.We then count two parsing steps, one for q1 and one for q2. Table  shows that there is a significant gain in space and time efficiency when moving from to .Apart from the dynamic costs of parsing, we have also measured somequantities relevant to the construction and storage of the two typesof tabular LR parser.  These data are given in Table . We see that the number of states is strongly reducedwith regard to traditional LR parsing. In the case of the Alveygrammar, moving from to amountsto a reduction to 20.3 %. Whereas time- and space-efficient computation of for this grammar is a serious problem,computation of will not be difficult on any moderncomputer. Also significant is the reduction from to,especially for the larger grammars. These quantities correlate with the amount of storage needed for naive representationof the respective automata.Our treatment of tabular LR parsing has two important advantages overthe one by Tomita: It is conceptually simpler, because we make use of simpleconcepts such as a grammar transformation and the well-understoodCYK algorithm, instead of a complicated mechanism working on graph-structured stacks.Our algorithm requires fewer LR states.This leads to faster parser generation, to smaller parsers, and to reduced time and space complexity of parsing itself.The conceptual simplicity of our formulation of tabularLR parsing allows comparison with other tabular parsing techniques,such as Earley's algorithm  and tabular left-corner  parsing , based on implementation-independent criteria. This is in contrast to experiments reported before (e.g. by),which treated tabular LR parsing differently from the othertechniques.The reduced time and space complexities reported in the previoussection pertain to the tabular realisation of twoparsing techniques, expressed by the automata and .The tabular realisation of the former automata isvery close to a variant of Tomita's algorithm by . The objective of our experiments was to show that theautomata provide a better basis than for tabular LR parsing with regard to space and time complexity. Parsing algorithms that are not based on the LR techniquehave however been left out of consideration, and sowere techniques for unification grammars and techniques incorporatingfinite-state processes.Theoretical  considerations ,, have suggested that for natural language parsing, LR-based techniques may notnecessarily be superior to other parsing techniques, althoughconvincing empirical data to this effect has never been shown.This issue is difficult to resolve because so much of the relative efficiency of the different parsing techniques dependson particular grammars and particular input, as well ason particular implementations of the techniques. We hopethe conceptual framework presented in this paper mayat least partly alleviate this problem.