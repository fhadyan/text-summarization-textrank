  The alternative syntactic readings of a sentence such as synamb probably number in the hundreds, whereas sentences such as hobbs would have hundreds of thousands scopally distinct readings if all permutations of scope-taking sentence constituents were considered admissible readings.
 Yet, human beings appear able to deal with these sentences effortlessly.
   This Combinatorial Explosion Puzzle is one of the most fundamental questions to be addressed by a theory of language processing, and a substantial problem for developers of Natural Language Processing () systems.
 systems which have to perform non-linguistic actions like booking a flight in response to an user's utterance must arrive at the preferred interpretation of their input in the context of the conversation, if one exists; otherwise, they must realize that their input is ambiguous and request a clarification.
 Examples such as synamb and hobbs indicate that such systems cannot adopt the sentence processing strategy of generating all the readings of an ambiguous sentence and choosing one of them, because there are too many such readings.
 In order to develop such systems, a theory of ambiguity processing is needed that is consistent both with linguistic facts and with what is known about the way humans disambiguate.
   Work on underspecified representations such as , , ,, because it is explicitly motivated by the Combinatorial Explosion Puzzle, and aims at a unified account of all interpretation processes, including those that occur before the scope of all operators has been determined.
 The work on underspecified representations holds the promise of yielding a better account of the way interpretive processes such as scope disambiguation and reference resolution affect each other.
   The existing theories of underspecification, however, have been motivated almost exclusively by computational considerations.
 For example, the semantics assigned to underspecified representations is designed so as to support those inferences that are deemed useful for an economical approach to disambiguation, rather than being motivated by an analysis of the phenomenon of ambiguity.
 In this paper I explore some of the issues that arise when trying to establish a connection between work on underspecification and, on the one side, work on ambiguity in semantics; on the other side, work on ambiguity in the psychological literature.
 A theory of underspecification is developed `from the first principles', i.e., starting from a definition of what it means for a sentence to be semantically ambiguous and from what we know about the way humans deal with ambiguity.
 The goal is to arrive at a linguistically and cognitively plausible theory of ambiguity and underspecification that, in addition to computational gains, may provide a better understanding of how humans process language.
   Many of the issues discussed in this paper arose from work on the project at the University of Rochester, in which the issues of language comprehension, planning, and reasoning encountered in task-oriented natural language conversations are studied .
 The theory of ambiguity proposed in this paper is the basis for the implemented surface discourse interpretation system -93, used in the -93 demo system.
 -93 is described in .
   The dictionary definitions of the terms ambiguity and ambiguous try to capture the intuition that an expression is ambiguous if `it has multiple meanings'.
 An example are the following entries, from Webster's: A more precise characterization of the notion of ambiguity is required to differentiate ambiguity from vagueness or indeterminacy, for example, or to clarify notions such as homonimy and polysemy (see below).
   An early attempt at making the notion of ambiguity more precise was presented in .
 Lakoff proposed linguistic tests that could be used to tell whether a sentence was ambiguous or not.
 (Lakoff's tests were meant to provide a way for distinguishing ambiguous sentences from indeterminate ones.) Zwicky and Sadock showed, however, that such tests do not result in an unambiguous classification of sentences, and that a formal characterization of the concepts of ambiguity and indeterminacy was required even to understand what these `ambiguity tests' really test.
   One problem to be tackled in attempting to make precise the definition of ambiguity is to say what `meanings' and `senses' are.
 In modern semantic theory, the meaning assigned to an expression by a grammar is a function from contexts (or discourse situations) to senses.
 Roughly speaking, the discourse situation provides a value for all context-dependent aspects of the sentence; the sense of a sentence (what we get once we resolve its context-dependent aspects) tells us under which circumstances in the world the sentence is true or false.
   Not all notions of `sense' employed in the literature can serve as the basis for a definition of ambiguity.
 For example, of the various notions of proposition (the sense of sentences), the simplest is the one according to which propositions are truth values.
 But if we were to use this notion of sense, the sentence Kermit croaked, ambiguous between a reading in which Kermit utters a frog-like sound and a reading in which he dies, would be classified as unambiguous with respect to all models in which Kermit has both the property of dying and the property of producing a frog-like sound, or he (it) has neither property.
 In other words, in providing a definition of ambiguity we find the same need for a fine-grained notion of sense that has been observed in connection with the semantics of attitude reports.
 A model-theoretic definition of ambiguity requires a finer-grained notion of proposition than simply truth values.
 In most recent semantic theories, senses are intensional objects; the simplest way of achieving intensionality is to use functions from possible worlds or situations to referents as one finds in Montague Grammar, where, for example, propositions are functions from possible worlds to truth values.
 This simple form of intensionality will be sufficient for the purposes of the present paper.
   The notions of `meaning' and `sense' just discussed are the starting point for the semantic account of the notion of ambiguity and its relation with vagueness developed by Pinkal introduces the notion of indefiniteness to subsume both ambiguity and vagueness.
 He defines indefiniteness as follows:   Pinkal formalizes the notion of indefiniteness in terms of precisification.
 According to Pinkal, a linguistic expression is semantically indefinite if it has the potential for being made precise in distinct ways.
 For example, the sentence The Santa Maria is a fast ship containing the degree adjective fast can be `made precise' (and assigned a definite truth value) either with respect to a context in which `fast' is interpreted as `fast for a modern ship', in which case the sentence is false; or with respect to a context in which `fast' is interpreted as `fast for a ship of her age', in which case the sentence can be true or false, depending on the class of comparison.
 Let p and q be two propositions.
 Proposition p is more precise than q iff (i) p is true (false) under all states of the world under which q is true (false), and (ii) p is true or false under certain circumstances under which q is indefinite.
 The idea of precisification is defined as follows:   The connection between indefiniteness and precisification is provided by the following Precisification Principle:   Precisification Principle : A sentence is of indefinite truth value in a context if and only if it can be precisified alternatively to ``true'' or to ``false''.
 which Pinkal also reformulates as follows: Extended Precisification Principle An expression is semantically indefinite in a context iff it can assume different senses in that context.
 Pinkal does not equate ambiguity with vagueness.
 His theory includes, in addition to the notion of precisification, additional criteria to differentiate different forms of ambiguity, as well as differentiating `pure' ambiguity from `pure' vagueness.
 The intuition he is trying to capture is that ``...whether an expression is ambiguous or only vague is a question that cannot be cleared once and for all.
 Indefiniteness is perceived as ambiguity when alternative precisifications are predominant, as vagueness when an unstructured continuum presents itself:''   Ambiguity (Pinkal) : If the precisification spectrum of an expression is perceived as discrete, we may call it ambiguous; if it is perceived as continuous, we may call it vague.
 Pinkal identifies two fundamental types of ambiguity, according to whether an expression has, or does not have, a `wider' sense that could be taken as most `basic'.
 For example, ball does not have a wide sense of `round object and dancing party', whereas American may either mean `person from the US' or `person from the American continent'.
 He classifies expressions like American which have a wider sense as having a multiplicity of use, whereas expressions such as ball or green which do require precisification are called narrowly ambiguous.
 The cases of ambiguity in the narrow sense are further distinguished in two classes, depending on whether they are subject to the Precisification Imperative.
 Although the two interpretations of green are distinct, it is possible of an object to be both green in the `ripe' sense and green in the `color' sense: for example, a green apricot.
 An object cannot, however, be a `band' both in the musical group sense and in the piece of tape sense.
 Pinkal proposes that polysemous expressions behave like green, and calls all of these expressions P-type ambiguous; expressions like band, however, are true homonyms, and therefore he calls them H-type ambiguous.
 These latter are defined as follows:   Precisification Imperative : An expression is H-type ambiguous iff its base level is inadmissible, i.e., if it requires precisification.
 For my purposes, it's not particularly important whether the difference between homonimy and polysemy is completely captured by the Precisification Imperative; what is important is the claim that H-type ambiguous expressions need precisification, and furthermore, that the Precisification Imperative ``is a second order phenomenon ...that lies beyond the scope of a strictly truth-conditional approach.'' ( , p.
 86-87).
 I will provide below independent reasons for including a formalization of reasoning in context in a treatment of ambiguity, and I will argue that such formalization provides the necessary tools to express the Precisification Imperative.
   To summarize, a sentence is H-type ambiguous iff the grammar assigns to it distinct precisifications (senses) in a given discourse situation, and if the `base level' of the expression requires precisification.
 Thus, the sentence Kermit croaked is considered ambiguous since in the `empty context' that provides all the senses of the expression according to the grammar G for English, that sentence has two senses: the proposition that attributes to Kermit the property of producing the sound that frogs produce, and the proposition that attributes to Kermit the property of dying.
 (I am assuming here that terms like Kermit refer unambiguously.) On the other hand, the sentence Kermit kissed Miss Piggy would be considered unambiguous with respect to the same context.
   Although Pinkal is only concerned with lexical ambiguity, the precisification approach can also be used to classify as ambiguous sentences which have more than one structural analysis (like the sentence They saw her duck) or are scopally ambiguous (cfr.
 the sentence Everybody didn't leave) whenever the grammar assigns to them more than one sense.
 I will discuss below how Pinkal's system can be extended to scopal and referential ambiguity.
   It is important to realize that saying that a sentence is ambiguous in a context if it has distinct precisifications is not the same as saying that an ambiguous sentence is equivalent to the disjunction of its distinct precisifications.
 Intuitively, in uttering S, whose two precisifications are the propositions P and Q, a speaker may have meant P or she may have meant Q, but the following does not hold:   [[A means that P] [A means that Q]] [A means that [P Q]] To treat an ambiguous sentence in such a way would be tantamount to propose that an ambiguous sentence has a single sense in any given discourse situation, namely, the proposition that is true at a situation if either of the distinct interpretations of the sentence is true at that situation; but according to the definition above, an ambiguous sentence is one which has more than one sense at a discourse situation.
 For example, according to the definition of ambiguity discussed above, the listener of an utterance of They saw her duck could either interpret the speaker as saying that the contextually determined set of individuals denoted by the pronoun they saw a contextually specified female person lowering herself, or as saying that that set of individuals saw the pet waterfowl of that female person.
 According to the disjunction theory, instead, the listener would attribute to the speaker of that sentence a single meaning, albeit a disjunctive one; namely, that it was either the case that they saw a contextually specified female person lowering herself, or it was the case that they saw the pet waterfowl of that female person.
   I will refer to the idea that a semantically ambiguous sentence denotes the disjunction of its alternative interpretations as the disjunction fallacy.
 The disjunction fallacy can be found in the literature in two forms.
 Its `purest' form is the hypothesis that the interpretation process literally involves generating all of the senses of an expression and putting them together in a disjunction.
 In this form, the disjunction 'theory' is not simply counterintuitive; it doesn't explain the combinatorial explosion puzzle at all.
 As far as I know, this `explicit' form of the theory has only been discussed jokingly.\nCHECK HOBBS REFERENCE FROM STALLARD.
 One can find in the literature, however, an `implicit' form of the disjunction theory, in theories of underspecification that assign to underspecified expressions a semantics that makes them equivalent to the disjunction of their readings.
 One such proposal is ; the semantics of UDRSs is also disjunctive .
   The Role of Syntactic and Semantic Constraints   Although the number of logical form permutations that one can obtain for a particular sentence by, e.g., considering all the permutations of its operators may be rather large, constraints of a syntactic and/or semantic nature drastically reduce this number.
   In the case of scopal ambiguity, for example, permutations may not correspond to actual readings for at least three reasons.
 First of all, some of these permutations result in logical expressions that are either ill-formed or contradictory,as noted by, e.g., Hobbs an Shieber .
 For example, samb-ex:impossiblea, in the interpretation in which the pronoun he is anaphoric on the every man, does not have a reading in which the the woman hei married outscopes the every mani.
 There is no well-formed logical expression that may represent this reading.
 Hobbs and Shieber point out that this constraint also prevents a quantifier to scope between a noun and its complement: for example, a meeting may not scope inside most but outside each in .
   Another reason why the number of actual readings of a sentence is much smaller than the number of permutations of its operators is that two distinct permutations may correspond to semantically equivalent readings.
 For example, has only one reading, even though (at least) two equivalent logical expressions can be obtained as the translation of the sentence.
   Finally, the readings corresponding to certain permutations may be unavailable because of syntactic constraints.
 Much work on uncovering readings that are absent due to constraints on syntactic trasformations and/or conditions on syntactic levels of representation has been done in the generative tradition .
   Some of the constraints proposed in this literature have been proved to yield quite robust predictions.
 Perhaps the best known example of syntactic constraint is the observation that a quantifier cannot take scope outside the clause in which it appears.
 The observation that clauses serve as `scope islands' goes back at least to Rodman , but was discussed most extensively by May ; the constraint was called Scope Constraint by Heim .
 The Scope Constraint is exemplified by the contrast in sc-1: whereas sc-1a has a reading in which every department is allowed to take wide scope over a student, this reading is not available for sc-1b, even though arguably from every department and who was from every department have the same denotation.
 Although syntactic and semantic constraints do not rule out all possible readings--for example, a sentence like They saw her duck still has more than one interpretation under all of these theories--a theory of disambiguation must be such that these constraints can play a role.
   As noted by Hirst , the discussions of ambiguity processing in the literature tend to ignore the fact that humans are aware that sentences can be ambiguous, and that they can exploit the ambiguity of sentences for rhetorical effect.
 Raskin, for example, claims that humor crucially relies on ambiguity.
 He discusses examples such as the following (p.
 25-26):   The joke relies on two assumptions about human processing: first, that the clause the first thing that strikes a stranger in New York gets interpreted before the end of the sentence, with strikes receiving the `surprise' interpretation; and second, that the reader, upon reading is a big car, will go back, produce a second interpretation, and entertain both interpretations simultaneously.
 The joke could not be understood unless the hearer were able to entertain the two interpretations of the sentence simultaneously.
 These jokes can exploit other forms of ambiguity, e.g., scopal ambiguity, as in Statistics show that every 11 seconds a man is mugged here in New York City.
 We are here today to interview him.
   The reader's ability to entertain more than one interpretation simultaneously is exploited in poetry, as well .
 The linguistic articles discussing ambiguity are another literary form that exploits this possibility.
 Examples such as They saw her duck are a clear case of deliberate ambiguity; the whole point of these examples is to show that a sentence can have more than one interpretation.
 The writer relies on the reader being able to entertain more than one interpretation at once.
   The opposite is true, as well: when clarity is a goal, writers and speakers tend to construct their sentences in such a way as to avoid ambiguity.
 Thus, most sentences one runs across in scientific texts or in transcripts of task-oriented conversations have a clearly preferred interpretation.
 This interpretation is sometimes suggested by the context, sometimes by means of disambiguation markers--expressions such as each, a different, or the same that suggest which interpretation is preferred.
 Thus, a writer will use sentences such as Every kid climbed the same tree, rather than Every kid climbed a tree, when we/she wants to make sure that the reader arrives at the interpretation in which there is a single tree.
   I will call the situation in which a listener arrives at more than one interpretation for an utterance perceived ambiguity.
 A situation in which B perceives an utterance as ambiguous may result in B's appreciating the joke, the poetic phrase, or the point of the linguistic example; if the ambiguity is not perceived as intended, B may say saying something like This is not very clear, or perhaps This sentence is ambiguous.
 This situation can be informally characterized as follows:   The phenomenon of deliberate ambiguity suggests that the solution to the Combinatorial Explosion Puzzle cannot be that humans either generate only one interpretation at a time by using some clever heuristics, or do not generate any interpretation at all.
 Humans entertain more than one interpretation at a time, and they may not be able to choose one among them.
 This conclusion is also supported by psychological results.
 There is evidence, for example, that during both lexical processing and syntactic processing several hypotheses are generated in parallel, and only later filtered on the basis of contextual information .
 Kurtzman and MacDonald suggest a similar model for scope disambiguation.
 As far as reference interpretation is concerned, there is some evidence that all pragmatically available referents become active before a referent is identified (see, e.g., ).
   These facts are consistent with the view of discourse interpretation taken in Artificial Intelligence, in which processes such as reference resolution or lexical disambiguation are modeled in terms of defeasible inference, which may result in alternative hypotheses.
 Examples include the theories of the effects of semantic priming on lexical disambiguation, as formalized, e.g., in Hirst's ABSITY system or, more recently, in statistically based terms; the theories about the effects of local focusing on the choice of pronoun antecedents such as ; the work on temporal interpretation by Asher, Lascarides, and Oberlander (see, e.g., ); and work on scopal disambiguation such as [, .
   A preliminary and, I hope, uncontroversial conclusion I intend to draw from the discussion on deliberate ambiguity and ambiguity processing is that a theory of ambiguity that aims at explaining the Combinatorial Explosion Puzzle needs to be concerned both with the interpretation that the grammar assigns to a sentence--i.e., what it means for a sentence to be semantically ambiguous--and with the process by which interpretations are generated, i.e., with what it means for an utterance to be perceived as ambiguous.
 On the one hand, the theory must explain why the disambiguation process will not generate all semantically available interpretations; on the other hand, it must predict that more than one interpretation will be generated.
 This conclusion is the central idea of this paper, indeed, what gives the paper its title.
 The inclusion of a theory of disambiguation will also remedy one of the omissions in Pinkal's theory, namely, how to formalize the Precisification Imperative.
   The discussion of perceived ambiguity supports a stronger claim, namely, that semantic ambiguity and perceived ambiguity are distinct notions, in the sense that whereas a model of semantic ambiguity has to express the truth-conditional properties of an expression, the reasoning processes involved in disambiguation, and that may lead to a perceived ambiguity, consist of defeasible inferences that are not supported by the semantics of ambiguous expressions.
   The distinction I intend to draw, then, is as follows.
 Semantic ambiguity is part of the specification of the grammar of a language; most, if not all, sentences are semantically ambiguous, but their ambiguity need not be noticed by listeners, and in fact it is typically discovered only by linguistic research.
 Perceived ambiguity, on the other hand, is a result of the interpretation process, that is defeasible in nature, and may therefore result in more than one interpretation in cases of miscommunication or when the speaker constructs the context appropriately to serve a rhetorical purpose, as in the puns presented above.
   Some readers may wonder why the developer of a system should be concerned with perceived ambiguity, i.e., with generating all of the contextually available interpretations of a sentence.
 The answer is that certain applications need this information.
 Consider the following example, again from the domain.
 Say that the user utters move the engine to Avon, and say that two different engines have been discussed during the elaboration of the current part of the plan.
 Clearly, we do not want the system to just come out with a plausible guess about which engine was meant: instead, we want it to recognize the ambiguity and ask for clarification.
 In general, all systems that engage in conversations with their users need to be able to recognize an ambiguity, to ask for clarifications when necessary rather than guess one possible interpretation, and to make their own output unambiguous.
 (Of course, the theory of contextual disambiguation must be such that no spurious ambiguities are obtained.)   All theories of semantic interpretation based on Montague's general program as exposed in Universal Grammar assume that the grammar of a language specifies two homomorphisms: one between syntactic trees and a disambiguated language , and a second one between the disambiguated language and objects of the model M (the senses).
 These two homomorphisms can be composed, thus making the intermediate level of the disambiguated language dispensable.
 The grammar assigns to an ambiguous expression of distinct expressions of , each of which has a unique interpretation.
   A direct implementation of this strategy in an system would require generating all senses of an ambiguous sentence-string, which would be clearly problematic.
 Many systems, instead, make use of heuristic methods that generate only one interpretation and ignore the alternatives.
 These heuristics work fairly well fairly often; such systems, however, won't be able to perceive an ambiguity even when it would be helpful to do so.
 Other systems therefore split the semantic problem of computing all the interpretations of a sentence from the processing problem of generating these interpretations in context, by making use of an intermediate, underspecified level of representation.
 One of the earliest examples of underspecified representations is the `Logical Form' of Schubert and Pelletier .
 The representation for proposed by Schubert and Pelletier, shown in , is a typical example of these underspecified representations: quantifiers are left in place and the referent for the definite description the tree is not specified.
 In more recent years, underspecified representations similar to Schubert and Pelletier's have been used by [], Fenstad , in Allen's textbook and, most recently, in the Core Language Engine ; the `uninterpreted conditions' produced during the intermediate steps of the construction algorithm in [] can be considered underspecified representations as well.
   Underspecified representations were originally conceived as a way to solve a problem in system implementation, namely, separating `context-independent' from `context dependent' aspects of the interpretation, thus making either part reusable for different applications.
 Since the motivation was strictly computational, the underspecified representations used in most systems are little more than data structures, in the sense that they do not have a interpretation other than the one provided by the procedures that interpret them.
 These representations `encode' the ambiguity of a sentence in the sense that that sentence has the reading r iff that reading can be generated by repeatedly applying `construction rules' to the underspecified representation.
   In recent years, there has been growing interest for the hypothesis that the ability to encode multiple interpretations in an underspecified language may be (part of) the explanation of the Combinatorial Explosion Puzzle.
 The idea is that humans, as well, make use of an underspecified language that can encode distinct meanings implicitly, and therefore do not need to generate all of these meanings.
 A semantically ambiguous sentence, therefore, need not cause problems for a human to process, because it is not necessarily perceived as ambiguous in the sense discussed in the previous section.
 I will call this assumption the Underspecification Hypothesis:   Underspecification Hypothesis : Human beings represent semantic ambiguity implicitly by means of underspecified representations that leave some aspects of interpretation unresolved.
 My goal in the rest of the paper is to spell out the Underspecification Hypothesis both as a theory of grammar and as a theory of discourse interpretation.
 I assume, that is, that the hypothesis is correct, and try to answer questions such as: what kind of language are underspecified representations? what is their semantics? and, what kind of inferences are done with them?   The novel aspect of this work is that the answers I give are based on the discussion of semantic ambiguity and perceived ambiguity in the previous section.
 I hypothesize that underspecified representations are used by humans as the translation of expressions that are indefinite in the sense of Pinkal, and assign them a semantics that reflects this hypothesis.
 I assume that the disambiguation process is consists of defeasible inferences, and examine the characteristics of defeasible reasoning with underspecified representations.
 Although the same position towards disambiguation and defeasible has been adopted in the Core Language Engine, most of the issues I discuss have not been mentioned so far in the discussion on underspecified representations.
   In the literature on underspecification, one often finds the argument that providing a semantics to underspecified representations is necessary because disambiguation requires inference, and therefore a `logic of underspecification' is needed .
 However, it is not at all clear whether the process of disambiguation involves much semantically justified reasoning; disambiguation seems to consist mostly of defeasible inferences.
 It is fair to say that the debate on this issue is very open at the moment, as certified by a number of recent panels on the subject.
 But whatever the final conclusion on this topic will be, it is clear that under the perspective that the grammar of a language is a mapping from elements of to underspecified representations, the semantics of these underspecified representations becomes a central aspect of the specification of the grammar.
 Furthermore, it also becomes clear that the semantics of underspecified representations must be based on an analysis of semantic ambiguity, otherwise we wouldn't even know whether the form of underspecified representation we develop does the job it is supposed to do.
   The simplest way to illustrate my implementation of the Underspecification Hypothesis is to start with lexical ambiguity.
 I present in this section a theory of grammar which makes use of an underspecified language to encode the `ambiguity potential' of lexically ambiguous expressions, as well as a simple formalization of lexical disambiguation as defeasible inference over underspecified representations.
 In the next section I will show how to extend the approach presented here to deal with expressions that exhibit other forms of semantic ambiguity.
   I want to emphasize that I start with lexical disambiguation for expository purposes only.
 Lexical ambiguity is the one case of ambiguity for which a `generate and test' strategy may well be compatible with the psychological results, therefore the one for which the need for underspecified representations is less clear.
 Furthermore, I will only discuss cases of lexical ambiguity in the narrow sense, which is perhaps the least interesting case of lexical indefiniteness.
 Discussing lexical disambiguation, however, is the simplest way to explain how underspecified representations can be given a semantics related to Pinkal's proposals about ambiguity, and how to defeasible reasoning with underspecified representations.
 In the next sections I will generalize the approach introduced here to cases of ambiguity for which the underspecified approach is much more plausible.
 Furthermore, at least one theory of lexical disambiguation, Hirst's proposal , makes use of `Polaroid words' which are essentially underspecified interpretations of lexical items.
   The presentation of a lexically underspecified grammar below is centered on the example of (H-type) lexical ambiguity discussed above, the verb croak, which can take two precisifications.
 Let be the language which consists of the single sentence Kermit croaked.
 This sentence is H-type ambiguous because it admits of two precisifications and it is subject to the precisification imperative.
 A `Montagovian' grammar MG would map (syntactic analyses of) the sentence into distinct expressions of a `disambiguated language' , each of which denotes a function from discourse situations into intensional objects of the appropriate type (in this case, propositions).
 A grammar UHG that subscribes to the Underspecification Hypothesis, on the other hand, maps syntactic analyses of expressions of into a single expression of a `lexically underspecified language' .
 The semantics of is based on the Precisification Principle: expressions of denote at each discourse situation a set of senses of the type they would be assigned by a Montagovian grammar.
   The lexically underspecified language has the following ingredients:     Note that in addition to two predicates croak1 and croak2, corresponding to the disambiguated senses of croak, the language includes an `underspecified' predicate croakU.
 The interpretation function for , , is defined as follows.
 Let M = UF be a model just like the one that would be used for a disambiguated language .
 The interpretation function assigns to an expression of a value with respect to M and a discourse situation d.
 The language has been deliberately kept simple to make it clear that the underspecified languages I propose have two basic properties: (i) the value of an expression at a discourse situation is a set of senses of the type that a sense of that expression would have in a disambiguated language ; and (ii) expressions can be divided into expressions whose denotation at a discourse situation is a singleton set, such as k or croak1, and expressions such as croakU that denote a non-singleton set.
 The latter expressions provide the interpretation for ambiguous expressions of .
   The clauses for application and the connectives show how ambiguity `percolates up' from lexical items.
 The value of an expression like () is obtained by taking the cross-product of the values of and , and it includes one function f per distinct pair of functions 11 in the denotations of and .
 The value assigned by the function f to the situation s is defined by applying a certain operation (in this case, application) to the values assigned to s by the functions and .
 Thus, if both the denotation of and the denotation of are singleton sets, the denotation of () is also a singleton set; otherwise, ambiguity `multiplies,' as it where.
 The same `multiplication' technique is also used to define the denotation of connectives.
   The following grammar generates an underspecified representation of Kermit croaked by mapping the semantically ambiguous predicate croaked into an `ambiguous' predicate of as follows:   The underspecified translation of Kermit croaked in , croakU(k), denotes a set of two propositions at a situation d: the function that assigns 1 to a situation iff Kermit produced a frog-like sound in that situation, and the function that assigns 1 to a situation iff Kermit died in that situation.
 This makes the sentence indefinite in Pinkal's sense.
 By contrast, an indeterminate sentence such as Kermit is the Ruritanian secretary of state would have a single sense at a given discourse situation.
   Pinkal's Precisification Imperative is an attempt at making more precise the observation that human beings don't seem to have good intuitions concerning what follows from a H-type ambiguous sentence.
 Even when subjects are able to pass judgments about what follows from an ambiguous sentence, it's arguable that they do not give judgments concerning what follows from the underspecified representation: rather, they first generate one interpretation, then decide what follows from that.
 The conclusion that I would be inclined to draw is that a relation of semantic entailment capturing human intuitions can only be defined, if at all, between expressions whose interpretation is not subject to the Precisification Imperative.
 So, although it would be possible to define, for example, a `strong' notion of entailment as what follows from all senses, this definition would be rather artificial.
 For this reason I will not attempt to define a notion of entailment between expressions of ; the readers interested in the issue are referred to Pinkal's book and to the discussion in van Deemter's dissertation .
   Discourse Interpretation and Perceived Ambiguity   A theory of ambiguity processing solves the Combinatorial Explosion Puzzle if it does not require that all distinct interpretations of a semantically ambiguous sentence are actually generated.
 A grammar consistent with the Underspecification Hypothesis such as the one just discussed moves us one step towards that goal, since it only imposes the constraint that a single underspecified interpretation be generated.
   On the other hand, we can conclude from the discussion of deliberate ambiguity and of the psychological work on ambiguity that a psychologically plausible theory of ambiguity must also predict that more than one interpretation may become available in a given context, although the number of such interpretations will in general be much smaller than the number of possible semantic interpretations.
   As discussed above, the view of discourse interpretation that I am going to take is the one typically found in the AI literature, according to which disambiguation involves the generation of (possibly distinct) hypotheses in parallel by means of defeasible inference.
 This perspective is found, for example, in the work on abductive discourse interpretation by Hobbs and colleagues , in the work on Bayesian disambiguation by, e.g., Charniak and his students and in the work on DICE and discourse interpretation by Asher, Lascarides, and Oberlander .
 Some of the formal models of defeasible reasoning that can be used to formalize the situation in which conflicting hypotheses are generated include Reiter's default logic , the abductive model , Bayesian Nets, and DICE ,].
   As a model of defeasible reasoning, I adopt Reiter's Default Logic.
 In default logic, the process that generates defeasible hypotheses is seen as the computation of the extensions of a default theory (D,W) where D is a set of default inference rules and W is a set of formulas.
 I will formalize discourse interpretation as the process of generating the extensions of the theory (DI,UF), where DI--the Discourse Interpretation Principles--are default inference rules, and UF is a set of expressions of an underspecified language like .
 Let us ignore for the moment the fact that the formulas in UF are underspecified representations.
 The Discourse Interpretation Principles formalize the defeasible inferences that take place in discourse interpretation, such as disambiguating inferences.
 These rules are operations that map a set of wffs that allow of a certain number of interpretations into a new set of wffs with a more restricted number of interpretations.
 An example of Discourse Interpretation Principle is the following:   This inference rule reads: if the set of wffs UF includes the fact that the object x has the property croakU and the property frog, and if it is consistent to assume that the interpretation croak1 of croakU was intended, then the inference rule CROAK1-IF-FROG produces a new set of wffs that includes the fact croak1(x).
 The application of CROAK1-IF-HUMAN-LIKE would be blocked by the presence in UF of the wff croak1k.
 Using Reiter's definition of extension in an `intuitive' fashion, we can see that the default theory   has the following (unique) extension:   The denotation of a set of wffs {1 ...n} will be defined as the denotation of the conjunction 1 ...n of these wffs.
 I also assume that the empty set of wffs denotes the function TRUE that is true at every situation.
 With this definition, and under the assumption that each `unambiguous' interpretation of the word croak is incompatible with the others (i.e., under the assumption that croak1x croak2x), the extension of (DF,UI) admits of only one denotation, the one under which the denotation of k produced a sound like the one frogs produce.
   A default theory always has an extension as long as all defaults are normal, but it may have more than one extension if the set of Discourse Interpretation Principles contains two inference rules that both apply but generate a conflict.
 Consider, for example, the default theory consisting of a set of discourse interpretation principles DI that includes, in addition to CROAK1-IF-FROG, a second discourse interpretation principle (let's call it CROAK2-IF-HUMAN-LIKE) stating that the croak2 interpretation is plausible for human-like beings; and of a set of wffs UF including the fact that Kermit is a human-like being.
   this theory would have two extensions:     Perceived ambiguity can now be redefined more precisely as the state that obtains when the default theory `encoding' the listener's discourse interpretation processes has more than one extension; and the cases of deliberate ambiguity discussed in section ambiguity_section can be formalized as cases in which the speaker has `reasoned about the other agent's reasoning,' as it were.
   Once we start allowing discourse interpretation processes like those just discussed, the Underspecification Hypothesis is not sufficient to explain the Combinatorial Explosion Puzzle anymore.
 The UH does not rule out a theory of discourse interpretation in which after an underspecified interpretation has been obtained, all possible senses of a sentence are generated.
 In fact, a lot of systems work this way, as well as interpretation procedures such as Hobbs and Shieber's scoping algorithm .
 In the framework for discourse interpretation just presented, theories of this kind could be formalized by including discourse interpretation principles that generate all the semantically justified interpretations at random.
 For the case of lexical disambiguation, for example, we could have a theory that includes the two following inference rules:     A theory of lexical disambiguation of this kind would simply produce all semantically justified interpretations of a sentence, and the Combinatorial Explosion Puzzle would remain a puzzle.
 To solve the puzzle, a theory of disambiguation must therefore supplement the Underspecification Hypothesis with constraints on discourse interpretation that ensure that only a few extensions are generated.
   The constraints need not be the same for all classes of ambiguity.
 For certain classes of ambiguity, including perhaps lexical ambiguity, the explanation may simply be that the disambiguation process is incremental, i.e., it takes place as the text is processed word by word or constituent by constituent, and each ambiguity is resolved locally; in this way, only a small number of alternative hypotheses have to be considered every time.
 For other classes of ambiguity, however, such as scopal ambiguity and referential ambiguity, incremental processing does not seem to be the solution, and different constraints must apply.
 In , the following constraint was proposed:   Anti-Random Hypothesis (Informal) Humans do not randomly generate alternative interpretations of an ambiguous sentence; only those few interpretations are obtained that (i) are consistent with syntactic and semantic constraints and (ii) are suggested by the context.
 The Anti-Random Hypothesis should be thought of as a `meta-constraint' on theories of interpretation: if we intend to account for the Combinatorial Explosion Puzzle, we have to develop theories of interpretation (e.g., theories of parsing, or theories of definite description interpretation) that satisfy this constraint, i.e., in which discourse interpretation principles like CROAK1-AT-RANDOM and CROAK2-AT-RANDOM are not allowed.
   In order to illustrate more concretely the difference between theories of discourse interpretation that satisfy the Anti-Random Hypothesis, and theories that do not, let us consider how one could formalize a theory of pronominal interpretation.
 A `random' theory of pronoun interpretation would go as follows: first, compute all possible antecedents of the pronoun in the discourse.
 Then, generate an hypothesis for each of them, stating that the pronoun refers to that antecedent.
 Finally, rank these hypotheses according to their plausibility.
 A random hypothesis generation process usually leaves the task of choosing one hypothesis to plan recognition; the problem is that most often, the alternatives are equally plausible.
   In contrast, centering theory is an example of non-random pronoun interpretation theory.
 According to centering theory, each utterance establishes a `backward looking center' (Cb), and a pronoun is by default interpreted to refer to the Cb.
 (I am glossing over a number of complexities here.) Such a theory would generate a single (or a few) hypothesis concerning the antecedent of a pronoun; the other possibilities, although semantically possible, would simply never come up.
 Examples of theories of definite description interpretation, tense interpretation, the interpretation of modals in discourse, and scope disambiguation that satisfy the Anti-Random Hypothesis are discussed in .
   The Anti-Random Hypothesis can be made more formal in the framework for discourse interpretation adopted here by introducing a slightly different syntax for default inference rules, one in which the underspecified condition is syntactically separated from additional contextual requirements such as the requirement in CROAK1-IF-FROG that the object in question be a frog:   Except for the fact that one of the prerequisite wffs is `singled out', an inference rule thus rewritten has the same interpretation as one of Reiter's default rules.
 We can then require the contextual requirements to be non-trivial (i.e., not satisfied in every situation) as follows:   Anti-Random Hypothesis A discourse interpretation theory (DI,UF) is Anti-Random iff for all discourse interpretation principles ::/ in DI, is not satisfied in every situation.
   The framework just introduced can also be used to formalize the `second order' aspects of Pinkal's theory, such as the Precisification Imperative.
 The Precisification Imperative can be seen as imposing a constraint on the extensions of a discourse interpretation theory, namely, as the requirement that extensions include a `disambiguating wff' like croak1k for each H-type ambiguous constituent of the set UF such as croakUk.
 I will call this constraint Condition on Discourse Interpretation.
 In first instance, the Condition on Discourse Interpretation might be formulated as follows, for the case of lexical ambiguity:   Condition on Discourse Interpretation (Preliminary): Each extension E of a discourse interpretation theory (DI,UF) must include, for each literal L in UF whose predicate is H-type ambiguous, a distinct disambiguating literal, i.e., a literal whose denotation is a single function among those in the denotation of L.
 The definition of the Condition on Discourse Interpretation just given is not very general: it depends on the assumption that all cases of H-type ambiguity are originated by predicates.
 A simpler, and more general, formulation of the Condition on Discourse Interpretation can be obtained by generalizing the format for the discourse interpretation principles once more.
   Default inference rules are typically used to augment a set of wffs with additional facts inferred by default: the fact that a particular bird flies, for example.
 But the purpose of discourse interpretation rules used for disambiguation, like CROAK1-IF-FROG, is to restrict the interpretation by eliminating certain readings.
 In this perspective, leaving the underspecified wffs around doesn't make much sense.
 I propose therefore to allow discourse interpretation principles to rewrite their `triggering wff' whenever this wff encodes an H-type ambiguity, in addition to adding new wffs to a set.
 The more general format for discourse interpretation principles is as follows:   A rule of this form is an operation from sets of wffs into sets of wffs that given a set W of wffs containing and and not containing , produces a set W of wffs containing , and in which has been replaced by .
 I will call the triggering condition.
 For example, a version of CROAK1-IF-FROG in which the triggering condition croakUx is rewritten by the consequent croak1x is as follows:   If all disambiguation rules are rewritten in this format, a completely disambiguated extension can simply be characterized as one which doesn't contain any H-type ambiguous wffs.
 The notion of H-type ambiguous wff can be characterized either syntactically (by identifying certain syntactic constituents as specifying H-type ambiguity, and by classifying as H-type ambiguous a wff that contains one of these constituents) or model-theoretically, e.g., by means of a function such that if X is a set of senses, (X) is 1 if the set of senses is admissible, 0 if it is inadmissible in Pinkal's sense.
 Whatever way we choose to define a H-type ambiguous wff, the Condition on Discourse Interpretation can now be formulated as follows:   Condition on Discourse Interpretation : An extension E of a discourse interpretation theory (DI,UF) cannot contain an H-type ambiguous wff.
 Notice that the statement of the Condition on Discourse Interpretation as a condition on pragmatic reasoning gives it the status of a felicity condition rather than of a hard constraint on interpretation.
   So far, I've been using the terminology from default logic as if the shift to an underspecified representation had no side effects, but this is not the case.
 Consider the way in which Reiter defines the notion of extension of a (closed) default theory, for example:   This definition crucially relies on the notion of deductive closure Th(S), defined as the set of wffs {w | S w}; but what we said with semantic entailment holds for provability, as well: no clear notion exists of what it means for an expression of an underspecified language to follow from a set of wffs of the same language.
 Two routes are open to us.
 One is to define a notion of `underspecified provability' , and to use to define an `underspecified' notion of closure ThU(S).
 For example, we could say that w w iff for each expression w that denotes a single one of the interpretations of w, w w.
 This route is not very appealing, however, if for no other reason that it's not clear that any way of defining an underspecified notion of provability will do.
   The alternative is to adopt a new notion of extension that does not rely on deductive closure, i.e., one in which an extension is a fixed point of the operator , which does not include condition D2 of the definition of :   Replacing with in the definition of extension has several consequences.
 First and foremost, dropping the requirement of deductive closure makes the test of whether it is consistent to assume 1, ..., m essentially syntactic: i.e., it is possible for j not to be included in (S) even though it is derivable from (S).
 (In general, this definition of extension is a much closer description of the behavior of actual implementations of non-monotonic reasoning than the original definition.) And therefore, a logic defined in this way does not have the property of Reiter's logic that a (closed) default theory (D,W) has an inconsistent extension iff W is inconsistent.
   In fact, each extension of a discourse interpretation theory under this new definition will, in general, be H-type ambiguous, some of the interpretations being inconsistent.
 However, if we adopt the `rewriting' version of disambiguation discussed above, and impose the Condition on Discourse Interpretation, each extension will have a single interpretation, and therefore its consistency can be checked.
 I propose therefore to define the notion of extension of a discourse interpretation theory as follows:   Extension: A set of closed wffs E L is an extension for the discourse interpretation theory iff E is a fixed point of the operator and satisfies the Condition on Discourse Interpretation.
   The theory of ambiguity introduced in the previous sections can be straightforwardly extended to obtain a treatment of two other classes of semantic ambiguity: scopal ambiguity and referential ambiguity.
 These extensions preserve the basic ideas of the theory, semantic ambiguity as multiplicity of meanings, and perceived ambiguity as multiple extensions of a default theory; what changes is that on the one hand, a more complex underspecified language is introduced, capable of encoding other forms of ambiguity; on the other hand, more complex inference rules are used.
   I will call the sentence constituents that modify the parameters of evaluation, and therefore affect the interpretation of other sentence constituents `in their scope', operators.
 Examples of operators are quantifiers (that affect the choice of the variable assignment used to evaluate expressions in their scope) and modals (that affect the choice of the world / situation at which expressions in their scope are evaluated).
 As it is well-known, one cause of semantic ambiguity is that sentences may contain more than one operator, and their relative scope is not completely determined by the sentence's syntactic structure.
 Sentences that have more than one meaning due to the interaction between operators are called scopally ambiguous.
   Historically, most underspecified representations have been introduced to deal with scopal ambiguity.
 Typically, an intermediate step of processing is assumed in which operators are left `in place,' as well as a subsequent step of processing in which their relative scope is determined by contextual processing.
 Schubert and Pelletier's underspecified representation of Every kid climbed a tree in is an example of underspecified representation in which the operators are left `in situ'.
   These representations are typically justified in terms of ease of processing, and their ability to represent `intermediate' readings.
 It is clear however that for the purposes of developing a `principled' theory of ambiguity processing, it would be much better to stick to as few new `levels of representation' as possible.
   In fact, there is no need to introduce a new level of representation.
 The two requirements on a scopally underspecified representation--that it allow representing the structural information provided by a sentence, and representing the intermediate steps of disambiguation--can be satisfied by using as an underspecified representation the syntactic structure of the sentence, augmented with information about the semantic interpretation of word-forms.
 In this way we can also maintain semantic translation of lexical items used in Montague grammar, that determine how they combine with other sentence constituents to determine a sentence's meaning.
   The `lexically and scopally underspecified language' I introduce to encode scopal ambiguity generalizes the language introduced in the previous section by allowing for arbitrary functional types.
 In this way, the lexical item every can be given its usual etett translation: The second augmentation to is the inclusion of tree-like expressions used to translate syntactic phrases.
 For example, the NPDetevery Ndog translates into the expression:   The expression in is the underspecified translation of the sentence Every dog saw a frog:   Besides reducing the number of representations floating around, this was of talking about scopal underspecification has two additional advantages over underspecified representations in which all syntactic information except for the position of operators is lost, such as Schubert and Pelletier's underspecified logical forms, Reyle's underspecified s or the Core Language Engine's s.
 First of all, the semantics of expressions such as --that, for historical reasons, I call logical forms-- can be computed in a completely classical fashion using the storage mechanism , with the result that syntactic constraints on the available readings, such as the Scope Constraint , can play a role in determining the semantics of these objects, without the need for additional constraints such as the label ordering constraints used in UDRS .
 Secondly, all structural information is preserved, not just information about the relative position of operators.
 Some of this syntactic information is used as a clue during disambiguation, for example, for interpreting pronouns, but also in certain theories of scopal disambiguation.
 (See, e.g., [] and for an account of scope disambiguation which esploits the syntactic information encoded by underspecified expressions such as .)   The semantics of the underspecified language is classically based on a set of semantic types, the smallest set such that (i) e and t are types; and (ii) If and are types, is a type.
 The set of meaningful expressions of type is indicated by ME .
 The set of non-logical constant expressions of type is indicated as CE ME .
   The semantics of is based on the same idea as the semantics of .
 Natural language expressions are assigned objects of the same type that they would receive in (as revised by Partee and Rooth parteero ), with the difference that, when I talk about `meaningful expressions of type ' below, therefore, I am really talking about expressions that denote sets of functions from the set of situations to elements of (the domain of type ).
 Thus for example, sentences are of type t both in Dowty, Wall and Peters' system, and in the current proposal; but a meaningful expression of type t in denotes a (function from a discourse situation to a) set of functions from situations to truth values.
 Or to make another example, relations have the same type eet here that they have in Dowty, Wall and Peters, but meaningful expressions of type eet now denote sets of functions from to eet.
   The sets of meaningful expressions of include all the expressions in : The set ME , for any type , includes a denumerably infinite set of variables of type .
 also includes lambda-abstracts and quantified expressions, defined below.
 The language also includes the new syntactic category of logical forms.
 The sets of logical forms of syntactic category XP, LFXP, are defined as follows: Meaningful expressions are assigned a value with respect to a universe .
 I use below the notation s to indicate that a `stands for' an object in , i.e., it is part of the metalanguage, as opposed to being a meaningful expression of the object language.
 The models with respect to which a expression is evaluated include a set of situations.
 The only fact about situations I use here is that they have constituents.
   The interpretation of types with respect to is defined as usual: De,U = ; Dt,U = {0,1}; D = Db[Da].
 In the rest of this paper, I generally drop the indication of the universe (e.g., I write De instead of De,U).
 The model of interpretation for expressions is the triple , , I.
 The interpretation function `I' assigns an interpretation to constants of type .
   The value of meaningful expressions is specified by a function .
 that includes an assignment function among its parameters, since the terms of include variables.
 The interpretation of variables is specified by the following clause:   The interpretation of constants, connectives and application is as in .
 The denotation of the other expressions is discussed below.
   The following grammar extends the grammar discussed in section lexamb_section by adding determiners and relations as new lexical items: and by adding phrase structure rules for s and transitive verbs:   This grammar generates, in addition to lexically ambiguous sentences such as Kermit croaked, scopally ambiguous sentences such as Every dog saw a frog.
   The denotation of logical forms is specified using the storage method, developed by Robin Cooper Cooper, R.
 as a way around a problem with Montague's quantifying in technique, namely, the fact that in order to get all the readings of a scopally ambiguous sentence, one has to stipulate that the sentence is syntactically ambiguous (see ).
   Cooper proposed that the value of a syntactic tree is a set of sequences, each sequence representing a distinct `order of application' of the operators that may result in a admissible interpretation of a sentence.
 For example, the quantifier a frog can `enter' the derivation of the saw a frog in two different ways.
 The narrow scope reading is obtained by immediately applying the interpretation of the quantifier to the translation of saw; but it is also possible to apply the predicate to the variable quantified over, and `wait' before applying the quantifier, in which case the wide scope reading is obtained.
 The value of the a frog, then, is the set of two sequences shown in .
 One sequence consists of a single element, the `traditional' Montague-style translation of every frog.
 The second sequence consists of two elements: the variable y, and the semantic translation of the quantified , put `in storage'.
   Ambiguity `propagates up' as follows.
 The value of the saw a frog in csex-a also consists of two sequences, one obtained by applying the first element of the first sequence in the denotation of every frog to the predicate saw, the other obtained by applying the predicate saw to the first element of the second sequence (the variable y).
 The result is as in .
   Finally, the value of a sentence is obtained by combining the value of the with the value of the in the usual fashion: the value of SEvery dog saw a frog is a set of two sequences, each representing a distinct reading of the sentence.
   It's easy to see that Cooper's technique can be used to assign to underspecified representations like a `multiple sense' denotation like those assigned to lexically ambiguous expressions in the previous section.
 All that is needed is a function CV that assigns to each expression of the form XP its `Cooper Value'; the denotation of sentence translations like S can then be defined in terms of CV as follows:   (I have taken into account the fact that an expression of our underspecified language denotes a set of objects, therefore each scopally disambiguated translation of a sentence will still denote a set of propositions.)   Cooper discusses in detail in how semantic and syntactic constraints on scope can be implemented as requirements that the storage be `discharged' at certain positions--, that no element in storage be `carried across' syntactic constructions that produce scope islands, such as S.
 In this way, no operator in a clause may take scope over operators in an higher clause, or in a sister clause, thus enforcing the Scope Constraint discussed in syn_sem_constr_section.
   The CV function used to define the interpretation of logical forms is based on an implementation of the storage idea less general than Cooper's, but simpler.
 In order to arrive at a uniform specification of the Cooper Value of all constructs, it is useful to define construct-specific versions of application in which to `bury' the differences in storage manipulation.
 These operations are defined as follows:   Next, we need an operation that combines two sets of sequences into one.
 The result of applying this operation to two sets of sequences X and Y is the set of sequences obtained by (typed) applying the first element of a sequence in X to the first element of a sequence in Y and then merging the rest of the sequences, as follows:   We also need an operation to put operators into store, and one to `discharge' them.
 The operation takes a set consisting a single single-element sequence and a result, and returns a set that consists of two sequences: the original sequence, and a new sequence consisting of the result and the operator in store.
   The operation takes a sequence and applies all operators back to obtain a set of sequences with a single element and an empty store.
 For simplicity, we will assume that all operators are generalised quantifiers, i.e., of type ett.
 (No other operators are specified in the grammar above.) is defined as follows:     [*](X), where X is a set of sequences, is the union (x).
 We can now specify the Cooper value of logical forms with respect to model M, variable assignment g, and discourse situation d as follows:   There are three tricky aspects to the definition of CV: the discharge operation in the definition of the Cooper Value of a sentence translation, the definition of CV(NPDet N) in which an operator is put in store, and the definition of CV(VPV NP in which two stores are combined, and that has different results depending on whether the is of type e or is a quantifier.
 I'll illustrate these cases by looking at the main steps of the computation of the CV of :   The Scope Constraint is enforced by requiring a complete discharge at the sentential level, which means no operators can `move up' outside the sentence in which it occurs, although of course this couldn't occur in this grammar since it doesn't cover relative clauses, sentential complements or coordination.
 I have assumed that discharge only takes place at sentential level, i.e., there are no operators taking scope over s; doing this would complicate matters a bit in that a `partial' discharge operation should be defined.
   Some care is required in the system developed here to get a semantics for lambda-abstraction that preserves properties such as - and -reduction.
 The clause specifying the denotation of lambda-abstraction in Dowty, Wall and Peters's book is the following:   If we generalize this clause in the `obvious' way we get:   Lambda-abstraction defined in this way does not have the required properties.
 To show that it does not preserve -reduction, it is sufficient to consider the following example: let = {s1, s2}, = {a,b}, and let the expression of type have the following denotation:   Then ()M,g{/a},d is as follows: and ()M,g{/b},dis as follows:   Then, under the definition above, () will contain the following function, that is not part of the denotation of (hence, -reduction is not a sound inference rule):   Intuitively, the problem with the definition above is that it does not `preserve' the functions in the denotation of .
 A definition of lambda-abstraction that does preserve these functions, and therefore preserves the soundness of - and -reduction, can be obtained as follows.
   The denotation function used so far assigns a value to expression ME in model M with respect to the parameters of evaluation g and d, .
 Another way of specifying the value of expressions is to define a function that assigns as value to at discourse situation d a set of functions of type (Ass ( )), from assignments to functions in ( ).
 For example, Dowty, Wall and Peters' clause for lambda abstraction could be rewritten as follows:   This definition can then be generalized as follows:   Lambda-abstraction defined this way does support -reduction.
 Since this more general way of assigning a value is not needed to provide a semantics for the other constructs of , I will continue using a function ., but the reader should keep in mind that a denotation function of this form is needed to deal with lambda abstraction, hence, with quantification.
 (And for referential ambiguity, as we will see below.)   The treatment of quantifiers in is based on Generalized Quantifiers Theory , i.e., the idea that determiners denote relations between two sets.
 The `restricted quantification' notation used in the examples above is defined in terms of the two determiners every and a, as follows:   A `single-valued' semantics for every(,) could be defined, in first approximation, as in the following clause:   This definition can be generalized as follows into one that works in the case in which .
 is a set:   The interpretation of expressions of the form a(,) is defined in a similar fashion, with the obvious semantics.
   Having extended the language into one that can be used to describe scopal underspecification, the framework for discourse interpretation developed in section disc_int_section can also be used to formalize the inferences involved in scope disambiguation.
 Partially disambiguated interpretations can be represented by expressions which mix logical forms with `traditional' expressions, as done in .
 For example, one could formalize Ioup's Grammatical Function Principle, stating that an in subject position by default takes scope over s in other position, as follows:   Logical forms in LFS are sentential expressions, and can therefore serve as triggering condition of discourse interpretation principles.
 They can also occur embedded in other expressions of .
 During the scope disambiguation process, `less ambiguous' expressions are inferred by deriving expressions such as ypySNPy VP in which some quantifiers have been extracted, by a process very similar to the one used in the top-down version of the construction algorithm [].
 `Partial' scopal disambiguation is thus represented by expressions which still contain logical forms.
   As some readers will have already observed, the rule GRAMMATICAL-FUNCTION-PRINCIPLE does not satisfy the Anti-Random restriction proposed in disc_int_section: the rule does not contain a non-trivial restriction on the contexts in which it can operate.
 The already mentioned proposal in overcomes this problem by making the activation of scope disambiguation rules depend on whether the appropriate domain for the quantifier (its resource situation) has been identified; a presentation of that proposal would however require introducing too much additional material.
   Yet another way in which the semantics of sentences is `underspecified' by their syntax is in the interpretation of anaphoric expressions and other expressions whose interpretation has to be fixed in context.
 In semantics, referential expressions are traditionally translated as free variables whose interpretation depends on the choice of an assignment function (for the cases of deictic anaphora) or by assigning them the same variable bound by the quantifier that serves as their antecedent (for the cases of bound anaphora).
 This translation does capture the intuition that the truth conditions of a sentence containing a referential expression can only be evaluated after fixing the value of the referential expressions.
 It is also clear, however, that distinct propositions are obtained depending on the value assigned to these expressions, much as distinct propositions are obtained depending on the choice of an interpretation for lexical items, or of a scope for operators: in other words, a sentence which includes a referential expression is semantically ambiguous much in the way a sentence containing a lexically ambiguous item is.
   A complete discussion of reference interpretation would require introducing a formalization of context, so I will only consider here the issue of providing an underspecified treatment of intra-clausal and deictic anaphora.
 I propose that referential expressions are cases of semantic ambiguity, and translate into a special kind of underspecified object that I will call parameters.
 Semantically, a parameter is a type e expression that, in a discourse situation d, denotes a set of functions from situations to elements of e in d.
 For example, the pronoun he would translate into a parameter x which, in a discourse situation d with constituents a1 ...an, and given the set of situations, will denote a set of functions {f1, ..., fm, ...} from situations in to a1 ...an, including at least the set of all constant functions that map each situation s into aj if aj is a constituent of that situation (see below), and the set of all variable denotations.
 The reader will immediately realize that parameters are the equivalent for type e expressions of `underspecified predicates' like croakU introduced above.
   More formally, I propose to extend the set of terms of with a new class of parameters, whose interpretation is defined as follows.
 First of all, let us reformulate the semantics of variables given before, and make variables functions from assignments to values (rather than the other way around).
 This involves again using as interpretation function one that maps expressions into functions from assignments to meanings, as done for lambda-abstracts.
   This definition of the meaning of a variable allows us to abstract away from assignments.
 We can now define the semantics of parameters as follows: For example, if the subset of e in d consists of the two atoms j and b, then xe = {f1,f2,...fi,...}, where f1, f2 etc.
 are the functions that may serve as the denotation of constants and variables--f1 is the function that maps each situation of which j is a constituent into j, f2 is the function that maps each situation of which b is a constituent into b-- and the other functions represent all the possible denotations of objects that the parameter may be resolved to.
 Note that the discourse situations plays here the role played by the variable assignment in `free variable' theories of context dependence.
   The grammar presented in the previous section can be straightforwardly extended as follows to generate sentences such as It croaked:   The definition of the interpretation of logical forms given above already gives the correct results for these cases.
   Referential ambiguity gets `resolved' by anchoring a parameter.
 A parameter is anchored if only one among the functions in its denotation results in a consistent interpretation of the set of sentences in which the parameter occurs; a parameter can be anchored by means of equality statements of the form =xa, where a is not parametric, or is already anchored: such equality statements make all but one of the interpretations of the parameter inadmissible.
 Once a parameter is anchored, it can be `replaced' by a term that denotes the one function among those in the interpretation of the parameter that does not result in an inconsistent interpretation, much as in the previous discussion of lexical disambiguation, an H-type ambiguous predicate could be replaced by a disambiguated version.
 So, the discourse interpretation principles formalizing pronoun disambiguation involve a rewriting operation, just as the discourse interpretation principles formalizing lexical disambiguation.
   An apparent disadvantage of the present theory with respect to the `free variable' theory of context dependence is that we can derive from the latter that the value of referential expressions has to be fixed in order to get the meaning of the sentence in which they occur.
 A conversation is infelicitous unless the referents of all pronouns and definite descriptions have been identified, the domain of quantification of all quantifiers has been appropriately restricted, and so forth: so much so that listeners appear to be ready to accomodate new information (e.g., to introduce into the discourse some otherwise unspecified antecedent for a pronoun) rather than leave the interpretation unspecified .
 But this fact about referential expressions also follows if we treat context dependence as a case of (H-type) semantic ambiguity; it is just a corollary of Pinkal's precisification imperative, from which I derived the Condition on Discourse Interpretation in section lexamb_section.
 Accomodation procedures can then be seen as a way of `precisifying' in lack of sufficient information.
   The one case of ambiguity that requires extending the framework introduced here considerably is syntactic ambiguity, as in They saw her duck.
 Furthermore, I haven't considered the problem of structural disambiguation in any detail.
 I refer the interested readers to , for a sketchy discussion of how to encode encoding syntactic ambiguity in an underspecified representation.
   I have suggested that to develop a theory of discourse interpretation that is consistent with what we know about the problem of ambiguity, we need to look both at the grammar and at discourse interpretation.
 I proposed a theory of grammar consistent with what I have called the Underspecification Hypothesis and which is not based on the assumption that all natural language expressions can be disambiguated; and a theory of discourse interpretation according to which a perceived ambiguity occurs when defeasible interpretation principles result in conflicting hypothesis.
 The interpretation process is subject to two constraints: the Anti-Random Hypothesis (interpretations are not generated at random) and the Condition on Discourse Interpretation, derived from the Precisification Imperative (H-type ambiguity has to be resolved).
 Although treatments of disambiguation based on defeasible reasoning have been proposed elsewhere in the literature (e.g., in ), I am not aware of any discussion of the characteristics of this inferential process, the consequences of reasoning with an underspecified representation, or the need for constraints on the inference rules.
   In the theory, semantic ambiguity is characterized model-theoretically in terms of multiplicity of sense, whereas perceived ambiguity is characterized in terms of inference.
 One may wonder if the distinction is really necessary; i.e., if it is really the case that the meaning of natural language expressions can be specified a priori.
 Two arguments in favor of a distinction are that it provides for a clean distinction between the role of grammar and the role of discourse interpretation; and that perceived ambiguity may also reflect non-semantic distinctions, e.g., distinctions in speech act interpretation; this question is not however totally resolved in the paper.
   There are two obvious directions in which the present model needs to extended: to provide a model of syntactic ambiguity, and to account for the effect of incrementality in sentence processing.
 Preliminary work in this direction is discussed in .
   An issue that deserves further inspection is whether the formal similarity between the system used here to assign a denotation to indefinite sentences, and the systems developed by Hamblin for dealing with questions and by Rooth for its alternative semantics has some significance.
 In particular, it would be interesting to explore the consequences of using parameters as the translation of focused elements.
 