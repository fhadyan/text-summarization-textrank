 The second method has the advantage of drastically reducing the number of model parameters and thereby reducing the sparse data problem; there is more data per group than per word, thus estimates are more precise..  Therefore, starting with an n-gram model yields a model that is at most equivalent to one that is generated when starting with the trivial model, and that can be much worse..  We want the constraints to be as week as possible to allow the maximal number of solutions but at the same time the number of merges must be manageable by the system used for computation (a SparcServer1000 with 250MB main memory .  Thus, in addition to finding a model with lower log perplexity than the bigram model (2.26 vs..  The bigram model assigns a log perplexity of 2.78, the merged model with 113 states assigns a log perplexity of 2.41 (see table .  This yields a bigram model..  Instead of starting with the trivial model, one can start with a smaller, easy-to-produce model, but one has to ensure that its size is still larger than the optimal model.. 