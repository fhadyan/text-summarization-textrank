 Stolcke and Omohundro start with a first order HMM where every state represents a single occurrence of a word in a corpus, and the goal is to maximize the a posteriori probability of the model..  All parts are mutually disjunct..  In the third experiment, part A was used for trigram training, part B for clustering and part C for testing..  In the fourth experiment, part A was used for trigram training, part C for clustering and part B for testing..  The probability of cliff being a common noun is the product of the respective contextual and lexical probabilities , regardless of other information provided by the actual words (a sheer cliff vs..  Can we use a similarity measure of probability distributions to identify optimal clusters? How far can we reduce the tagset without losing accuracy? . 