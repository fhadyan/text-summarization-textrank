 For each parsed sentence in the tree growing corpus, the correct state sequence is traversed..  each tagging event is used for growing the tagging tree, etc .  It remains to be shown that an accurate broad-coverage parser can improve the performance of a text processing application..  This treebank is described in great detail in ..  After over ten years of grammar development, the IBM parser achieved a 0-crossing-brackets score of 69 .  On this same test set, SPATTER scored 76 .  To evaluate SPATTER's performance on this domain, I am using the PARSEVAL measures, as defined in : Precision Recall Crossing Brackets no..  The leaf nodes represent the unique states in the decision-making problem, i.e..  On the other hand, the decision-tree learning algorithm increases the size of a model only as the training data allows..  Thus, it can consider very large history spaces, i.e..  The leaf distributions in decision trees are empirical estimates, i.e..  For detailed descriptions and discussions of the decision-tree algorithms used in this work, see ..  For more discussion of the use of binary decision-tree questions, see .. 