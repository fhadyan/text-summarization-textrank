 Similarly, pM(s|q,u)is the probability of mapping to s, and pI(s) is the probability of inserting s..  In this class of grammars, terminals are drawn from an arbitrary alphabet..  Some of the coincidences in the input data are of interest, and others are not..  However disappointing previous results have been, there is reason to be optimistic..  In fact, the grammars make no use of context whatsoever..  These two steps are iterated until convergence; two or three iterations usually suffice..  Finally, we wish it to learn given even the most complex of inputs..  Finally, ( ) concludes..  This is the same input described in de Marcken ..  Not surprisingly, therefore, the source text is very irregular and contains few repetitions..  The probabilities of all other words will increase, and their code lengths shorten..  We will not explore this further here..  The best grammar for U is the one that minimizes this quantity..  We adopt a natural and convenient model of the underlying representation of sound in memory.. 