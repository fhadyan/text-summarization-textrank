 Let g1 and g2 denote clusters of G1 and G2 respectively..  Unfortunately, because the algorithm is quadratic in C, this may be very costly..  When such a clustering algorithm is applied to a large training corpus, e.g..  The factor C[2] will thus be smaller than E E+C and is only given for completeness..  This in a way counter intuitive result could be explained by the following hypothesis..  In the following, the optimisation criterion for a bigram based model (e.g..  Given these probability estimates pG(w|v the likelihood FMLof the training data, e.g..  The so-called leaving-one-out technique is a special case of cross-validation ( , pp.75 .  use , where n1 and n2 depend on G..  Do corpora currently used for language modeling, e.g..  Moreover, infrequent words (e.g.. 