 Let g1 and g2 denote clusters of G1 and G2 respectively..  Unfortunately, because the algorithm is quadratic in C, this may be very costly..  When such a clustering algorithm is applied to a large training corpus, e.g..  The factor C[2] will thus be smaller than E E+C and is only given for completeness..  Furthermore, the speed up of the algorithm seems to be closely related to the number of clusters divided by t..  This in a way counter intuitive result could be explained by the following hypothesis..  In this appendix, we will present the derivation of the optimisation function for the extended clustering algorithm in detail..  Given these probability estimates pG(w|vM v1 the likelihood FMLof the training data, e.g..  In the following, the optimisation criterion for a bigram based model (e.g..  Given these probability estimates pG(w|v the likelihood FMLof the training data, e.g..  The so-called leaving-one-out technique is a special case of cross-validation ( , pp.75 .  Do corpora currently used for language modeling, e.g..  Moreover, infrequent words (e.g.. 