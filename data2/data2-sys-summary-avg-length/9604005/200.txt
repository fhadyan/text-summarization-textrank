 trigrams or higher, if the corpora are large enough 4..  it must be easy to construct..  The corpus consists of transliterated dialogues on business appointments..  Training and test parts are disjunct..  The initial log perplexity of the training part is 0.12..  The constraints and the point of changing the constraint are chosen for pragmatic reasons..  Computation would not have been feasible without this reduction..  Figure also shows the perplexity's slope..  The effect of further retaining the constraint is shown by the thin lines..  Again, we find the discontinuity at the point where the constraint is changed..  1,440 .  This yields a bigram model..  This method does not need a pre-annotated corpus for parameter estimation..  It is 2.39, thus again lower than the perplexity of the bigram model (see table .  The rest of the paper is structured as follows..  These can be calculated in the following way..  Model merging induces Markov models in the following way..  Thus, each step of merging is O(l[3 .  This behavior can be exploited by introducing constraints on the merging process.. 