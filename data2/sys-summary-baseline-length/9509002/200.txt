 Even though text is becoming increasingly available, it is often expensive, especially if it must be annotated.  Consider the decisions facing the SLL technology consumer, that is, the architect of a planned commercial NLP system.  For each module which is to employ SLL, an appropriate technique must be selected.  If different techniques require different amounts of data to achieve a given accuracy, the architect would like to know what these requirements are in advance in order to make an informed choice.  Further, once the technique is chosen, she must decide how much data to collect or purchase for training.  Because this data can be expensive, foreknowledge of data requirements is highly valuable.  Thus, in order to make statistical NLP technology practical, a predictive theory of data requirements is needed.  Despite this need, very little attention has been paid to the problem.  All the SLL systems mentioned above employ knowledge gained from a corpus to make decisions.  Abstractly, this knowledge can be represented as a mapping from observable features (inputs) to decision outcomes (outputs  