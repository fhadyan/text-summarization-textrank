 Statistical approaches to natural language processing are becoming increasingly popular, being applied to a wide variety of tasks.  For example, Weischedel et al.  (1993) explores part-of-speech tagging, parsing and acquisition of lexical frames.  Nonetheless, all these tasks share some important characteristics, not the least of which is the requirement for a sizable corpus of training data.  One question which has largely been ignored is how much data is enough? For example, given a limited body of training data, it is essential to know which statistical NLP methods are likely to be accurate before pursuing any one.  Also, given a particular method, when will acquiring further training data cease to improve the system accuracy? Currently, the field is conspicuously lacking a general theory of data requirements for statistical NLP.  In this paper, I present the first steps towards the development of such a theory.  I begin by formulating a framework for statistical NLP systems designed to capture some of the elements crucial to data requirements analysis.  A statistical NLP system deals with a certain linguistic universe. 