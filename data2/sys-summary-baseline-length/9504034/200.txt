 In applications such as speech recognition, handwriting recognition, and spelling correction, performance is limited by the quality of the language model utilized , , , .  However, static language modeling performance has remained basically unchanged since the advent of n-gram language models forty years ago .  Yet, n-gram language models can only capture dependencies within an n-word window, where currently the largest practical n for natural language is three, and many dependencies in natural language occur beyond a three-word window.  In addition, n-gram models are extremely large, thus making them difficult to implement efficiently in memory-constrained applications.  An appealing alternative is grammar-based language models.  Language models expressed as a probabilistic grammar tend to be more compact than n-gram language models, and have the ability to model long-distance dependencies , , .  However, to date there has been little success in constructing grammar-based language models competitive with n-gram models in problems of any magnitude.  In this paper, we describe a corpus-based induction algorithm for probabilistic context-free grammars that outperforms n-gram models and the Inside-Outside algorithm in medium-sized domains. 