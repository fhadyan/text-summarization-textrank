 In applications such as speech recognition, handwriting recognition, and spelling correction, performance is limited by the quality of the language model utilized , , , .  However, static language modeling performance has remained basically unchanged since the advent of n-gram language models forty years ago .  Yet, n-gram language models can only capture dependencies within an n-word window, where currently the largest practical n for natural language is three, and many dependencies in natural language occur beyond a three-word window.  In addition, n-gram models are extremely large, thus making them difficult to implement efficiently in memory-constrained applications.  An appealing alternative is grammar-based language models.  Language models expressed as a probabilistic grammar tend to be more compact than n-gram language models, and have the ability to model long-distance dependencies , , . 