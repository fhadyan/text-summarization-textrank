 Statistical part-of-speech disambiguation can be efficiently done with n-gram models , .  These models are equivalent to Hidden Markov Models (HMMs) of order n-1.  The states represent parts of speech (categories, tags there is exactly one state for each category, and each state outputs words of a particular category.  The transition and output probabilities of the HMM are derived from smoothed frequency counts in a text corpus.  Generally, the categories for part-of-speech tagging are linguistically motivated and do not reflect the probability distributions or co-occurrence probabilities of words belonging to that category.  It is an implicit assumption for statistical part-of-speech tagging that words belonging to the same category have similar probability distributions.  But this assumption does not hold in many of the cases.  Take for example the word cliff which could be a proper (NP) or a common noun (NN) (ignoring capitalization of proper nouns for the moment   The two previous words are a determiner (AT) and an adjective (JJ   the wise Cliff   Obviously, information useful for probability estimation is not encoded in the tagset. 