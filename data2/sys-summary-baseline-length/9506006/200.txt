 Many, diverse tagged and parsed corpora have been developed.  Amongst the applications of annotated corpora are as training sets for the extraction of models used in speech and handwriting recognition.  Such training sets need to be as large as possible and there is anecdotal evidence that even the largest on its own is too small for a general statistical model of higher-level syntactic structure.  As annotating corpora using hand-crafted markup or some semi-automated process followed by correction by linguistic experts is slow and expensive , it would be preferable if some other method of building a large annotated corpus could be found.  Existing corpora were not designed to a specific framework of annotations so corpora can not easily be collated into a single large training set.  The AMALGAM (automatic mapping among lexico-grammatical annotation models) project was set up to research ways of mapping between annotation schemes in order to increase the size of corpus tagged with the schemes included in the project , .  We are developing a multi-tagged corpus and a multi-treebank, a single text-set annotated with all the tagging and parsing schemes we include in the mappings. 