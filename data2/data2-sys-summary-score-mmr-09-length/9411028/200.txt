 Thirdly, combining functions collect instances of linguistic objects such as: N-grams in the underlying word string; the syntax rules used to create the QLF; and triples of the form (H1,R,H2 where H1 and H2 are the head predicates of QLF substructures representing words or phrases in the sentence and R indicates the relationship (e.g.  Scaling factor training has two phases.  After scaling in this way, the preference functions are able to select the correct QLF (as judged by an expert) in 90 to 95% of cases when trained on four fifths of a corpus of the reference versions of 4092 within-domain, within-coverage ATIS sentences of up to 15 words in length and tested on the other one fifth, with each one fifth being held out in turn for testing.  In practice, of course, by no means all incorrect solutions will be excluded in this way; so we train preference functions on recognizer and language analysis output, to maximize our chances of distinguishing correct from incorrect solutions, whatever stage of processing they arise from. 