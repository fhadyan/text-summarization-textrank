 However, the immediately preceding word leads to an even bigger reduction of about 34   It is possible, that more accurate data (e.g.  fewer, but only correct pairs) would lead to a different result.  The circles correspond to states and the arcs to transitions.  Given this automaton, we then consider each occurrence of a noun (e.g.  Entries that would not normally be considered verb-object pairs are marked with   We can formulate the task of the language model as the prediction of the value of some variable Y (e.g.  In this paper, we report on our work in progress on this topic.  Given these probability estimates pG(yl|xk the likelihood FMLof the training data, e.g.  The likelihood of the training data FML is simply Assuming that the classification is unique, e.g.  The so-called leaving-one-out technique is a special case of cross-validation ( , pp.75   Moreover, we do not consider infrequent words (e.g.  words with occurrence counts smaller than 5) for clustering, because the information they provide is not very reliable. 