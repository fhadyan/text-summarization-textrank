 Where more then ten hypothesis were output for a sentence, only the top ten were considered.  If it turns out not to, considerable ingenuity and effort may have been wasted.  The baseline score for this test corpus, expected from a random choice of (analysable) hypothesis, was 23.2   This technique addresses as follows the three drawbacks just alluded to.  However, clustering can have two important uses.  Most other work on clustering for language modeling (e.g.  (See e.g.  Cover and Thomas, 1991, chapter 2 for the reasoning behind this   Merge that pair of clusters that entails the least additional cost, i.e.  the smallest reduction in the value of PKfor the subcorpus seen so far.  In practice, we keep track not of PK but of the overall corpus entropy HK = -log(PK   This heightens the similarities within clusters and the differences between them.  In the first experiment, sentence hypotheses were evaluated on the N-grams of words and word classes they contained.  The per-item entropy of the training set (i.e. 