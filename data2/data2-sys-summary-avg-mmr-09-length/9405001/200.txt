 Data sparseness is an inherent problem in statistical methods for natural language processing.  Our model provides a basic scheme for probabilistic similarity-based estimation that can be developed in several directions.  First, variations of ( ) may be tried, such as different similarity metrics and different weighting schemes.  Other types of conditional cooccurrence probabilities have been used in probabilistic parsing .  An obvious case is that of trigrams, for which the sparse data problem is much more severe.  Their similarity-based model avoids clustering altogether.  Using this scheme, they predict which unobserved cooccurrences are more likely than others.  However, this estimates the probability of any unseen bigram to be zero, which is clearly undesirable.  We use the estimates given by the standard back-off model, which satisfy that requirement.  The best parameter values found were k=60, t=2.5, and .  The bigram similarity model was also tested as a language model in speech recognition.  The similarity model was correct in 64 cases, and the back-off model in 32.  This advantage for the similarity model is statistically significant at the 0.01 level. 