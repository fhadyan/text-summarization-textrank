 A noun architecture may be applied to noun compounds It has the effect of dividing the evidence from a training instance across all possible categories for the words Given the high level descriptions in section it remains only to formalise the decision process used to analyse a noun compound For three word compounds it suffices to compute the ratio of two probabilities, that of a left-branching analysis and that of a right-branching one For the adjacency model, when the given compound is w1 w2 w3, we can estimate this ratio as: For the dependency model, the ratio is: In both cases, we sum over all possible categories for the words in the compound Because the dependency model equations have two factors, they are affected more severely by data sparseness0) is used to favour a left-branching analysis Comparisons are made across five dimensions: Each of two analysis models are applied: adjacency and dependency Eight different training schemes have been used to estimate the parameters and each set of estimates used to analyse the test set under both the adjacency and the dependency model The schemes used are: the pattern given in section ; and windowed training schemes with window widths of 2, 3, 4, 5, 10, 50 and 100 words As can be seen, the dependency model is more accurate than the adjacency model This is true across the whole spectrum of training schemes Each of a range of training schemes are employed0316 demonstrating the superiority of the dependency model, at least for the compounds within Grolier's encyclopedia Lauer and Dras (1994) suggest two improvements to the method used above While these changes are motivated by the dependency model, I have also applied them to the adjacency model for comparison Five training schemes have been applied with these extensions A marked improvement is observed for the adjacency model, while the dependency model is only slightly improved To determine the difference made by conceptual association, the pattern training scheme has been retrained using lexical counts for both the dependency and adjacency model, but only for the words in the test set Each of two parameterisations are used: associations between words and associations between concepts One problem with the training methods given in section is the restriction of training data to nouns in  Yet when they occur as nouns, they still provide useful training information that the current system ignores To test whether using tagged data would make a difference, the freely available Brill tagger (Brill, 1993) was applied to the corpus Since no manually tagged training data is available for our corpus, the tagger's default rules were used (these rules were produced by Brill by training on the Brown corpus  Three training schemes have been used and the tuned analysis procedures applied to the test set If anything, admitting additional training data based on the tagger introduces more noise, reducing the accuracy However, for the pattern training scheme an improvement was made to the dependency model, producing the highest overall accuracy of 81  While Hindle and Rooth (1993) use a partial parser to acquire training data, such machinery appears unnecessary for noun compounds In all experiments the dependency model provides a substantial advantage over the adjacency model, even though the latter is more prevalent in proposals within the literature The model also has the further commendation that it predicts correctly the observed proportion of left-branching compounds found in two independently extracted test sets An analogous approach to compounds is used in Lauer (1994) and constitutes one scheme evaluated below Yarowsky (1992) uses a fixed 100 word window to collect information used for sense disambiguation A range of windowed training schemes are employed below There are at least four existing corpus-based algorithms proposed for syntactically analysing noun compounds Three of the algorithms use what I will call the ADJACENCY MODEL, an analysis procedure that goes back to Marcus (1980, p253  Given a three word compound, a search is conducted elsewhere in the corpus for each of the two possible subcomponents While substantial work on noun compounds exists in both linguistics (e Their proposal involves comparing the mutual information between the two pairs of adjacent words and bracketing together whichever pair exhibits the highest Resnik (1993) used unambiguous noun compounds from the parsed Wall Street Journal (WSJ) corpus to estimate the association values and analysed a test set of around 160 compounds It uses what I will call the DEPENDENCY MODEL Figure shows a graphical comparison of the two analysis models The dependency model attempts to choose a parse which makes the resulting relationships as acceptable as possible Consider the compound calcium ion exchange, which is typically left-branching (that is, the first two words are bracketed together  Lauer and Dras (1994) show that under a dependency model, left-branching compounds should occur twice as often as right-branching compounds (that is two-thirds of the time  In the test set used here and in that of Resnik (1993 the proportion of left-branching compounds is 67% and 64% respectively The dependency model has also been proposed by Kobayasi et al (1994) for analysing Japanese noun compounds, apparently independently Using a corpus to acquire associations, they bracket sequences of Kanji with lengths four to six (roughly equivalent to two or three words  A test set of syntactically ambiguous noun compounds was extracted from our 8 million word Grolier's encyclopedia corpus in the following way To distinguish nouns from other words, the University of Pennsylvania morphological analyser (described in Karp et al, 1992) was used to generate the set of words that can only be used as nouns (I shall henceforth call this set  All consecutive sequences of these words were extracted, and the three word sequences used to form the test set The remaining compounds were assigned either a left-branching or right-branching analysis One problem with applying lexical association to noun compounds is the enormous number of parameters required, one for every possible pair of nouns Resnik and Hearst (1993) coined the term CONCEPTUAL ASSOCIATION to refer to association values computed between groups of words By the assumption that words within a group behave nounly, this is constant given the two categories To ensure that the test set is disjoint from the training data, all occurrences of the test noun compounds have been removed from the training corpus Two types of training scheme are explored in this study, both unsupervised The second type uses a window to collect training instances by observing how often a pair of nouns co-occur within some fixed number of words.