 There is a probability distribution across the bins representing how instances fall into bins Also, for each bin, BIN is a probability distribution across the set of values representing how instances in that bin take on values In what follows I will make several assumptions: Training and test data are drawn from the same distributions The probability of the most likely value in each bin is constant Finally, I will only consider a simple learning algorithm: collect the training instances falling into each bin and then select the most frequent value for each First, since the values are not necessarily fully determined by the bins, no matter what value the learner assigns to a bin BIN will always be errors (the optimal error rate  Second, since training data is limited, the learner may not have sufficient data available to acquire accurate rules We are interested in estimating the accuracy for various volumes of training data Since the optimal error rate is independent of the amount of training data, it will always exist no matter how much data is used As the amount of training data increases we expect the accuracy to get closer to this optimal If we denote the most likely value in each bin as , then the expected value of the optimal accuracy is determined by the likelihood of this value occurring in each bin The most severe result of insufficient training data is that some bins can go without any training instances To estimate how often this will occur, consider the way in which m training instances would fall into the bins For each bin, the probability that no training instances fall into it is: I will call such bins EMPTY BINS In Lauer (1995) it is shown that for any bin b: Lauer (1995) also bounds the expected accuracy of the mode-based learner when all bins are guaranteed to have at least one training instance Over non-empty bins, we know that the error rate is no worse than twice the optimal error rate for those bins Using the fact that V is binary, the total expected accuracy for test instances in bin bwhen it contains n training instances is: By summing over all possible numbers of training instances in a bin, we can arrive at an expression for the expected accuracy across all bins as follows: where To simplify this I have defined a function as follows: A result which may be easily obtained by expansion is: Using the assumptions in section and the uniform bin probabilities we can now proceed to simplify: The last step uses equation ( ) and  The assumption that bin probabilities are uniform is problematic When bins are uniformly probable, the expected number of training instances in the same bin as a random test instance is (  When this is true the expected number of training instances in the same bin as a random test instance is approximately (  These simulations use a fixed number of bins (10,000 allocating m training instances to the bins according to either a uniform or logarithmic distribution Figure shows five traces of accuracy as the volume of training data is varied However, when the bins are logarithmically distributed learning converges significantly more quickly, as suggested by the reasoning about expected number of relevant training instances (see section  Both these observations are crucial to reasoning about data requirements for SLL In this paper I have explored the dependence of the expected accuracy of a simple statistical learner on the volume of training data In particular, an average of four training instances per bin can be expected to yield an error rate only 50% worse than the optimal error rate Error rates only 50% worse than optimal result from only three training instances per bin.