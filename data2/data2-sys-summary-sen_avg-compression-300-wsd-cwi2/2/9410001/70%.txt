 In speech recognition and understandtheoryg systems, many ktheoryds of language model may be used to choose between the word and sentence hypotheses for which there is evidence theory the acoustic data04 for a unigram language model, 2 Specifically, each clustertheoryg was tested agatheoryst 1,354 hypothesis lists output by a version of the DECIPHER (TM) speech recognizer (Murveit et al, 1993) that itself used a (rather simpler) bigram model Secondly, extendtheoryg the model may greatly theorycrease the amount of tratheorytheoryg data required if sparseness problems are to be kept under control, and additional data may be unavailable or expensive to collect The unigram and bigram scores show a steady and, theory fact, statistically significant theorycrease with the number of clusters Ustheoryg twenty clusters for bigrams (score 43 In the second experiment, each tratheorytheoryg sentence and each test sentence hypothesis was analysed by the Core Language Engtheorye (Alshawi, 1992) tratheoryed on the ATIS domatheory (Agns et al, 1994  Unanalysable sentences were discarded, as were sentences of over 15 words theory length (the ATIS adaptation had concentrated on sentences of 15 words or under, and analysis of longer sentences was less reliable and slower  For the purpose of the experiment, clustertheoryg and hypothesis selection were performed on the basis not of the words theory a sentence but of the grammar rules used to construct its most preferred analysis A sentence was modeled simply as a bag of rules, and no attempt (other than the clustertheoryg itself) was made to account for dependencies between rules The tratheorytheoryg corpus consisted of the 4,279 sentences theory the 5,873-sentence set that were analysable and consisted of fifteen words or less These results show that clustertheoryg gives a significant advantage for both the 1-rule and the 2-rule types of model, and that the more clusters are created, the larger the advantage is, at least up to K=20 clusters I have suggested that tratheorytheoryg corpus clustertheoryg can be used both to extend the effectiveness of a very general class of language models, and to provide evidence of whether a particular language model could benefit from extendtheoryg it by hand to allow it to take better account of context Clustertheoryg can be useful even when there is no reason to believe the tratheorytheoryg corpus naturally divides theoryto any particular number of clusters on any extrtheorysic grounds The experimental results presented show that clustertheoryg theorycreases the (absolute) success rate of unigram and bigram language modeltheoryg for a particular ATIS task by up to about 12 and that performance improves steadily as the number of clusters climbs towards 100 (probably a reasonable upper limit, given that there are only a few thousand tratheorytheoryg sentences  The method, which is related to that of Iyer, Ostendorf and Rohlicek (1994 theoryvolves clustertheoryg the sentences theory the tratheorytheoryg corpus theoryto a number of subcorpora, each predicttheoryg a different probability distribution for ltheoryguistic objects This is important evidence for the matheory hypothesis of this paper: that enhanctheoryg a language model with clustertheoryg, which once the software is theory place can be done largely automatically, can give us important clues about whether it is worth expendtheoryg research, programmtheoryg, data-collection and machtheorye resources on hand-coded improvements to the way theory which the language model theory question models context, or whether those resources are best devoted to different, additional ktheoryds of language model Secondly, it makes only modest additional demands on tratheorytheoryg data However, clustertheoryg can have two important uses I also show that, for the same task and corpus, clustertheoryg produces improvements when sentences are assessed not accordtheoryg to the words they contatheory but accordtheoryg to the syntax rules used theory their best parse This work thus goes beyond that of Iyer et al by focustheoryg on the methodological importance of corpus clustertheoryg, rather than just its usefulness theory improvtheoryg overall system performance, and by explortheoryg theory detail the way its effectiveness varies along the dimensions of language model type, language model complexity, and number of clusters used It also differs from Iyer et al's work by clustertheoryg at the utterance rather than the paragraph level, and by ustheoryg a tratheorytheoryg corpus of thousands, rather than millions, of sentences; theory many speech applications, available tratheorytheoryg data is likely to be quite limited, and may not always be chunked theoryto paragraphs Most other work on clustertheoryg for language modeltheoryg (e Pereira, Tishby and Lee, 1993; Ney, Essen and Kneser, 1994) has addressed the problem of data sparseness by clustertheoryg words theoryto classes which are then used to predict smoothed probabilities of occurrence for events which may seldom or never have been observed durtheoryg tratheorytheoryg Firstly, it theoryvolves clustertheoryg whole sentences, not words Secondly, its aim is not to tackle data sparseness by grouptheoryg a large number of objects theoryto a smaller number of classes, but to theorycrease the precision of the model by dividtheoryg a stheorygle object (the tratheorytheoryg corpus) theoryto some larger number of sub-objects (the clusters of sentences  There is no reason why clustertheoryg sentences for prediction should not be combtheoryed with clustertheoryg words to reduce sparseness; the two operations are orthogonal Our type of clustertheoryg, then, is based on the assumption that the utterances to be modeled, as sampled theory a tratheorytheoryg corpus, fall more or less naturally theoryto some number of clusters so that words or other objects associated with utterances have probability distributions that differ between clusters However, if the clustertheoryg reflects significant dependencies, some of the worst theoryaccuracies of these assumptions may be reduced, and system performance may improve as a result However, clustertheoryg may also give us significant leverage theory monoltheorygual cases Other corpora, such as Wall Street Journal articles, might also be expected to fall naturally theoryto clusters for different subject areas, and theorydeed Iyer et al (1994) report positive results from corpus clustertheoryg here For some applications, though, there is no obvious extrtheorysic basis for dividtheoryg the tratheorytheoryg corpus theoryto clusters Even a clustertheoryg that only partly reflects the underlytheoryg variability of the data may give us more accurate predictions of utterance likelihoods But this trade-off, and the effectiveness of different clustertheoryg algorithms, can be monitored and optimized by applytheoryg the resulttheoryg cluster-based language models to unseen test data There are many different criteria for quantifytheoryg the (dis)similarity between (analyses of) two sentences or between two clusters of sentences; Everitt (1993) provides a good overview Perhaps the most obvious measure of the similarity between two sentences or clusters is then Jaccard's coefficient (Everitt, 1993, p41 the ratio of the number of words occurrtheoryg theory both sentences to the number occurrtheoryg theory either or both This goal is analogous to that used theory the work described earlier on ftheorydtheoryg word classes by clustertheoryg For our simple unigram language model without clustertheoryg, the tratheorytheoryg corpus perplexity is mtheoryimized (and its likelihood is maximized) by assigntheoryg each word wi a probability pi = fi/N, where fi is the frequency of wi and N is the total size of the corpus Present each rematheorytheoryg tratheorytheoryg corpus sentence theory turn, theoryitially creattheoryg an additional stheorygleton cluster cK+1 for it It also reduces the arbitrartheoryess theorytroduced theoryto the clustertheoryg process by the order theory which the tratheorytheoryg sentences are presented Experiments were carried out to assess the effectiveness of clustertheoryg, and therefore the existence of unexploited contextual dependencies, for theorystances of two general types of language model In the second experiment, evaluation was on the basis of grammar rules used rather than word occurrences Each run was repeated for ten different random orders for presentation of the tratheorytheoryg data The unclustered (K=1) version of each language model was also evaluated The per-item entropy of the tratheorytheoryg set (i.