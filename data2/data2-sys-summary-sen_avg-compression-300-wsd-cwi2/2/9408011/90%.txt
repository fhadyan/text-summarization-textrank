 Figure shows the five words most similar to the each cluster centroid for the four clusters resulting from the first two cluster splits Figure plots the average relative entropy of several data sets to asymmetric clustered models of different sizes, given by where tn is the relative frequency distribution of verbs taking nas direct object in the test set For each critical value of , we show the relative entropy with respect to the asymmetric model based on of the training set (set train of randomly selected held-out test set (set test and of held-out data for a further 1000 nouns that were not clustered (set new  Unsurprisingly, the training set relative entropy decreases monotonically Thus this test evaluates how well the models reconstruct missing data in the verb distribution for n from the cluster centroids close to n The resulting training set was used to build a sequence of cluster models as before Our approach avoids both problems Our classification method will construct a set of clusters and cluster membership probabilities p(c|n  To cluster nouns n according to their conditional verb distributions pn, we need a measure of similarity between distributions We can thus use the relative entropy between the context distributions for two words to measure how likely they are to be instances of the same cluster centroid Finally, relative entropy is a natural measure of similarity between distributions for clustering because its minimization leads to cluster centroids that are a simple weighted average of member distributions With the maximum entropy (ME) membership distribution, ML estimation is equivalent to the minimization of the average distortion of the data In particular, the model we use in our experiments has noun clusters with cluster memberships determined by p(n|c) and centroid distributions determined by p(v|c  First, for fixed average distortion between the cluster centroid distributions p(v|c) and the data p(v|n we find the cluster membership probabilities, which are the Bayes's inverses of the p(n|c that maximize the entropy of the cluster distributions Given any similarity measure d(n,c) between nouns and cluster centroids, the average cluster distortion is If we maximize the cluster membership entropy subject to normalization of p(n|c) and fixed ( we obtain the following standard exponential forms for the class and membership distributions where the normalization sums (partition functions) are and  We say then that the original cluster has split into the two new clusters We start with very low and a single cluster whose centroid is the average of all noun distributions.