 Figure shows the five words most similar to the each cluster centroid for the four clusters resulting from the first two cluster splits Figure plots the average relative entropy of several data sets to asymmetric clustered models of different sizes, given by where tn is the relative frequency distribution of verbs taking nas direct object in the test set Unsurprisingly, the training set relative entropy decreases monotonically The resulting training set was used to build a sequence of cluster models as before Our classification method will construct a set of clusters and cluster membership probabilities p(c|n  To cluster nouns n according to their conditional verb distributions pn, we need a measure of similarity between distributions We will take ( ) as our basic clustering model In particular, the model we use in our experiments has noun clusters with cluster memberships determined by p(n|c) and centroid distributions determined by p(v|c  First, for fixed average distortion between the cluster centroid distributions p(v|c) and the data p(v|n we find the cluster membership probabilities, which are the Bayes's inverses of the p(n|c that maximize the entropy of the cluster distributions We start with very low and a single cluster whose centroid is the average of all noun distributions.