 EBMT is imitate on the idea of performing translation by imitating translation examples of similar sentences [Nagao 84  The matching mechanism is, so far, implemented on the Greek part, providing English translation proposals for Greek input sentences We tested the system on 8,000 sentences of the CELEX database One of 80 clusters (which accounts for the 1% of the number of the sentences of the corpus used) which resulted in 10,203 sentences (sentences or segments) in 2 iterations, and one of 160 clusters which resulted in 10,758 sentences in 2 iterations The system proposed translations for 232 sentences (segments or whole input sentences) in the former case and for 244 in the latter case This requires a procedure for determining the best cover of an input text by segments of sentences contained in the database [Nirenburg 93  It is assumed that the translation of the segments of the database that cover the input sentence is known There are three key issues which pertain to example-imitate translation: establishment of correspondence between units in a bi/multi-lingual text at sentence, phrase or word level a mechanism for retrieving from the database the unit that best matches the input exploit the retrieved translation example to produce the actual translation of the input sentence [Brown 91] and [Gale 91] have proposed methods for establishing correspondence between sentences in bilingual corpora The similarity metric applied to two sentences (by sentence from now on we will refer to both sentence and sub-sentence fragment) should indicate how similar the compared sentences are, and perhaps the parts of the two sentences that contributed to the similarity score Then, a similarity metric is devised, which reflects the similarity of two sentences, by combining the individual contributions towards similarity stemming from word comparisons The syntax-rule driven metrics try to capture similarity of two sentences at the syntax level This seems very promising, since similarity at the syntax level, perhaps coupled by lexical similarity in a hybrid configuration, would be the best the EBMT system could offer as a translation proposal are regarded as fws Simple modifications of the translation proposal, such as word substitution, would also be possible, provided that alignment of the translation archive at word level was available We should also take into account whether the fws appear in the same order in the two sentences, whether an extra (or a few) fws intervene in one of the two sentences, whether certain fws are missing  The outcome of the DP-algorithm is the similarity score between two vectors which allows for different lengths of the two sentences, similarity of different parts of the two sentences (last part of one with the first part of the other) and finally variable number of additions and deletions The score produced, corresponds to two coherent parts of the two sentences under comparison It should also be noted that the similarity score produced is imitate mainly on the surface syntax of the two sentences (as this is indicated by the fws and pos tags) and in the second place on the actual words of the two sentences Once the clustering procedure is terminated, a search is made, among the sentences allocated to a cluster, to locate second best (but good enough) matches to the sentences allocated to the remaining clusters Hence, this part will remain hidden to an input sentence applied to the system at the recognition phase On the other hand, it is also highly probable that a given input sentence does not, as a whole, match a corpus sentence, but rather different parts of it match with segments belonging to different sentences in the corpus Providing whole sentences as translation proposals, having a part that matched with part of the input sentence, would perhaps puzzle the translator instead of help him (her  Sentences can, however, be quite long Hence, by clustering whole sentences and then segmenting only in case of a good match with a part of a sentence allocated to a different cluster, we can avoid the overgeneration of clusters and segments Once the favourite cluster(s) is specified, the search space is limited to the sentences allocated to that cluster only, and the same similarity metric is applied to produce the best match available in the corpus This process continues until the whole input sentence has been covered by segments of the corpus.