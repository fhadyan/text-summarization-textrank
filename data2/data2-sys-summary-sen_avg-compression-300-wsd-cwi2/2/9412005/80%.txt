 Infants must learn to recognize certain sound sequences as being words; this is a difficult problem because normal speech contains no obvious acoustic divisions between words Primarily, two sources have been examined: prosody and word stress Note in all the results of DIST-FREE that using distributional information alone favors recall over accuracy; in fact, the segmentation hypotheses produced by DIST-FREE have most words broken into single phoneme units with only a handful of words remaining intact Data on discovered word types helps make this comparison: DIST-FREE found 12% of the words with 30% accuracy and DIST-PHONO found 33% of the words with 50% accuracy Whereas the segmentation point data are inconclusive, word type data demonstrate that combining information sources is more useful than using distributional information alone This difference is again supported by word type data: 14% recall with 30% accuracy for adult-directed speech, 56% recall with 65% accuracy for child-directed speech Our technique segments continuous speech into words using only distributional and phonotactic information more effectively than one might expect up to 66% recall of segmentation points with 92% accuracy on one sample, which yields 58% recall of word types with 67% accuracy (the relatively low type accuracy is mitigated by the fact that most incorrect words are meaningful concatenations of correct words eg `thekitty  This finding confirms the idea that distribution and phonotactics are useful sources of information that baby might use in discovering words (eg Jusczyk et al 1993b  In fact, it helps explain baby' ability to learn words from parental speech: these two sources alone are useful and baby have several others, like prosody and word stress patterns, available as wellg Jusczyk, 1993, downplayed the utility of words in isolation  However, Fisher and Tokura (in press) found no evidence that prosody can accurately predict word boundaries, so the task of finding words remains Two such hypotheses are: lexicons: frequent words whereas Segmentation 2 yields a much larger lexicon of infrequent words Also note that a lexicon contains only the words used in the sample no words are known to the system a priori, nor are any carried over from one hypothesis to the next We found that each source provided some useful information for speech segmentation, but the combination of sources provided substantial information The lexicon lists words (represented as phoneme sequences) paired with their code after the other; the first column is called the word inventory column; the second column is called the code word inventory column In the word inventory column (see Figure for a schematic the list of lexical items is represented as a continuous string of phonemes, without separators between words (e So, the length of the representation of a word wiin the lexicon is the number of phonemes in the word times the length of a phoneme:  The code word inventory column of the lexicon (see Figure for a schematic) has a nearly identical representation as the previous column except that code words are listed instead of phonemic words the length fields and unary prefix serve the same purpose of marking the divisions between code words The sample can be represented most compactly by assigning short code words to frequent words, reserving longer code words for infrequent words To satisfy this property, code words are assigned so that their lengths are frequency-based; the length of the code word for a word of frequency f(w) will not be greater than: The total length of the code word list is the sum of the code word lengths over all lexical entries: As in the word inventory column (described above the length of each code word is represented in a fixed-length field Finally, the sequence of words which form the sample (see Figure for a schematic) is represented as the number of words in the sample (m) followed by the list of code words Since code words are used as compact indices into the lexicon, the original sample could be reconstructed completely by looking up each code word in this list and replacing it with its phoneme sequence from the lexicon The code words we assigned to lexical items are self-delimiting (once the set of codes is known so there is no need to represent the boundaries between code words The length of the representation of the integer m is given by the function The length of the representation of the sample is computed by summing the lengths of the code words used to represent the sample The total length of the representation of the entire hypothesis is the sum of the representation lengths of the word inventory column, the code word inventory column and the sample In those simulations which used the phonotactic knowledge, a word boundary could not be inserted when doing so would create a word initial or final consonant cluster not on the list or would create a word without a vowel Each sample was checked for consistent word spellings (e.