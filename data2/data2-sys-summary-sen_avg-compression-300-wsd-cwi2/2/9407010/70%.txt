 Recognizer score + linguistic KSs 3 Two variants of the highest-in-coverage method provided a lower limit: the straight method, and one in which the hypotheses were first rescored using the optimized combination of M score and N-gram discriminant KSs Table shows the sentence error rates for different preference methods and utterance lengths, using 10-best lists; Table shows the word error rates for each method on the full set The word error rate decreased from 7 It is apparent that nearly all of the improvement is coming from the linguistic KSs; the difference between the lines M + linguistic KSs and all available KSs is not significant In view of the considerable computational overhead required to perform linguistic analysis on a large number of speech hypotheses, its worth is dubious The improvement in sentence error rate on this method of evaluation is from 28 On either type of evaluation, the difference between all available KSs and any other method except M + linguistic KSs is significant at the 5% level according to the McNemar sign test  This led to results which were both better and also qualitatively different; the N-gram KSs made a much larger contribution, and appeared to dominate the linguistic KSs The methods described here were tested on a 1001-utterance unseen subset of the ATIS corpus; speech recognition was performed using the DECIPHER (TM) M , , and linguistic analysis by a version of the Core Language Engine (CLE;  For 10-best hypothesis lists, the best method yielded a proportional reductions of 13% in the word error rate, and 11% in the sentence error rate; if sentence error was scored in the context of the task, the reduction was about 21  By contrast, the corresponding figures for the highest-in-coverage method were a 7% reduction in word error rate, a 5% reduction in sentence error rate (strictly measured) and a 12% reduction in the sentence error rate in the context of the task This section describes how different knowledge sources (KSs) can be combined Typical examples are the score which the M assigned to H, and the score for whether or not Hreceived a linguistic analysis (1 or 0 respectively  More commonly, however, the KS will produce a list of one or more linguistic items associated with H, for example surface N-grams in H or the grammar rules occurring in the best linguistic analysis of H, if there was one A given linguistic item L is associated with a numerical score through a discrimination function (one function for each type of linguistic item that summarizes the relative frequencies of occurrence of L in correct and incorrect hypotheses respectively The total score for each hypothesis is a weighted sum of the scores contributed by the various KSs The intent is that linguistic items which tend to occur more frequently in correct hypotheses than incorrect ones will get positive scores; those which occur more frequently in incorrect hypotheses than correct ones will get negative scores Comparing the different hypotheses for various utterances, we discover that if we have two distinct hypotheses for the same utterance, one of which is correct and the other incorrect, and the hypotheses differ by one of them containing a list of while the other contains a list the, then the hypothesis containing a list of is nearly always the correct one This justifies giving the trigram a list of a positive score, and the trigram a list the a negative one We now define formally the discrimination function dT for a given type T of linguistic item We start by defining dT as a function on linguistic items dT(L) for a given linguistic item L is computed as follows The language processor brings more sophisticated linguistic knowledge sources to bear, typically some form of syntactic and/or semantic analysis, and uses them to choose the most plausible member of the N-best list The training corpus is analyzed, and each hypothesis is tagged with its set of associated linguistic items Section gives the results The M used a class bigram language model Each N-best hypothesis received a numerical plausibility score; only the top 10 hypotheses were retained The 1-best sentence error rate was about 34 the 5-best error rate (i The CLE normally assigns a hypothesis several different possible linguistic analyses, scoring each one with a plausibility measure Only the most plausible linguistic analysis was used2% of all hypotheses timed out during linguistic analysis; the average analysis time required per hypothesis was 2 The following knowledge sources were used in the experiments: Recognizer score: The numerical score assigned to each hypothesis by the DECIPHER (TM) M In coverage: Whether or not the CLE assigned the hypothesis a linguistic analysis (1 or 0  Unlikely grammar construction: 1 if the most plausible linguistic analysis assigned to the hypothesis by the CLE was unlikely 0 otherwise Class N-gram discriminants (four distinct knowledge sources Discrimination scores for 1 2 3- and 4-grams of classes of surface linguistic items Grammar rule discriminants: Discrimination scores for the grammar rules used in the most plausible linguistic analysis of the hypothesis, if there was one Semantic triple discriminants: Discrimination scores for semantic triples in the most plausible linguistic analysis of the hypothesis, if there was one The first is the singleton consisting of the M score KS; the second contains the four class N-gram discriminant KSs; the third consists of the remaining linguistic KSs Recognizer score + class N-gram discriminant KSs 2.