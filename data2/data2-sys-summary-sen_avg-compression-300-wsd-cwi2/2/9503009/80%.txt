 One way to do this is to let the right context vector record which classes of left context vectors occur to the right of a word The rationale is that words with similar left context characterize words to their right in a similar way This proposal was implemented by applying a singular value decomposition to the 47025-by-250 matrix of left context vectors and clustering the resulting context vectors into 250 classes A generalized right context vector v for word w was then formed by counting how often words from these 250 classes occurred to the right of w Generalized left context vectors were derived by an analogous procedure using word-based right context vectors This differs from previous approaches , in which left and right context vectors of a word are always used in vector concatenated vector There are arguably fewer different types of right syntactic contexts than types of syntactic categories This generalization could not be exploited if left and right context were not treated separately Another argument for the two-step derivation is that many words don't have any of the 250 most frequent words as their left or right neighbor The generalized context vectors were input to the tag induction procedure described above for word-based context vectors: 20,000 word triplets were selected from the corpus, encoded as 1,000-dimensional vectors (consisting of four generalized context vectors decomposed by a singular value decomposition and clustered into 200 classes Tables present results for word type-based induction and induction based on word type and context Table shows that performance for generalized context vectors is better than for word-based context vectors (0 those not containing punctuation marks and rare words Rare words are difficult because of lack of distributional evidence applies factor analysis to collocations of two target words certain and right with their immediate neighbors We start by constructing representations of the syntactic behavior of a word with respect to its left and right context0 if two words share many neighbors, and 0 We refer to the vector of left neighbors of a word as its left context vector, and to the vector of right neighbors as its right context vector The unreduced context vectors in the experiment described here have 250 entries, corresponding to the 250 most frequent words in the Brown corpus Yet intuitively, they are similar with respect to their right syntactic context despite the lack of common right neighbors We can represent the left vectors of all words in the corpus as a matrix C with n rows, vector for each word whose left neighbors are to be represented, and kcolumns, vector for each of the possible neighbors Neighbors with highest similarity according to both left and right context are listed These examples demonstrate the importance of representing generalizations about left and right context separately The left and right context vectors are the basis for four different tag induction experiments, which are described in detail below: induction based on word type only induction based on word type and context induction based on word type and context, restricted to natural contexts induction based on word type and context, using generalized left and right context vectors The two context vectors of a word characterize the distribution of neighboring words to its left and right The concatenation of left and right context vector can therefore serve as a representation of a word's distributional behavior ,  We formed such concatenated vectors for all 47,025 words (surface forms) in the Brown corpus Here, we use the raw 250-dimensional context vectors and apply the SVD to the 47,025-by-500 matrix (47,025 words with two 250-dimensional context vectors each  In order to exploit contextual information in the classification of a token, we simply use context vectors of the two words occurring next to the token An occurrence of word w is represented by a concatenation of four context vectors: The right context vector of the preceding word The left context vector of w The right context vector of w The left context vector of the following word We randomly selected 20,000 word triplets from the corpus and formed concatenations of four context vectors as described above Distributional tagging of an occurrence of a word w proceeds then by retrieving the four relevant context vectors (right context vector of previous word, left context vector of following word, both context vectors of w) concatenating them to vector 1000-compvectornt vector, mapping this vector to 50 dimensions, computing the correlations with the 200 cluster centroids and, finally, assigning the occurrence to the closest cluster The context vectors of punctuation marks contribute little information about syntactic categorization since there are no grammatical dependencies between words and punctuation marks, in contrast to strong dependencies between neighboring words Contexts with rare words (less than ten occurrences) were also excluded for similar reasons: If a word only occurs nine or fewer times its left and right context vectors capture little information for syntactic categorization The context vectors used so far only capture information about distributional interactions with the 250 most frequent words.