g a GB grammar While this is not an absolute limitation, in that it is theoretically possible to extract this information manually or semi-automatically from a corpus, time constraints entailed the rejection of this approach the greater likelihood of an NP featuring in the verb's theta grid It is this property of a number of modules working hand in hand that needs to be carried over into the probabilistic domain The objections that linguists once held against statistical methods are disappearing slowly, partly due to results in corpora analysis that show the inadequacy of linguistic theory when applied to naturally occurring data O(2[n ,  A feature of principle-based grammars is their potential to assign some meaningful representation to a string which is strictly ungrammatical , , , , this is a feature which has to be `added on' and tends to affect the statement of the grammar Each proper branch is a binary branching structure, and so all grammatical constraints will need to be encoded locally structure proposing engines) while keeping the grammar constant A small principle-based parser was built, following the proper branch formalism developed in  Although the grammar is very limited, the use of probabilities in ranking the parser's output can be seen as a first step towards implementing a principle-based parser using a more fully specified collection of grammar modules This representation is interpreted as S-structure Explanations of the knowledge contained within each grammar principle is given in the following sections This paper falls directly between these approaches, using statistical information derived from corpora analysis to weight syntactic analyses produced by a `principles and parameters' parser X-bar Theory uses a set of schemata to license local subtrees Category: the standard category names are employed Specifier (SPEC this feature specifies whether the word at the head of the phrase being built requires a specifier The head (i lexical item) of a node is carried on each projection of that node along with its theta grid The use of probabilistic information in principle-based grammars and parsers is considered, including discussion on some theoretical and computational problems that arise the main verbs were all from the set for which theta role data was obtained the examples were manually parsed by the authors The work on stochastic context-free grammars suggests a different set of results, in that the specific categories involved in expansions are all important When a more modular theory is employed, the source of the supposedly category specific information is not as obvious The use of lexical probabilities on specifier and complement co-occurrence with specific heads (i lexical items) could exihibit properties that appear to be category specific, but are in fact caused by common properties which are shared by lexical items of the same category Theta theory is concerned with the assignment of an argument structure to a sentence a transitive verb has one theta role to `discharge' which must be assigned to an NP The easiest method of obtaining and applying theta probabilities will be with reference to whole theta grids Although the distinction between complements and adjuncts is a theoretically interesting one, the process of determining which constructions fill which functional roles in the analysis of real text often creates a number of problems (see for discussion on this issue regarding output of the Fidditch parser  Case theory differs from both X-bar and Theta theory in that it is category specific: only NPs require, or indeed can be assigned, abstract case If we are to implement a probabilistic version of a modular grammar theory incorporating a Case component, a relevant question is: are there multiple ways of assigning Case to noun phrases in a sentence? i As a result, the use of Case probabilities in a parser would be at best unimportant, since some form of ambiguity is needed in the module, i it is possible to satisfy the Case filter in more than one way, for probabilities associated with the module to be of any use The use of a heterogeneous grammar formalism and multiple probabilities invokes the problem of their combination Alternatively, a combination of some or all of the daughters' probability features could be employed, thus making, eg the X-bar probability of the mother dependent upon all the stochastic information from the daughters, including theta and Case probabilities, etc The need for a method of combining the daughter probabilities into a useful figure for the calculation of the mother probabilities is likely to involve trial and error, since theory thus far has had nothing to say on the subject The manner in which the global probability is calculated will be partly dependent upon the information contained in the local probability calculations If the probabilities for partial analyses have been calculated using only probabilities of the same types from the subanalyses e X-bar, Theta the probabilities at the top level will have been calculated using informationally distinct figures the X-bar, theta, etc The parser used in testing employed the first method and therefore produced separate module probabilities for each node was kept constant The fact that each string received multiple parses (the mean number of analyses being 9 The grammar employed is a partial characterisation of Chomsky's Government-Binding theory , and only takes account of very local constraints (i ) will be needed before a grammar of sufficient coverage to be useful in corpora analysis can be formulated While GB is an elegant theory of cognitive syntax, it has yet to be shown that such a modular characteristion can be successfully employed in corpus analysis.