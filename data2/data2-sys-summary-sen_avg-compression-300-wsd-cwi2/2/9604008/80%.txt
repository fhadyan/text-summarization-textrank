 This would change the probabilities of the derivations; however the probabilities of parse trees would not change, since there would be correspondingly more derivations for each tree Thus, every STSG tree would be produced by the PCFG with equal probability Since the total probability of the trees produced by the STSG is 1, and the PCFG produces these trees with the same probability, no probability is left over for any other trees There are several different evaluation metrics one could use for finding the best parse In the section covering previous research, we considered the most probable derivation and the most probable parse tree Thus, chance tree has on average 2 constituents correct We call the best parse tree under chance criterion the Maximum Constituents Parse Bod shows that the most probable derivation does not perform as well as the most probable parse for the DOP model, getting 65% exact match for the most probable derivation, versus 96% correct for the most probable parse This is not surprising, since each parse tree can be derived by many different derivations; the most probable parse criterion takes all possible derivations into account However, while the Inside-Outside algorithm is a grammar re-estimation algorithm, the algorithm presented here is just a parsing algorithm We use the reduction and algorithm to parse held out test data, comparing these results to a replication of Pereira and Schabes on the same data After that, put the most likely constituents together to form a parse tree, using dynamic programming The probability that a potential constituent occurs in the correct parse tree, , will be called g(s, t, X  We can compute chance probability using elements of the Inside-Outside algorithm The original ATIS data from the Penn Tree Bank, version 0 We also ran experiments using Bod's data, 75 sentence test sets, and no limit on sentence length It is also noteworthy that the results are much better on Bod's data than on the minimally edited data: crossing brackets rates of 96% and 97% on Bod's data versus 90% on minimally edited data However, for his algorithm to have some reasonable chance of finding the most probable parse, the number of times he must sample his data is at least inversely proportional to the conditional probability of that parse For instance, if the maximum probability parse had probability 1/50, then he would need to sample at least 50 times to be reasonably sure of finding that parse Now, we note that the conditional probability of the most probable parse tree will in general decline exponentially with sentence length Since the probability of the most probable parse decreases exponentially in sentence length, the number of random samples needed to find chance most probable parse increases exponentially in sentence length Thus, when using the Monte Carlo algorithm, one is left with the uncomfortable choice of exponentially decreasing the probability of finding the most probable parse, or exponentially increasing the runtime In the DOP model, a sentence cannot be given an exactly correct parse unless all productions in the correct parse occur in the training set Thus, we can get an upper bound on performance by examining the test corpus and finding which parse trees could not be generated using only productions in the training corpus Bod randomly split his corpus into test and training According to his thesis , only one of his 75 test sentences had a correct parse which could not be generated from the training data Examining Bod's data, we find he removed productions Now, use these trees to form a Stochastic Tree Substitution Grammar (STSG  These trees can be combined in various ways to parse sentences There are two existing ways to parse using the DOP model First, one can find the most probable derivation Using the most probable derivation criterion, one simply finds the most probable way that a sentence could be produced For the string x x, what is the most probable derivation? The parse tree x 1A x 1CD 2S has probability of being generated by the trivial derivation containing a single tree One could try to find the most probable parse tree For a given sentence and a given parse tree, there are many different derivations that could lead to that parse tree The probability of the parse tree is the sum of the probabilities of the derivations Given our example, there are two different ways to generate the parse tree x 1E x 1B 2S each with probability , so that the parse tree has probability  This parse tree is most probable Bod shows how to approximate chance most probable parse using a Monte Carlo algorithm The algorithm randomly samples possible derivations, then finds the tree with the most sampled derivations Bod shows that the most probable parse yields better performance than the most probable derivation on the exact match criterion We will need to create one new non-terminal for each node in the training data We will call non-terminals of chance form interior non-terminals, and the original non-terminals in the parse trees exterior  Parsing using the DOP model is especially difficult The model can be summarized as a special kind of Stochastic Tree Substitution Grammar (STSG given a bracketed, labelled training corpus, let every subtree of that corpus be an elementary tree, with a probability proportional to the number of occurrences of that subtree in the training corpus We call a PCFG tree isomorphic to a STSG tree if they are identical when internal non-terminals are changed to external non-terminals Our main theorem is that chance construction produces PCFG trees isomorphic to the STSG trees with equal probability For every STSG subderivation, there would be an isomorphic PCFG subderivation, with equal probability Thus for every STSG derivation, there would be an isomorphic PCFG derivation, with equal probability Thus every STSG tree would be produced by the PCFG with equal probability.