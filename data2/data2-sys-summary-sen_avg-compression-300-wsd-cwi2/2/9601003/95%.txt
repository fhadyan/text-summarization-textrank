 rating task has connections with SU 1 independent evaluations for spoken and written language engineering Then there's handwriting as well  But evaluation requires modularity to be enforced everywhere, and that will just block innovation 2: We need to bear in mind that finer granularity may differ substantially between systems, so that in may cases comparative evaluation may not be possible generation ? I don't know what 'willing' is supposed to mean here - this is a very resource-dependent thing phone lattice production ? 3 prosodic marking ? many spoken language systems wont produce any of these native-non-native speakers ? 26 Implementation of Comparative TA SE Two possible scenarios for developing an evaluation programme are the following In this scenario the type of evaluation evolves out of actual funded pilot applications This would be done in the 2nd and final calls the bottom-up scenario ? 2 bottom up  homogeneous 2: I don't think either of theses is an effective way to achieve this goal, but 2 comes closest general business letter dictation Clearly some mix is required5 Content of Comparative TA SE One can look at a major task components in a system (e Any task or component (including non-linguistic ones) within a system also has an environment A grid evaluation methodology naturally emerges Assessment comprises verification and validation Having determined these, one way of subjecting a system to user validation is by carrying out a grid evaluation, varying environment variables alongside system parameters So one cannot have `pure' task validation, independent of any user requirements A technology has user validity if it can be deployed within some system meeting genuine user needs Therefore, the term technology rating can be somewhat misleading: (user-transparent) task rating might be better terminology Internal rating will normally make use of evaluation data specific to the needs of the project: either the user needs, or the technological needs Assessment issues can be divided into those concerning user-centered rating, and those concerning technology rating Given that applications will vary from project to project, this user-specific data may not be directly amenable for use as comparative evaluation data Technological evaluation data will depend on the precise instances of technology used within the project So again, the technological data may not be directly amenable for use as comparative evaluation data Comparative rating involves taking different systems and comparing either their system-wide performance, and/or the performance of individual components / task competencies Flexibility: For an individual system one can vary environment factors such as user profile, domain, etc to see how flexible a system is Evaluation data comes in three forms 1 Test, or input data 2 In collecting evaluation data for rating, a decision must be made about the level of: 1g do we evaluate the system against data from different languages, different domains etc  This section addresses user-centered rating The trigger papers, and one reaction paper, are Sparck Jones Crouch: General Technology Assessment Netter: Technology Assessment for Written NL Applications Steeneken: Speech Technology Assessment Adriaens: User-Centered Assessment for RTD in Language Engineering King: Reactions to G One possible mode of user-centered comparative evaluation would be to try the user group from one project out on a system from another project having a broadly similar, though not necessarily identical, functionality What could be distilled from these data are abstractions from comparable evaluation scenarios, evaluation metrics etc task and sub-task performance) is a natural way of pursuing comparative evaluation The second discusses user-centered rating, followed by an extensive discussion of technology rating `Pure' technology evaluation metrics require user-centered validation User-centered validation of technology evaluation metrics has received scant attention Performing meaningful technology rating entails the identification of relevant environment and user attributes One possibility would be to set up a small ARPA-like evaluation project under FP-4 corpora and, more importantly, the evaluation data with `answers' for chosen tasks The evaluation structure should allow both technological and user-centered evaluation As far as possible the comparative evaluation exercise should sit on top of, and make use of, project internal evaluation These evaluation points allow for a variety of different kinds of evaluation When a sub-task is user significant, then user-centered evaluation metrics can be applied Generally, we can talk of a system or system component as implementing a task A braided evaluation structure allows for comparative and individual evaluation of different systems at different levels (user-centered, task-specific, general technology  Thus, identifying task structure rather than system architecture is the first step towards defining an evaluation framework One could envisage user, task and general technology measures being applied to Task 3 The coarse task decomposition above provides little space for technology rating Own-language retrieval would be done only to provide evaluation data Three topics of particular interest in the evaluation of the present state-of-the-art speech technology: user appreciation of spoken language systems; rating of technology modules; and the interaction between spoken language and natural language systems Total system performance (does the user get the required information efficiently user appreciation and the performance of different technologies are main issues for rating Total system performance : benefit vs Human performance also offers a bench mark for system evaluation In general a robust evaluation experiment should include experiments on various items of user appreciation, system performance or technology Three examples are given: Application oriented: The evaluation of a system using speech input and speech output in a dialogue concept, such as used for a travel information system, allows for the rating of the total system (user appreciation and system performance  Internal technology rating may or may not form part of progress evaluation, which would in any case require a large element of user-centered rating Namely, (a) that the form of project internal technology evaluation can vary from project to project, and (b) that technology evaluation provides the core of comparative rating But what one would like is for comparative technology rating to build on the back of project internal rating Thus, in order to facilitate comparison and reusability of evaluation data, a limited degree of standardisation is required for project internal technology rating projects themselves should carry out project internal technology rating, though there may be a case for having evaluation data assembled by independent `experts 3 To a large degree, methods of project internal evaluation will be a matter for negotiation between users and system developers within individual projects Early acquisition (and use) of evaluation data should be encouraged Wherever possible, projects should be encouraged to make their internal evaluation strategies, test data, user profiles etc Using these tools, individual projects should produce annotated answer data from their own evaluation corpora Individual projects should make available as much additional linguistic and non-linguistic data pertinent to evaluation as possible QUESTIONNAIRE ON ASSESSMENT AND EVALUATION IN LANGUAGE ENGINEERING 1uni-sb user validation; and 20 Questionnaire 21 Introduction and Terminology Terms later used in the specific sense introduced here are capitalised spontaneous speech channel conditions (telephone vs Would be willing to conduct it 4 Maybe you should have an option 5 Would be willing to participate 4 comparative rating is not going to divert substantial resources from internal rating and development; 2 that comparative rating is likely to bring about project internal improvements, in the same way that internal evaluation should; 3 whatever validation the users deem sufficient) ? 2 2: qualitative rating is not enough 2: User validation alone won't promote generality and reusability of components Therefore technological rating levels are undetournable only on unseen input-output pairings whose linguistic features match those of the project application 2 2: This answer needs to be qualified This reflects a system's maintainability, portability and flexibility internally by project participants (developers and users) ? 23 above.