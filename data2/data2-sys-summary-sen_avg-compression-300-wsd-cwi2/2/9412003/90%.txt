 Thus, if one starts out with an initial clustering in which no cluster occurs only once, and if one never moves words that occur only once, then one will never have a cluster which occurs only once Let C be the maximal number of clusters for G, let E be the number of elements one tries to cluster (e For each w, one needed to calculate the number of times w occurred with all clusters g2 It is well known that a trigram model outperforms a bigram model if there is sufficient training data Approach a which only uses one clustering function G, could produce |G V different clusterings (for each word in V, it can choose one of the |G| clusters  The optimisation criterion for the extended algorithm is The corresponding clustering algorithm, which is shown in figure , is a straight forward extension of the one given in section  For each w, one needed to calculate the number of times w occurred with all clusters g2 If one wants to use the clustering algorithm on large corpora, the complexity of the algorithm becomes a crucial issue If, based on some heuristic, one could select a fixed number t of target clusters, then one could only try moving w to these tclusters, rather than to all possible clusters C When such a clustering algorithm is applied to a large training corpus, e One then simply calculates the number of clusters that are in both lists and takes this as the heuristic score H(g1  One can cluster calculate the heuristic score of all C clusters in O(C  The heuristic itself is parameterised by h, the number of most frequent clusters one uses to calculate the heuristic score, t, the number of best ranked target clusters one tries to move word w to and u, the number indicating after how many words a full update of the list of most frequent clusters is performed Table contains a comparison of the results using approximately one million words of training data (from the Wall Street Journal corpus) and values t=10, h=10 and u=1000 One can see that the execution time of the standard algorithm seems indeed quadratic in the number of clusters, whereas that of the heuristic version seems to be linear Rather than trying to move each word wto all possible clusters, as the algorithm requires initially, one only tries moving w to a fixed number of clusters that have been selected from all possible clusters by a simple heuristic The clustered models were produced with the extended heuristic version of the algorithm One frequently used approach to alleviate this problem is to construct a clustered language model One can see that the clustered trigram outperforms the clustered bigram, at least with sufficient training data From all the results given here, one can see that the clustered language models can still compete with unclustered models, even when a large corpus, such as the Wall Street Journal corpus, is being used Given these probability estimates pG(w|vM v1 the likelihood FMLof the training data, e75 is used during clustering Taking the logarithm, one obtains the final optimisation criterion F LO  One way to alleviate this problem is to use class based models00002*109  In the following, the optimisation criterion for a bigram based model (e relative frequencies: where N(x) denotes the number of times x occurs in the training data Given these probability estimates pG(w|v the likelihood FMLof the training data, e the probability of the training data being generated by our probability estimates pG(w|v measures how well the training data is represented by the estimates and can be used as optimisation criterion (  However, the problem with this maximum likelihood criterion is that one first estimates the probabilities pG(w|v) on the training data T and then, given pG(w|v one evaluates the classification G on T It divides the data into N-1 samples as retained part and only one sample as held-out part Furthermore, since the clustering of one word affects the future clustering of other words, the order in which words are moved is important.