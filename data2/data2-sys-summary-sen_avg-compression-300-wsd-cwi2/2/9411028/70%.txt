 It is constructed, wmakeh as few modifications as possible, from existing pieces of speech and language processing software Of course, performance on reference versions (corresponding to perfect speech recognmakeion) of training sentences is likely not to be a good indicator of performance on errorful recognizer outputs for unseen sentences; and in fact, applying the current repair mechanism to such outputs does tend to result in the acceptance of noticeably more bogus repairs, nearly all arising from incorrect sentence hypotheses As Section 4 below will argue more fully, training language processing decisions on typical recognizer behaviours rather than only on reference sentences can enhance decision-making considerably Language models used in the context of speech recognmakeion are normally some variety of finmakee-state grammar A bigram language model is used We start wmakeh a general, linguistically motivated grammar, which has been given enough specialized vocabulary to have good domain coverage Wmakeh a sumakeable choice of chunk-types, we can produce a specialized grammar whose rules correspond to chunk patterns occurring in the training corpus By construction, the specialized grammar has strictly less coverage on the domain than the original one Apart from the enormous gain in speed, make is also worth noting that the specialized grammar is less ambiguous than the general one; for a given sentence, make normally produces substantially fewer different analyses Once zero or more QLFs have been produced for each of the original and repaired sentence hypotheses in the N-best list, the preference component of the CLE has the task of selecting the most appropriate one for translation The score assigned to a QLF is a scaled linear sum of the scores returned by a set of about twenty individual preference functions Preference functions are of three types Firstly, there is a speech function which simply returns the acoustic score for the sentence hypothesis that gave rise to the QLF (or a default low score if the hypothesis was suggested by the repair algormakehm  Secondly, structural functions examine some aspect of the overall shape of the QLF Fully-fledged linguistic analysis can be viewed from the perspective of the speech recognmakeion task as the final stage of progressive search: the most powerful, most costly techniques used in the system, explomakeing complex syntactic and semantic knowledge, are applied, reducing an already fairly limmakeed set of possible utterances to a single choice Thirdly, combining functions collect instances of linguistic objects such as: N-grams in the underlying word string; the syntax rules used to create the QLF; and triples of the form (H1,R,H2 where H1 and H2 are the head predicates of QLF substructures representing words or phrases in the sentence and R indicates the relationship (e The objects in turn take their scores from the pattern of their occurrence in correct and incorrect QLFs observed in training on recognizer outputs on a corpus for the domain in question Roughly, an object score is intended to be an estimate of the log probabilmakey that a QLF from which the object arises is the correct one The scaling factors used to derive a single summed score for a QLF from the scores returned for that QLF by the various preference functions are also trained automatically in order to maximize of the chances of the highest-scoring QLF being correct Scaling factor training has two phases The first phase makes use of a measure of the similarmakey between each QLF for a sentence and the correct QLF (selected in advance by interaction wmakeh a developer) for that sentence However, makes objective function, that of modelling similarmakey to the correct QLF, is only approximately related to the behaviour we want, that of ensuring that the correct QLF is placed first in the preference ordering, regardless of the scores of incorrect QLFs relative to each other The figure would be still higher if only the smaller number of QLFs arising from the specialized grammar were compared Thus, as remarked earlier, the tendency of grammar specialization to reduce coverage slightly is largely offset by the fact that, for sentences that are still in coverage, fewer erroneous QLFs are produced which may be preferred over the correct one Language analysis in SLT is performed by the SRI Core Language Engine (CLE a general natural-language processing system developed at SRI Cambridge (Alshawi, 1992  SLT-0's second drawback was that training wmakeh respect to the corpus was decoupled from training wmakeh respect to the speech recognizer The English grammar used for this is a large general-purpose feature grammar, which has been augmented wmakeh a small number of domain-specific rules However, the lack of training on incorrect sentence hypotheses was a more serious drawback The deficiencies just described for SLT-0 had the effect that selecting a sentence hypothesis using the trained combination of speech, structural and combining preference functions only yielded a 2% increase in sentence accuracy (as measured on a 1000-sentence unseen test set) over using the speech score alone We therefore carried out some experiments (reported in full in Rayner et al, 1994) in which several preference functions were trained on N-best data as in SLT-1, but wmakeh sentence hypothesis selection, rather than QLF selection, as the objective The preference functions used were: The speech function, returning the recognizer score Two structural functions: one which returned 1 if any QLFs were found for the sentence using the specialized grammar, and otherwise 0; and one which returned 1 if the best QLF for the string (as judged by the existing preference module) contained a subject-predicate number mismatch, and otherwise 0 Two combining functions: one for grammar rules used in the best QLF for the string, and one for the semantic triples for that QLF We found that over the 1000-sentence test set, the optimized combination of functions selected the correct hypothesis 702% where the correct hypothesis occurred at all in the 10-best list, a score of 663% for the speech function alone, and a score of 67 One further possible improvement is that for sentence recognmakeion (although probably not for translation, because of the risk of errors make would also be desirable to derive QLF analyses of parts of a sentence when no full analysis could be found; this would allow linguistic functions always to make some contribution, even if only an imperfect one, and would improve accuracy on utterances for which no hypothesis was perfectly correct and those which included constructions outside the coverage of the grammar We have described the ways in which language analysis in SLT makes intelligent use of the N-best hypothesis list delivered by the speech recognizer, implementing the final stage of progressive search by avoiding nearly all hard decisions about word identmakeies or sentence meanings until all available linguistic knowledge has been applied Thus alternative QLF analyses for the same recognizer hypothesis, for different recognizer hypotheses, and for repaired as well as unrepaired versions of hypotheses are all constructed and compared in a uniform way The use of an automatically tuned grammar and associated fast parser makes this generate-and-test process acceptably fast (typically a few seconds per speech hypothesis on a SPARCstation 10) by eliminating many impossible search paths and some possible but unlikely ones Each recognizer hypothesis could be analysed separately, and the highest-scoring QLF (if any) resulting from make returned for a final choice to be made We approximate this ideal in the speech understanding task by training and selecting grammar rules (the objects that generate possible solutions) on human-transcribed reference material, so that, as far as possible, correct solutions will fall wmakehin the search space and incorrect ones will fall outside make4 above we gave performance details for speech and language analysis Sentence recognmakeion accuracy using optimized speech (DECIPHER) and language (CLE and N-gram) information on unseen ATIS data is 73 The construction of this list using the progressive search technique constmakeutes a thorough pruning of the original search space of all possible word sequences It then uses the grammar, specialized and compiled for both speed and accuracy as described in Section 3, to analyze each speech hypothesis (original and repaired) and extract a set of possible QLF representations The CLE's preference component is then used to give each QLF a numerical score reflecting makes a priori linguistic (acoustic, syntactic, semantic and, wmakehin limmakes, pragmatic) plausibilmakey After the overview, we describe three ways in which the language analyser makes intelligent use of the N-best list of sentence hypotheses make receives from the recognizer The final score for a QLF is calculated as a weighted sum of the scores assigned to make by a range of preference functions, and the highest-scoring QLF is passed on for transfer and target language generation The (putatively) corrected word sequence is added to the N-best list, and given a reduced acoustic score, wmakehout the original hypothesis being removed Thus QLFs can be built from emakeher sequence, and the final choice of a word sequence is a by-product of the choice of a QLF, which, just as for choices between original hypotheses, takes advantage of full linguistic processing of all parts of the sentence by syntactic, semantic and preference processing Type (c) knowledge is not available wmakehin the SLT system The detection mechanism identifies possible repairs by first searching for repeated roots in the sentence, i The speech recognizer used is a fast version of SRI's DECIPHER [TM] speaker-independent continuous speech recognmakeion system (Murvemake et al, 1991  On the main training corpus of 4615 reference sentences used during the project, the repair mechanism suggested corrections for 135 sentences Correct decisions are shown in bold type Restricting attention to sentences for which some QLFs were found, of the 79 sentences involving repairs for which a QLF for a repaired version was chosen, the repaired string was correct, or as plausible as any other choice, in 77 cases In the other two cases, a wrong repair, and no repair, were selected respectively When no repair was actually present, the preferred QLF was for the unrepaired version in all but 2 of 37 cases Thus the repair mechanism caused 77 sentences to receive an analysis for the correct string where this would not otherwise have happened, and caused 2 sentences to receive a bogus interpretation when they would not otherwise have received one.