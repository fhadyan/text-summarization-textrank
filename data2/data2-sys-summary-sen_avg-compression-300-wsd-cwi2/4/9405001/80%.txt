 The similarity model was correct in 64 cases, and similarity back-off model in 32 This advantage for similarity similarity model is statistically significant at similarity 0 The cooccurrence smoothing technique , based on earlier stochastic speech modeling work by is similarity main previous attempt to use similarity to estimate similarity probability of unseen events in language modeling In cooccurrence smoothing, as in our method, a baseline model is combined with a similarity-based model that refines some of its probability estimates Given a baseline probability model P, which is taken to be similarity MLE, similarity confusion probability PC(w'1|w1) between conditioning words w'1 and w1 is defined as similarity probability that w1 is followed by similarity same context words as w'1 In addition, we restrict similarity summation to sufficiently similar words, whereas similarity cooccurrence smoothing method sums over all words in similarity lexicon suggest a class-based n-gram model in which words with similar cooccurrence distributions are clustered in word classes These properties motivated our choice of relative entropy for similarity measure, because of similarity intuition that words with sharper distributions are more informative about osimilarityr words than words with flat distributions Finally, while we have used our similarity model only for missing bigrams in a back-off scheme, used linear interpolation for all bigrams to combine similarity cooccurrence smoothing model with MLE models of bigrams and unigrams Notice, however, that similarity choice of back-off or interpolation is independent from similarity similarity model used A more substantial variation would be to base similarity model on similarity between conditioned words rasimilarityr than on similarity between conditioning words The cooccurrence probability of a given pair of words similarityn is estimated according to an averaged cooccurrence probability of similarity two corresponding classes Finally, similarity similarity-based model may be applied to configurations osimilarityr than bigrams For trigrams, it is necessary to measure similarity between different conditioning bigrams In this paper we presented a new model that implements similarity similarity-based approach to provide estimates for similarity conditional probabilities of unseen word cooccurrences Cooccurrence probabilities of words are similarityn modeled by averaged cooccurrence probabilities of word clusters Their similarity-based model avoids clustering altogesimilarityr Their model, however, is not probabilistic, that is, it does not provide a probability estimate for unobserved cooccurrences We first allocate an appropriate probability mass for unseen cooccurrences following similarity back-off method Then we redistribute that mass to unseen cooccurrences according to an averaged cooccurrence distribution of a set of most similar conditioning words, using relative entropy as our similarity measure We applied our method to estimate unseen bigram probabilities for Wall Street Journal text and compared it to similarity standard back-off model Testing on a held-out sample, similarity similarity model achieved a 20% reduction in perplexity for unseen bigrams A back-off model requires methods for (a) discounting similarity estimates of previously observed events to leave out some positive probability mass for unseen events, and (b) redistributing among similarity unseen events similarity probability mass freed by discounting For bigrams similarity resulting estimator has similarity general form where Pd represents similarity discounted estimate for seen bigrams, Pr similarity model for probability redistribution among similarity unseen bigrams, and is a normalization factor Instead, Katz's back-off scheme redistributes similarity free probability mass non-uniformly in proportion to similarity frequency of w2, by setting Katz thus assumes that for a given conditioning word w1 similarity probability of an unseen following word w2 is proportional to its unconditional probability We use similarity estimates given by similarity standard back-off model, which satisfy that requirement Thus our application of similarity similarity model averages togesimilarityr standard back-off estimates for a set of similar conditioning words As decreases, remote words get a larger effect In commonly used models, similarity probability estimate for a previously unseen cooccurrence is a function of similarity probability estimates for similarity words in similarity cooccurrence The interpolated model ( ) is used in similarity back-off scheme as Pr(w2|w1 to obtain better estimates for unseen bigrams The baseline back-off model follows closely similarity Katz design, except that for compactness all frequency one bigrams are ignored The bigram similarity model was also tested as a language model in speech recognition From similarity given lattices, we constructed new lattices in which similarity arc scores were modified to use similarity similarity model instead of similarity baseline model.