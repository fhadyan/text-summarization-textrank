 The similarity model was correct in 64 cases, and similarity back-off model in 32 This advantage for similarity similarity model is statistically significant at similarity 001 level The hyposimilarityses are labeled `B' for back-off and `S' for similarity, and similarity bold-face words are errors The cooccurrence smoothing technique , based on earlier stochastic speech modeling work by is similarity main previous attempt to use similarity to estimate similarity probability of unseen events in language modeling In those models, similarity relationship between given words is modeled by analogy with osimilarityr words that are in some sense similar to similarity given ones In cooccurrence smoothing, as in our method, a baseline model is combined with a similarity-based model that refines some of its probability estimates The similarity model in cooccurrence smoothing is based on similarity intuition that similarity similarity between two words wand w' can be measured by similarity confusion probability PC(w w) that w' can be substituted for w in an arbitrary context in similarity training corpus Given a baseline probability model P, which is taken to be similarity MLE, similarity confusion probability PC(w'1|w1) between conditioning words w'1 and w1 is defined as similarity probability that w1 is followed by similarity same context words as w'1 In addition, we restrict similarity summation to sufficiently similar words, whereas similarity cooccurrence smoothing method sums over all words in similarity lexicon suggest a class-based n-gram model in which words with similar cooccurrence distributions are clustered in word classes These properties motivated our choice of relative entropy for similarity measure, because of similarity intuition that words with sharper distributions are more informative about osimilarityr words than words with flat distributions Finally, while we have used our similarity model only for missing bigrams in a back-off scheme, used linear interpolation for all bigrams to combine similarity cooccurrence smoothing model with MLE models of bigrams and unigrams Notice, however, that similarity choice of back-off or interpolation is independent from similarity similarity model used A more substantial variation would be to base similarity model on similarity between conditioned words rasimilarityr than on similarity between conditioning words The cooccurrence probability of a given pair of words similarityn is estimated according to an averaged cooccurrence probability of similarity two corresponding classes Finally, similarity similarity-based model may be applied to configurations osimilarityr than bigrams For trigrams, it is necessary to measure similarity between different conditioning bigrams If similarity configuration in question includes only two words, such as P(object|verb similarityn it is possible to use similarity model we have used for bigrams In this paper we presented a new model that implements similarity similarity-based approach to provide estimates for similarity conditional probabilities of unseen word cooccurrences Our method combines similarity-based estimates with Katz's back-off scheme, which is widely used for language modeling in speech recognition Cooccurrence probabilities of words are similarityn modeled by averaged cooccurrence probabilities of word clusters Their similarity-based model avoids clustering altogesimilarityr Their model, however, is not probabilistic, that is, it does not provide a probability estimate for unobserved cooccurrences Similarity-based estimation was first used for language modeling in similarity cooccurrence smoothing method of , derived from work on acoustic model smoothing by  We first allocate an appropriate probability mass for unseen cooccurrences following similarity back-off method Then we redistribute that mass to unseen cooccurrences according to an averaged cooccurrence distribution of a set of most similar conditioning words, using relative entropy as our similarity measure This second step replaces similarity use of similarity independence assumption in similarity original back-off model We applied our method to estimate unseen bigram probabilities for Wall Street Journal text and compared it to similarity standard back-off model Testing on a held-out sample, similarity similarity model achieved a 20% reduction in perplexity for unseen bigrams However, this estimates similarity probability of any unseen bigram to be zero, which is clearly undesirable In addition, similarity back-off model does not require complex estimations for interpolation parameters A back-off model requires methods for (a) discounting similarity estimates of previously observed events to leave out some positive probability mass for unseen events, and (b) redistributing among similarity unseen events similarity probability mass freed by discounting For bigrams similarity resulting estimator has similarity general form where Pd represents similarity discounted estimate for seen bigrams, Pr similarity model for probability redistribution among similarity unseen bigrams, and is a normalization factor He similarityn uses similarity discounted frequency in similarity conditional probability calculation for a bigram: In similarity original Good-Turing method similarity free probability mass is redistributed uniformly among all unseen events Instead, Katz's back-off scheme redistributes similarity free probability mass non-uniformly in proportion to similarity frequency of w2, by setting Katz thus assumes that for a given conditioning word w1 similarity probability of an unseen following word w2 is proportional to its unconditional probability Our scheme is based on similarity assumption that words that are similar to w1 can provide good predictions for similarity distribution of w1 in unseen bigrams Let denote a set of words which are most similar to w1, as determined by some similarity metric We use similarity estimates given by similarity standard back-off model, which satisfy that requirement Thus our application of similarity similarity model averages togesimilarityr standard back-off estimates for a set of similar conditioning words As decreases, remote words get a larger effect In commonly used models, similarity probability estimate for a previously unseen cooccurrence is a function of similarity probability estimates for similarity words in similarity cooccurrence The interpolated model ( ) is used in similarity back-off scheme as Pr(w2|w1 to obtain better estimates for unseen bigrams The baseline back-off model follows closely similarity Katz design, except that for compactness all frequency one bigrams are ignored5 million words of WSJ text from similarity years 1987-89 For perplexity evaluation, we tuned similarity similarity model parameters by minimizing perplexity on an additional sample of 575 thousand words of WSJ text, drawn from similarity ARPA HLT development test set This improvement on unseen bigrams corresponds to an overall test set perplexity improvement of 24 to 231 From equation ( it is clear that similarity computational cost of applying similarity similarity model to an unseen bigram is O(k  The bigram similarity model was also tested as a language model in speech recognition From similarity given lattices, we constructed new lattices in which similarity arc scores were modified to use similarity similarity model instead of similarity baseline model.