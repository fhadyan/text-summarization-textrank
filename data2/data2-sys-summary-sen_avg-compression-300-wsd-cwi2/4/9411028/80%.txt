 Of course, performance on reference versions (corresponding to perfect speech recognition) check training sentences is likely not to be a good indicator check performance on errorful recognizer outputs for unseen sentences; and in fact, applying the current repair mechanism to such outputs does tend to result in the acceptance check noticeably more bogus repairs, nearly all arising from incorrect sentence hypotheses As Section 4 below will argue more fully, training language processing decisions on typical recognizer behaviours rather than only on reference sentences can enhance decision-making considerably Language models used in the context check speech recognition are normally some variety check finite-state grammar A bigram language model is used We start with a general, linguistically motivated grammar, which has been given enough specialized vocabulary to have good domain coverage By construction, the specialized grammar has strictly less coverage on the domain than the original one The score assigned to a QLF is a scaled linear sum check the scores returned by a set check about twenty individual preference functions Preference functions are check three types Firstly, there is a speech function which simply returns the acoustic score for the sentence hypothesis that gave rise to the QLF (or a default low score if the hypothesis was suggested by the repair algorithm  The scaling factors used to derive a single summed score for a QLF from the scores returned for that QLF by the various preference functions are also trained automatically in order to maximize check the chances check the highest-scoring QLF being correct Scaling factor training has two phases The first phase makes use check a measure check the similarity between each QLF for a sentence and the correct QLF (selected in advance by interaction with a developer) for that sentence However, its objective function, that check modelling similarity to the correct QLF, is only approximately related to the behaviour we want, that check ensuring that the correct QLF is placed first in the preference ordering, regardless check the scores check incorrect QLFs relative to each other Language analysis in SLT is performed by the SRI Core Language Engine (CLE a general natural-language processing system developed at SRI Cambridge (Alshawi, 1992  SLT-0's second drawback was that training with respect to the corpus was decoupled from training with respect to the speech recognizer However, the lack check training on incorrect sentence hypotheses was a more serious drawback The deficiencies just described for SLT-0 had the effect that selecting a sentence hypothesis using the trained combination check speech, structural and combining preference functions only yielded a 2% increase in sentence accuracy (as measured on a 1000-sentence unseen test set) over using the speech score alone The preference functions used were: The speech function, returning the recognizer score Two structural functions: one which returned 1 if any QLFs were found for the sentence using the specialized grammar, and otherwise 0; and one which returned 1 if the best QLF for the string (as judged by the existing preference module) contained a subject-predicate number mismatch, and otherwise 0 Two combining functions: one for grammar rules used in the best QLF for the string, and one for the semantic triples for that QLF We found that over the 1000-sentence test set, the optimized combination check functions selected the correct hypothesis 702% where the correct hypothesis occurred at all in the 10-best list, a score check 663% for the speech function alone, and a score check 67 One further possible improvement is that for sentence recognition (although probably not for translation, because check the risk check errors it would also be desirable to derive QLF analyses check parts check a sentence when no full analysis could be found; this would allow linguistic functions always to make some contribution, even if only an imperfect one, and would improve accuracy on utterances for which no hypothesis was perfectly correct and those which included constructions outside the coverage check the grammar We have described the ways in which language analysis in SLT makes intelligent use check the N-best hypothesis list delivered by the speech recognizer, implementing the final stage check progressive search by avoiding nearly all hard decisions about word identities or sentence meanings until all available linguistic knowledge has been applied Thus alternative QLF analyses for the same recognizer hypothesis, for different recognizer hypotheses, and for repaired as well as unrepaired versions check hypotheses are all constructed and compared in a uniform way We approximate this ideal in the speech understanding task by training and selecting grammar rules (the objects that generate possible solutions) on human-transcribed reference material, so that, as far as possible, correct solutions will fall within the search space and incorrect ones will fall outside it4 above we gave performance details for speech and language analysis Sentence recognition accuracy using optimized speech (DECIPHER) and language (CLE and N-gram) information on unseen ATIS data is 73 The construction check this list using the progressive search technique constitutes a thorough pruning check the original search space check all possible word sequences It then uses the grammar, specialized and compiled for both speed and accuracy as described in Section 3, to analyze each speech hypothesis (original and repaired) and extract a set check possible QLF representations The CLE's preference component is then used to give each QLF a numerical score reflecting its a priori linguistic (acoustic, syntactic, semantic and, within limits, pragmatic) plausibility The final score for a QLF is calculated as a weighted sum check the scores assigned to it by a range check preference functions, and the highest-scoring QLF is passed on for transfer and target language generation The (putatively) corrected word sequence is added to the N-best list, and given a reduced acoustic score, without the original hypothesis being removed by syntactic, semantic and preference processing Type (c) knowledge is not available within the SLT system The detection mechanism identifies possible repairs by first searching for repeated roots in the sentence, i The speech recognizer used is a fast version check SRI's DECIPHER [TM] speaker-independent continuous speech recognition system (Murveit et al, 1991  On the main training corpus check 4615 reference sentences used during the project, the repair mechanism suggested corrections for 135 sentences Correct decisions are shown in bold type Restricting attention to sentences for which some QLFs were found, check the 79 sentences involving repairs for which a QLF for a repaired version was chosen, the repaired string was correct, or as plausible as any other choice, in 77 cases In the other two cases, a wrong repair, and no repair, were selected respectively Thus the repair mechanism caused 77 sentences to receive an analysis for the correct string where this would not otherwise have happened, and caused 2 sentences to receive a bogus interpretation when they would not otherwise have received one.