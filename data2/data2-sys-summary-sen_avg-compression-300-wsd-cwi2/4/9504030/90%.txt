 SPATTER consists of three main decision-tree models: a part-of-speech tagging model, a node-extension model, and a node-labeling model The parsing procedure is a search for the highest probability parse tree SPATTER's search procedure uses a two phase approach to identify the highest probability parse of a sentence Using these two search modes, SPATTER guarantees that it will find the highest probability parse Experimentally, the search algorithm guarantees the highest probability parse is found for over 96% of the sentences parsed7 words I begin by describing decision-tree modeling, showing that decision-tree models are equivalent to interpolated n-gram models The first question a decision tree might ask is: 1 Each node defines a probability distribution on the space of possible decisions A node at which the decision tree stops asking questions is a leaf node A decision-tree model is not really very different from an interpolated n-gram model The number of parameters in this n-gram model is Using this definition, an n-gram model can be represented by a decision-tree model with n-1 questions What is the tag of the word two words back? But can a decision-tree model be represented by an n-gram model? No, but it can be represented by an interpolated n-gram model A decision-tree model can be represented by an interpolated n-gram model as follows A leaf node in a decision tree can be represented by the sequence of question answers, or history values, which leads the decision tree to that leaf These probabilities are estimated using statistical decision tree models The probability of a complete parse tree (T) of a sentence (S) is the product of each decision (di) conditioned on all previous decisions: Each decision sequence constructs a unique parse, and the parser selects the parse whose decision sequence yields the highest cumulative probability A parse tree for a sentence is constructed by starting with the sentence's words as leaves of a tree structure, and labeling and extending nodes these nodes until a single-rooted, labeled tree is constructed A parse tree can be viewed as an n-ary branching tree, with each node in a tree labeled by either a non-terminal label or a part-of-speech label For an n word sentence, a parse tree has n leaf nodes, where the word feature value of the ith leaf node is the ith word in the sentence.