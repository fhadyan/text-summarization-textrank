 27 were ie correctly, ie 23 were ie wrongly, ie 50 were not ie at all, ie one or more of the transitions necessary to find a parse path was lacking, even after generalizing the transitions Not only will this hopefully save a certain amount of drudgery, it should also help to minimize errors and maintain consistency The formalism should be fine-grained, ie In other words, using the formalism one should be able to characterize the structural regularities of language with at least the sophistication of modern competence grammars In the finite-state grammar each word is associated with a transition between two categories, in the tree above `a' with the transition A B and so on can be represented as: \t\tThe \t\tS \t\t[ ] man \t\tN \t\t[VP] gave \t\tVP \t\t[ ] (1a t\tthe \t\tNP \t\t[NP] dog \t\tN \t\t[NP] a \t\tNP \t\t[ ] bone \t\tN \t\t[ ] Intuitively, syntactic links between non-adjacent words, impossible in a standard finite-state grammar, are here established by passing categories along on the stack through the state of intervening words A perusal of the state transitions associated with individual words in (1a) reveals an obvious relationship to the types of categorial grammar which have the respective analyses: \t\tI \t\tS \t\t[ ] saw \t\tVP \t\t[ ] a \t\tNP \t\t[NP(t t\t`t' = dog \t\tN \t\t[NP(t t\t`time adjunct' (3a t\twhich \t\tS(rel) \t\t[NP(t \t\t `rel' = had \t\tVP \t\t[NP(t \t\t `relative' no \t\tNP \t\t[NP(t nose \t\tN \t\t[NP(t yesterday \t\tNP(t) \t\t[ ] \t\tI \t\tS \t\t[ ] saw \t\tVP \t\t[ ] a \t\tNP \t\t[NP(t dog \t\tN \t\t[NP(t (4a t\tyesterday \t\tNP(t) \t\t[S(rel which \t\tS(rel) \t\t[ ] had \t\tVP \t\t[ ] no \t\tNP \t\t[ ] nose \t\tN \t\t[ ] The only transition in (4a) that differs from that of the corresponding word in the `core' variant (3a) is that of `dog' which has the respective transitions: N [NP(t S(rel) [NP(t (in 3a) N [NP(t NP(t) [S(rel (in 4a) Both nouns introduce a relative clause modifier S(rel the difference being that in the discontinuous variant a category has been taken off the stack at the same time as the modifier has been placed on the stack should not only contain information on the structural possibilities of the general language system, but also on details of actual language use in a language community This approach entails however that a corpus has first to be pre-analyzed (ie \t\tFido \t\tS \t\t[ ] had \t\tVP \t\t[ ] a \t\tNP \t\t[NP(t (7a t\tbone \t\tN \t\t[NP(t yesterday \t\tNP(t) \t\t[N [NP(t and \t\tN \t\t[NP(t biscuit \t\tN \t\t[NP(t today \t\tNP(t) \t\t[ ] In this analysis instead of a regular transition for `bone' of: N [NP(t NP(t) [ ] there is instead a transition introducing coordination: N [NP(t NP(t) [N [NP(t Allowing categories on the stack to themselves have non-empty stacks moves the formalism one step further from being an indexed grammar It should be noted that an indefinite amount of centre-embedding can be described, but only at the expense of unlimited growth in the length of states: \t\tThe \t\tS \t\t[ ] fly \t\tN \t\t[VP] the \t\tS(np) \t\t[VP] dog \t\tN \t\t[VP(np VP] (8 t\tthe \t\tS(np) \t\t[VP(np VP] cat \t\tN \t\t[VP(np VP(np VP] scratched \t\tVP(np) \t\t[VP(np VP] swallowed \t\tVP(np) \t\t[VP] died \t\tVP \t\t[ ] This contrasts with unlimited right-recursion where there is no growth in state length: \t\tI \t\tS \t\t[ ] saw \t\tVP \t\t[ ] the \t\tNP \t\t[ ] cat \t\tN \t\t[ ] (9 t\tthat \t\tS(rel) \t\t[ ] scratched \t\tVP \t\t[ ] the \t\tNP \t\t[ ] dog \t\tN \t\t[ ] that \t\tS(rel) \t\t[ ] \t\t  \t\t  Therefore, when tuned to any particular language corpus the resulting grammar will be effectively finite-state2 N [NP(t S(rel) [NP(t t\t02 N [ ] S(rel) [ t\t0 However this simple-minded approach, although easy to implement, in other ways leaves much to be desired Although far from exhausting the possible methods for smoothing, the following three are those used in the implementation described at the end of the paper2 \t\tN [ ] [S(rel t\t02 \t\tN [ ] S(np) [ t\t02 \t\tN [ ] S(rel) [ t\t0 Factor out other features which are merely passed from state to state Establish word paradigms, ie classes of words which occur with similar transitions These paradigms will correspond to a great extent to the word classes of rule-based grammars The advantage would be retained however that the system is still fine-grained enough to reflect the idiosyncratic patterns of individual words and could override this paradigmatic information if sufficient data were available Although essential for effective processing, the smoothing operations may give rise to new problems This penalty may easily be calculated according to the lengths of states in the ie corpus Taking as an example heavy-NP shift suppose that the corpus contained two distinct transitions for the word `threw with the particle `out' both before and after the object threw \t\tVP NP, X(out) \t\tprob: p1 VP X(out NP \t\tprob: p2 Even if p1 were considerably greater than p2, the cumulative negative effect of the longer states in (10) would eventually lead to the model giving the sentence with the shifted NP (11) a higher probability It would not be difficult to make a small extension to the present model to capture such information, namely by introducing an additional feature containing the lexical value of the head of a phrase Transition probabilities were generalized in the ways discussed in the previous section.