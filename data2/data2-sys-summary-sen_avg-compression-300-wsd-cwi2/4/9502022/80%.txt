 Our techniques apply to the feature structures described by Carpenter  which are generated As things stand the stochastic procedure is free to generate structures where , but , which are not in fact legal feature structures This leads to distortions of the probability estimates since the training algorithm spends part of its probability mass on impossible structures Each rule has a probability and the probabilities for all the rules that expand a given non-terminal must sum to one We associate probabilities with partial phrase markers, which are sets of terminal and non-terminal nodes generated by beginning from the starting node successively expanding non-terminal leaves of the partial tree Phrase markers are those partial phrase markers which have no non-terminal leaves The definition of PCFGs implies that the probability of a phrase marker depends only on the choice of rules used in expanding non-terminal nodes Consider the simple grammar in figure and its training against the corpus in figure  Each introduction relationship has a probability and the probabilities for all the introduction relationships that apply to a given non-maximal type must sum to one Secondly, we introduce an additional term in the head of introduction rules to signal the fact that when we apply a particular introduction relationship to a node we also specialize the type of the node by picking exactly one of the direct subtypes of its current type In the probabilistic type hierarchy, it is the iterated introduction relationships which correspond to the context-free rewrite rules of a PCFG The hierarchy whose ALE syntax is given in figure is captured in the new notation by figure We associate probabilities with feature structures, which are sets of maximal and non-maximal nodes generated by beginning from the starting node and successively expanding non-maximal leaves of the partial tree Maximally specified feature structures are those feature structures which have only maximal leaves We start by reviewing the training and use of probabilistic context-free grammars (PCFGs  If F is a feature structure, and F' is a partial feature structure which differs from it only in that a single non-maximal node NT[k] of type T0[k] in F has been refined to type T1[k] expanded to in F then  At this point we have provided a system which allows us to use feature structures instead of PCFGs, but we have not yet dealt with the question of re-entrancy, which forms a crucial part of the expressive power of typed feature structures We then develop a technique to allow analogous probabilistic annotations on type hierarchies The probabilities tell us that the corpus contains no free-standing structures of type num The zero probability of codifies a similar observation that there are no free-standing structures with type phrase Since items of type phrase are never introduced at that type, but only in the form of sub-types, there are no transitions from phrase in the corpus This gives us a clear account of the relationship between a large class of feature structures and their probabilities, but does not treat re-entrancy The essential insight is that the choice of a fully inequated feature structure involving a set of nodes is the same thing as the choice of an arbitrary equivalence relation over these nodes, and this is in turn equivalent to the choice of a partition of the set of nodes into a set of non-empty sets These sets of nodes are equivalence classes The basis of the stochastic procedure for generating fully-inequated feature structures is to interleave the generation of equivalence classes with the expansion from the initial node as described above For the purposes of the expansion algorithm, a fully inequated feature structure consists of a feature tree (as before) and an equivalence relation over all the maximal nodes in that tree.