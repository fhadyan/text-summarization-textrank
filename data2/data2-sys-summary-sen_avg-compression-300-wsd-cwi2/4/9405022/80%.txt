33 and 000 metric Repeat i i+1; ; N[i] N N 3 Until for some appropriate set metric (e We will use a simple interval-bisection technique for finding the appropriate threshold value If then goto else ; 3 If C(N) [ C0 then Shigh Smid else Slow Smid; Nc N; 5 Goto ; 6 Likewise, the coverages of the upper and lower bound may be far apart even though the entropy difference is small, and vice versa The resulting rules will be the set of cut-up training examples A threshold value of say 1 Very briefly, this consists of redoing the derivation of each training example top-down by letting the implicit parse tree drive a rule expansion process, and aborting the expansion of the specialized rule currently being extracted if the current node of the implicit parse tree meets a set of tree-cutting criteria Note that they not correspond to any training example The likelihood for this to happen by chance decreases drastically with increased rule length A second reason for this is that the number of states visited will decrease with increasing reduction length This is in sharp contrast to what the above scheme accomplishes; the corresponding figures are about 20 or 30 percent each for lengths one and two An attempted solution to this problem is to impose restrictions on neighbouring cutnodes It has proved reasonable to assume that the coverage is monotone on both sides of some maximum, which simplifies this task considerably The table below summarizes the results for some grammars of different coverage extracted using: 1 Hand-coded tree-cutting criteria The parse trees are implicit in the sense that each node in the tree is the (mnemonic) name of the grammar rule resolved on at that point, rather than the syntactic category of the LHS of the grammar rule as is the case in an ordinary parse tree Entropy is a measure of disorder Figure shows five examples of implicit parse trees Perplexity is related to entropy as follows We now turn to the task of calculating the entropy of a node in a parse tree In the training set, the LHS PP is attached to the RHS PP of the rule np_np_pp in two cases and to the RHS PP of the rule vp_vp_pp in one case, giving it the entropy  The RHS preposition Prep is always a lexical lookup, and the entropy is thus zero, while the RHS NP in one case attaches to the LHS of rule np_det_np, in one case to the LHS of rule np_num, and in one case is a lexical lookup, and the resulting entropy is thus  The complete table is given here: If we want to calculate the entropy of a particular node in a parse tree, we can either simply use the phrase entropy of the RHS node, or take the sum of the entropies of the two phrases that are unified in this node For example, the entropy when the RHS NP of the rule pp_prep_np is unified with the LHS of the rule np_det_n will in the former case be 110 + 1 A new grammar is created by cutting up each implicit parse tree in the treebank at appropriate points, creating a set of new rules that consist of chunks of original grammar rules33 = 2 Let us call these nodes cutnodes  Calculate the entropy of each or-node where it is difficult to predict which rule will be resolved on next This corresponds exactly to the nodes in the and-or tree that exhibit high entropy values The LHS of each new rule will be the LHS phrase of the original grammar rule at the root of the tree chunk and the RHS will be the RHS phrases of the rules in the leaves of the tree chunk Determine a threshold entropy that yields the desired coverage This can be done using for example interval bisection First, the treebank is partitioned into a training set and a test set The training set will be indexed in an and-or tree and used to extract the specialized rules The test set will be used to check the coverage of the set of extracted rules Then, the set of implicit parse trees is stored in an and-or tree Each such arc leads to an or-node We have now reached a point of recursion and can index the corresponding subtree First we need to calculate the entropy of each or-node This means that if we cut at a node corresponding to e an NP, i Repeat i i+1; N[i] N(N[i-1 3 Until N[i] = N[i-1]4 In the simplest scheme for calculating the entropy of an or-node, only the RHS phrase of the parent rule, i the dominating and-node, contributes to the entropy, and there is in fact no need to employ an and-or tree at all, since the tree-cutting criterion becomes local to the parse tree being cut up In a slightly more elaborate scheme, we sum over the entropies of the nodes of the parse trees that match this node of the and-or tree.