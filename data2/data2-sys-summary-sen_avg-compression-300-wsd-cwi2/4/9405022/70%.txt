 Previous work by Manny Rayner and the author, see attempts to tailor an existing natural-language system to a specific application domain by extracting a specialized grammar from the original one using a large set of training examples The training set is a treebank consisting of implicit parse trees that each specify a verified analysis of an input sentence33 and 000 respectively Return N N Here S(n) is the entropy of node n In a third version of the scheme, the relative frequencies of the daughters of the or-nodes are used directly to calculate the node entropy: Here A is the set of arcs, and is an arc from nto ni This is basically the entropy used in  Repeat i i+1; ; N[i] N N 3 Until for some appropriate set metric (e We will use a simple interval-bisection technique for finding the appropriate threshold value If C(N) [ C0 then Shigh Smid else Slow Smid; Nc N; 5 Goto ; 6 The technical vehicle tailorly used to extract the specialized grammar is explanation-based generalization (EBG see e Likewise, the coverages of the upper and lower bound may be far apart even though the entropy difference is small, and vice versa When retrieving the specialized rules, we will match each training example against the and-or tree The resulting rules will be the set of cut-up training examples A threshold value of say 1 Very briefly, this consists of redoing the derivation of each training example top-down by letting the implicit parse tree drive a rule expansion process, and aborting the expansion of the specialized rule currently being extracted if the current node of the implicit parse tree meets a set of tree-cutting criteria Note that they not correspond to any training example The likelihood for this to happen by chance decreases drastically with increased rule length A second reason for this is that the number of states visited will decrease with increasing reduction length This is in sharp contrast to what the above scheme accomplishes; the corresponding figures are about 20 or 30 percent each for lengths one and two An attempted solution to this problem is to impose restrictions on neighbouring cutnodes The tree-cutting criteria can be local The LHS of the original grammar rule is an NP or dependent on the rest of the parse tree that doesn't dominate the empty string only and tailor choices of nodes to cut at and there is no cut above the current node that is also labelled NP  It has proved reasonable to assume that the coverage is monotone on both sides of some maximum, which simplifies this task considerably The table below summarizes the results for some grammars of different coverage extracted using: 1 Hand-coded tree-cutting criteria In the latter two cases experiments were carried out both with and without the restrictions on neighbouring cutnodes discussed in the tailor section With the mixed entropy scheme it seems important to include the restrictions on neighbouring cutnodes, while this does not seem to be the case with the RHS phrase entropy scheme The parse trees are implicit in the sense that each node in the tree is the (mnemonic) name of the grammar rule resolved on at that point, rather than the syntactic category of the LHS of the grammar rule as is the case in an ordinary parse tree Entropy is a measure of disorder Let the next word be wkand the tailor word string w1 wk-1 Figure shows five examples of implicit parse trees Perplexity is related to entropy as follows Thus, the entropy is the logarithm of the local perplexity at a given point in the word string We now turn to the task of calculating the entropy of a node in a parse tree Assume that we wish to calculate the entropy of the phrases of the rule PP Prep NP, which is named pp_prep_np In the training set, the LHS PP is attached to the RHS PP of the rule np_np_pp in two cases and to the RHS PP of the rule vp_vp_pp in one case, giving it the entropy  The RHS preposition Prep is always a lexical lookup, and the entropy is thus zero, while the RHS NP in one case attaches to the LHS of rule np_det_np, in one case to the LHS of rule np_num, and in one case is a lexical lookup, and the resulting entropy is thus  The complete table is given here: If we want to calculate the entropy of a particular node in a parse tree, we can either simply use the phrase entropy of the RHS node, or take the sum of the entropies of the two phrases that are unified in this node For example, the entropy when the RHS NP of the rule pp_prep_np is unified with the LHS of the rule np_det_n will in the former case be 110 + 1 A new grammar is created by cutting up each implicit parse tree in the treebank at appropriate points, creating a set of new rules that consist of chunks of original grammar rules33 = 2 Let us call these nodes cutnodes  Calculate the entropy of each or-node The rationale for this is that we wish to cut up the parse trees where we can expect a lot of variation i where it is difficult to predict which rule will be resolved on next This corresponds exactly to the nodes in the and-or tree that exhibit high entropy values The LHS of each new rule will be the LHS phrase of the original grammar rule at the root of the tree chunk and the RHS will be the RHS phrases of the rules in the leaves of the tree chunk Determine a threshold entropy that yields the desired coverage This can be done using for example interval bisection Cut up the training examples by matching them against the and-or tree and cutting at the determined cutnodes First, the treebank is partitioned into a training set and a test set The training set will be indexed in an and-or tree and used to extract the specialized rules The test set will be used to check the coverage of the set of extracted rules Then, the set of implicit parse trees is stored in an and-or tree For example, cutting up the first parse tree of Figure at the NP of the rule vp_v_np yields rules 2 and 3 of Figure  Each such arc leads to an or-node We have now reached a point of recursion and can index the corresponding subtree First we need to calculate the entropy of each or-node This means that if we cut at a node corresponding to e an NP, i Repeat i i+1; N[i] N(N[i-1 3 Until N[i] = N[i-1]4 Return N[i Here N(N[j is the set of cutnodes N[j] augmented with those induced in one step by selecting N[j] as the set of cutnodes In practice this was accomplished by compiling an and-or graph from the and-or tree and the set of selected cutnodes, where each set of equated nodes constituted a vertex of the graph, and traversing it In the simplest scheme for calculating the entropy of an or-node, only the RHS phrase of the parent rule, i the dominating and-node, contributes to the entropy, and there is in fact no need to employ an and-or tree at all, since the tree-cutting criterion becomes local to the parse tree being cut up In a slightly more elaborate scheme, we sum over the entropies of the nodes of the parse trees that match this node of the and-or tree.