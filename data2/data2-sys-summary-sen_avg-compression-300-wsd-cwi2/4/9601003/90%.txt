 rating task has connections with SU 1 independent evaluations for spoken and written language engineering Then there's handwriting as well  at the level of user-significant components only 2 But evaluation requires modularity to be enforced everywhere, and that will just block innovation 2: We need to bear in mind that finer granularity may differ substantially between systems, so that in may cases comparative evaluation may not be possible generation ? I don't know what 'willing' is supposed to mean here - this is a very resource-dependent thing phone lattice production ? 3 prosodic marking ? many spoken language systems wont produce any of these native-non-native speakers ? 26 Implementation of Comparative TA SE Two possible scenarios for developing an evaluation programme are the following In this scenario the type of evaluation evolves out of actual funded pilot applications This would be done in the 2nd and final calls the bottom-up scenario ? 2 bottom up  homogeneous What does are required, where appropriate, to participate I am afraid that this will not lead anywhere 2: I don't think either of theses is an effective way to achieve this goal, but 2 comes closest Task Decomposition: There may be more than one way of decomposing a task into constituent sub-tasks7 Other Comments If you have any other comments you would like to make about the way you think TA SE should be carried out in the EC, or about aspects of this questionnaire (omissions or commissions) then please make them here general business letter dictation Strategic Marked and Innovation Watch, as well as with sector-based tasks concerned with rating and evaluation you set up false dichotomies, where one actually has a gradation eg for 2 Clearly some mix is required Not all respondents replied to all questions5 Content of Comparative TA SE a translation system for weather reports and a translation system for airline enquiries One can look at a major task components in a system (e translation, document retrieval, speech recognition) in the context of other major task components in the system Any task or component (including non-linguistic ones) within a system also has an environment A grid evaluation methodology naturally emerges language, subject domain, text type) against different settings of task parameters Users are the prime determinants of system environment variables Assessment comprises verification and validation Having determined these, one way of subjecting a system to user validation is by carrying out a grid evaluation, varying environment variables alongside system parameters So one cannot have `pure' task validation, independent of any user requirements A technology has user validity if it can be deployed within some system meeting genuine user needs Shared tasks within project clusters could include, amongst others, market research and user requirement definitions, the establishment of user forums, data gathering, the testing and rating of generic technologies, and the development of user validation guidelines Therefore, the term technology rating can be somewhat misleading: (user-transparent) task rating might be better terminology Technology validation is just as important as technology verification Internal rating will normally make use of evaluation data specific to the needs of the project: either the user needs, or the technological needs Assessment issues can be divided into those concerning user-centered rating, and those concerning technology rating Given that applications will vary from project to project, this user-specific data may not be directly amenable for use as comparative evaluation data Technological evaluation data will depend on the precise instances of technology used within the project So again, the technological data may not be directly amenable for use as comparative evaluation data Changes in environment might result from: different user groups, different domains, addition of further task requirements, and so on Comparative rating involves taking different systems and comparing either their system-wide performance, and/or the performance of individual components / task competencies More generally Environment: This has several sub-variables, applicable in different cases: User profile (system rating) Overall surrounding task (component/sub-task rating) Domain, text type, etc Channel conditions, speaker accent, etc Other non-linguistic factors Task: One can vary the task or system goals Task Attributes: These include Depth, accuracy, robustness and efficiency Grammars, lexicons, language models etc Task decomposition Implementation: Which covers such things as hardware platforms, operating systems, programming language, code, etc One can fix, vary or ignore combinations of these factors to get different kinds of evaluation, not all of which are strictly comparative Varying things at the system level, one can have, e Flexibility: For an individual system one can vary environment factors such as user profile, domain, etc to see how flexible a system is The core evaluation activity involves providing a system or system component with some input and comparing its output with the results expected But for all these modes of comparison, evaluation data is required Evaluation data comes in three forms 1 Test, or input data 2 Training data Training data may not always be required, but for any system employing statistical methods it is likely to be essential For test data to be realistic, it must be the kind of input data that the system or component would actually receive in real use In collecting evaluation data for rating, a decision must be made about the level of: 1g do we evaluate the system against data from different languages, different domains etc  This section addresses user-centered rating As such it is primarily, though not exclusively, concerned with project internal evaluation; the next section on technology rating takes up the issue of comparative rating The trigger papers, and one reaction paper, are Sparck Jones Crouch: General Technology Assessment Netter: Technology Assessment for Written NL Applications Steeneken: Speech Technology Assessment Adriaens: User-Centered Assessment for RTD in Language Engineering King: Reactions to G alpha test (system or system increment test by other people than the developers, typically prospective end users, possibly still in a controlled environment) 4 acceptance test (a formal test initiated by the prospective procurer/buyer of a system or system increment, typically at the installation site; for custom-built solutions, this is a final test in the test chain for a system or system increment) 5 One possible mode of user-centered comparative evaluation would be to try the user group from one project out on a system from another project having a broadly similar, though not necessarily identical, functionality What could be distilled from these data are abstractions from comparable evaluation scenarios, evaluation metrics etc In the medium and long run this could result in a kind of evaluation library containing user models, abstract scenarios, evaluation methods, test suites, tools, metrics etc which have been successfully applied in user validation task and sub-task performance) is a natural way of pursuing comparative evaluation The second discusses user-centered rating, followed by an extensive discussion of technology rating `Pure' technology evaluation metrics require user-centered validation User-centered validation of technology evaluation metrics has received scant attention On the assumption that technology rating can be carried out using standardised test data, users and systems designers can identify appropriate technological tools without first having to perform user-specific evaluation, where the collection of test data is likely to be expensive A properly set up comparative evaluation can do this, since components of a given system will be run on data from different domains, and where the data arise in response to different sets of user requirements Additionally Comparative rating involves a degree of abstraction away from specific tasks, domains and user groups Performing meaningful technology rating entails the identification of relevant environment and user attributes One possibility would be to set up a small ARPA-like evaluation project under FP-4 However, this would (a) exclude remaining FP-4 projects from comparative evaluation, and (b) does not sit well with emphasis on users in FP-4 the evaluation task would doubtless involve a large degree of artificiality Another way of tackling things would be to build an evaluation exercise bottom-up from existing FP-4 projects, hopefully building on the work done for internal project evaluation corpora and, more importantly, the evaluation data with `answers' for chosen tasks The evaluation structure should allow both technological and user-centered evaluation As far as possible the comparative evaluation exercise should sit on top of, and make use of, project internal evaluation These evaluation points allow for a variety of different kinds of evaluation When a sub-task is user significant, then user-centered evaluation metrics can be applied Generally, we can talk of a system or system component as implementing a task A braided evaluation structure allows for comparative and individual evaluation of different systems at different levels (user-centered, task-specific, general technology  Evaluation data has to be chosen that is sufficiently representative for the task or application to be assessed But in any case, more than corpora will be required to support comparative evaluation using common data Any project that tackles internal evaluation seriously is likely to build up evaluation data in the form of layered corpora, lexicons, terminological databases, databases, etc, as described above Thus, identifying task structure rather than system architecture is the first step towards defining an evaluation framework Many of the performance evaluation methods, such as employed in MUC and TREC, are task specific However, it will provide material for a task specific evaluation, which e So direct, task-specific comparisons between systems employing different retrieval systems may very well not be possible The pair L-OBJ 1 and L-OBJ 2 provides material for evaluation of the retrieval system One could envisage user, task and general technology measures being applied to Task 3 The coarse task decomposition above provides little space for technology rating However, different systems may perform these tasks to different depths of analysis and levels of detail Own-language retrieval would be done only to provide evaluation data In relation to speech processing evaluation in particular, various speech input conditions are possible for Task 1, and there are also speech input and output possibilities at Task 3 Three topics of particular interest in the evaluation of the present state-of-the-art speech technology: user appreciation of spoken language systems; rating of technology modules; and the interaction between spoken language and natural language systems This adaptation requires specific rating methods Total system performance (does the user get the required information efficiently user appreciation and the performance of different technologies are main issues for rating The rating is very much application related and generally requires specific data bases Total system performance : benefit vs Human performance also offers a bench mark for system evaluation Interaction between a human and a system may fail Specific requirements of a certain application may lead to a different rating methods, and at least to different metrics and criteria In general a robust evaluation experiment should include experiments on various items of user appreciation, system performance or technology Three examples are given: Application oriented: The evaluation of a system using speech input and speech output in a dialogue concept, such as used for a travel information system, allows for the rating of the total system (user appreciation and system performance  Combination of Spoken Language and Natural Language Projects: In general SL and NLP use different rating methods and scoring metrics The role of project internal technology rating is likely to be primarily one of producing diagnostic information Internal technology rating may or may not form part of progress evaluation, which would in any case require a large element of user-centered rating To a large degree, internal technology rating is a matter of internal project policy Namely, (a) that the form of project internal technology evaluation can vary from project to project, and (b) that technology evaluation provides the core of comparative rating This suggests that internal and comparative technology rating are very different kinds of animal But what one would like is for comparative technology rating to build on the back of project internal rating Another obvious point to make is that if an evolutionary system development cycle is followed, then snapshots of a system at different times can profitably be regarded, for the purposes of comparative evaluation as different systems for the same domain, task and user group Thus, in order to facilitate comparison and reusability of evaluation data, a limited degree of standardisation is required for project internal technology rating projects themselves should carry out project internal technology rating, though there may be a case for having evaluation data assembled by independent `experts 3 While technology evaluation must form the core of the exercise, user-centered issues may readily be catered for by paying due attention to the environmental aspects of technology rating To a large degree, methods of project internal evaluation will be a matter for negotiation between users and system developers within individual projects However, there are a number of points that are not only desirable for internal evaluation, but which would also facilitate comparative evaluation: Projects should be encouraged to develop a well defined, evolutionary evaluation strategy Early acquisition (and use) of evaluation data should be encouraged Wherever possible, projects should be encouraged to make their internal evaluation strategies, test data, user profiles etc Projects whose task structure includes a particular evaluation point are expected to employ the annotation relevant to that point as part of their internal rating regime (They may of course also apply additional rating measures  One should aim to exploit existing tools where possible, and otherwise distribute development effort over different projects and the evaluation coordinators Using these tools, individual projects should produce annotated answer data from their own evaluation corpora Individual projects should make available as much additional linguistic and non-linguistic data pertinent to evaluation as possible This would allow comparison of technology measures and system adequacy under relatively fixed environmental constraints QUESTIONNAIRE ON ASSESSMENT AND EVALUATION IN LANGUAGE ENGINEERING 1uni-sb user validation; and 20 Questionnaire 21 Introduction and Terminology Terms later used in the specific sense introduced here are capitalised spontaneous speech channel conditions (telephone vs Example To make these distinctions clearer consider this example Relevant linguistic features of the input data include: English language, single speaker, read speech, limited domain, short passages, formulaic style; relevant linguistic features of the output are French language, formulaic style, limited domain Would be willing to conduct it 4 Maybe you should have an option 5 Would be willing to participate 4 4: We already actively participate in common comparative evaluations Not convinced how useful it would be, but that's not what you asked comparative rating is not going to divert substantial resources from internal rating and development; 2 that comparative rating is likely to bring about project internal improvements, in the same way that internal evaluation should; 3 whatever validation the users deem sufficient) ? 2 2: qualitative rating is not enough Once you can do that, the considerations change 2: User validation alone won't promote generality and reusability of components at the level of user-significant components only 2 Therefore technological rating levels are undetournable For semi-objective UV it only makes to look a user-significant components 2: User validation alone won't promote generality and reusability of components only on unseen input-output pairings whose linguistic features match those of the project application 2 2: This answer needs to be qualified This reflects a system's maintainability, portability and flexibility internally by project participants (developers and users) ? 23 above Depends on cases again.