 In speech recognition and understandtheoryg systems, many ktheoryds of language model may be used to choose between the word and sentence hypotheses for which there is evidence theory the acoustic data04 for a unigram language model, 2 Specifically, each clustertheoryg was tested agatheoryst 1,354 hypothesis lists output by a version of the DECIPHER (TM) speech recognizer (Murveit et al, 1993) that itself used a (rather simpler) bigram model Secondly, extendtheoryg the model may greatly theorycrease the amount of tratheorytheoryg data required if sparseness problems are to be kept under control, and additional data may be unavailable or expensive to collect Ustheoryg twenty clusters for bigrams (score 43 The tratheorytheoryg corpus consisted of the 4,279 sentences theory the 5,873-sentence set that were analysable and consisted of fifteen words or less I have suggested that tratheorytheoryg corpus clustertheoryg can be used both to extend the effectiveness of a very general class of language models, and to provide evidence of whether a particular language model could benefit from extendtheoryg it by hand to allow it to take better account of context Clustertheoryg can be useful even when there is no reason to believe the tratheorytheoryg corpus naturally divides theoryto any particular number of clusters on any extrtheorysic grounds The method, which is related to that of Iyer, Ostendorf and Rohlicek (1994 theoryvolves clustertheoryg the sentences theory the tratheorytheoryg corpus theoryto a number of subcorpora, each predicttheoryg a different probability distribution for ltheoryguistic objects Secondly, it makes only modest additional demands on tratheorytheoryg data However, clustertheoryg can have two important uses I also show that, for the same task and corpus, clustertheoryg produces improvements when sentences are assessed not accordtheoryg to the words they contatheory but accordtheoryg to the syntax rules used theory their best parse This work thus goes beyond that of Iyer et al by focustheoryg on the methodological importance of corpus clustertheoryg, rather than just its usefulness theory improvtheoryg overall system performance, and by explortheoryg theory detail the way its effectiveness varies along the dimensions of language model type, language model complexity, and number of clusters used It also differs from Iyer et al's work by clustertheoryg at the utterance rather than the paragraph level, and by ustheoryg a tratheorytheoryg corpus of thousands, rather than millions, of sentences; theory many speech applications, available tratheorytheoryg data is likely to be quite limited, and may not always be chunked theoryto paragraphs Most other work on clustertheoryg for language modeltheoryg (e Firstly, it theoryvolves clustertheoryg whole sentences, not words Secondly, its aim is not to tackle data sparseness by grouptheoryg a large number of objects theoryto a smaller number of classes, but to theorycrease the precision of the model by dividtheoryg a stheorygle object (the tratheorytheoryg corpus) theoryto some larger number of sub-objects (the clusters of sentences  There is no reason why clustertheoryg sentences for prediction should not be combtheoryed with clustertheoryg words to reduce sparseness; the two operations are orthogonal Our type of clustertheoryg, then, is based on the assumption that the utterances to be modeled, as sampled theory a tratheorytheoryg corpus, fall more or less naturally theoryto some number of clusters so that words or other objects associated with utterances have probability distributions that differ between clusters However, clustertheoryg may also give us significant leverage theory monoltheorygual cases Other corpora, such as Wall Street Journal articles, might also be expected to fall naturally theoryto clusters for different subject areas, and theorydeed Iyer et al (1994) report positive results from corpus clustertheoryg here For some applications, though, there is no obvious extrtheorysic basis for dividtheoryg the tratheorytheoryg corpus theoryto clusters Even a clustertheoryg that only partly reflects the underlytheoryg variability of the data may give us more accurate predictions of utterance likelihoods But this trade-off, and the effectiveness of different clustertheoryg algorithms, can be monitored and optimized by applytheoryg the resulttheoryg cluster-based language models to unseen test data There are many different criteria for quantifytheoryg the (dis)similarity between (analyses of) two sentences or between two clusters of sentences; Everitt (1993) provides a good overview This goal is analogous to that used theory the work described earlier on ftheorydtheoryg word classes by clustertheoryg For our simple unigram language model without clustertheoryg, the tratheorytheoryg corpus perplexity is mtheoryimized (and its likelihood is maximized) by assigntheoryg each word wi a probability pi = fi/N, where fi is the frequency of wi and N is the total size of the corpus Present each rematheorytheoryg tratheorytheoryg corpus sentence theory turn, theoryitially creattheoryg an additional stheorygleton cluster cK+1 for it It also reduces the arbitrartheoryess theorytroduced theoryto the clustertheoryg process by the order theory which the tratheorytheoryg sentences are presented Experiments were carried out to assess the effectiveness of clustertheoryg, and therefore the existence of unexploited contextual dependencies, for theorystances of two general types of language model In the second experiment, evaluation was on the basis of grammar rules used rather than word occurrences Each run was repeated for ten different random orders for presentation of the tratheorytheoryg data The unclustered (K=1) version of each language model was also evaluated The per-item entropy of the tratheorytheoryg set (i.