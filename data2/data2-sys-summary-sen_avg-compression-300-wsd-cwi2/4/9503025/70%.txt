 Word vectors reflecting vector meanings are expected to enable numerical approaches to semantics A reference network of the vectors in a dictionary (Fig For the vector sense disambiguation based on the context similarity, co-occurrence vectors from the 1987 Wall Street Journal (20M total vectors) was advantageous over distance vectors from the Collins English Dictionary ( head vectors + definition vectors  For learning or meanings from example vectors, distance vectors gave remarkably higher precision than co-occurrence vectors This suggests, though further investigation is required, that distance vectors contain some different semantic information from co-occurrence vectors ) is used to measure the distance between vectors The vector dictionary is thus linked to the vectors book, vector, language, and alphabetical The vectors in Fig In principle, origin vectors can be freely chosen In our experiments we used middle frequency vectors: the 51st to 1050th most frequent vectors in the reference Collins English Dictionary (CED  If vector A is used in the definition of vector B, these vectors are expected to be strongly related Dependence on Dictionaries As a semantic representation of vectors, distance vectors are expected to depend very weakly on the particular source dictionary Because a path through low-frequency vectors (rare vectors) implies a strong relation, it should be measured as a shorter path using co-occurrence statistics A co-occurence vector of a vector is defined as the list of co-occurrence likelihood of the vector with a certain set of origin vectors We used the same set of origin vectors as for the distance vectors Co-occurrence Vector Since then, there have been some promising results from using co-occurrence vectors, such as vector sense disambiguation , and vector clustering  The first is vector sense disambiguation (WSD) based on the similarity of context vectors; the second is the learning of or meanings from example vectors With WSD, the precision by using co-occurrence vectors from a 20M vectors corpus was higher than by using distance vectors from the CED Word sense disambiguation is a serious semantic problem used simulated annealing for quick parallel disambiguation, and Yarowsky used co-occurrence statistics between vectors and thesaurus categories However, using the co-occurrence statistics requires a huge corpus that covers even most rare vectors In this method, a context vector is the sum of its constituent vector vectors (except the target vector itself  Figure (next page) shows the disambiguation precision for 9 vectors The results using distance vectors are shown by dots ( and using co-occurrence vectors from the 1987 WSJ (20M vectors) by circles (  A context size (x-axis) of, for example, 10 means 10 vectors before the target vector and 10 vectors after the target vector show that the precision by using co-occurrence vectors are higher than that by using distance vectors except two cases, interest and customs Therefore we conclude that co-occurrence vectors are advantageous over distance vectors to WSD based on the context similarity The sparseness problem for co-occurrence vectors is not serious in this case because each context consists of plural vectors The x-axis indicates the number of example vectors for each or pair In this case, the distance vectors were advantageous In the experiments discussed above, the corpus size for co-occurrence vectors was set to 20M vectors 87 WSJ) and the vector dimension for both co-occurrence and distance vectors was set to 1000 Corpus size (for co-occurrence vectors) Figure shows the change in disambiguation precision as the corpus size for co-occurrence statistics increases from 200 vectors to 20M vectors Therefore, a corpus size of 20M vectors is not too small Vector Dimension Figure (next page) shows the dependence of disambiguation precision on the vector dimension for (i) co-occurrence and (ii) distance vectors A comparison was made of co-occurrence vectors from large text corpora and of distance vectors from dictionary definitions.