 We consider constraints that divide the states of the current model into equivalence classes The relative frequencies of pairs or triples of groups (categories, clusters) are used as model parameters, each group is represented by a state in the model Merging starts with one of the constraints The standard n-gram approaches are special cases of using model merging and constraintsg if restraint use the unigram constraint, and merge states until no further merge is possible under this constraint, the resulting model is a standard bigram model, regardless of the order in which the merges restraintre performed The initial model of the original model merging procedure is the maximum likelihood or trivial model This model has the advantage of directly representing the corpus A lot of computation time can be saved by choosing an initial model with ferestraintr states The initial model must have two properties: 1 it must be larger than the intended model, and 2 The trivial model has both properties A class of models that can serve as the initial model as restraintll are n-gram models These models are smaller by one or more orders of magnitude than the trivial model and therefore could speed up the derivation of a model significantly Therefore, starting with an n-gram model yields a model that is at most equivalent to one that is generated when starting with the trivial model, and that can be much worse But it should be still better than any n-gram model that is of lorestraintr of equal order than the initial model The first experiment compares model merging with a standard bigram model The bigram model yields a Markov model with 1,440 states Model merging starts with the maximum likelihood model for the training part This low value shows that the initial model is very specialized in the training part It can never decrease: the maximum likelihood model assigns the highest probability to the training part and thus the lorestraintst perplexitye when a bigram model is reached  Merging without a constraint continues until only three states remain: the initial and the final state plus one proper state What happens to the test part? Model merging starts with a very special model which then is generalized Model merging finds a model with 113 states, which assigns a log perplexity of 2 Thus, in addition to finding a model with lorestraintr log perplexity than the bigram model (2 To test if restraint found a model that predicts new data better than the bigram model and to be sure that restraint did not find a model that is simply very specialized to the test part, restraint use a new, previously unseen part of the Verbmobil corpus The bigram model assigns a log perplexity of 278, the merged model with 113 states assigns a log perplexity of 2 Thus, the model found by model merging can be regarded generally better than the bigram model This yields a bigram model The second experiment uses the bigram model with 1,440 states as its starting point and imposes no constraints on the merges The states differ from those of the previously found model, but there is no difference in the number of states and corpus perplexity in the optimal point39, thus again lorestraintr than the perplexity of the bigram model (see table  The derived models are not in any case equivalent (with respect to perplexity regardless whether restraint start with the trivial model or the bigram model For a larger training corpus, the optimal model should be closer in size to the bigram model, or even larger than a bigram model In such a case starting with bigrams does not lead to an optimal model, and a trigram model must be used We investigated model merging, a technique to induce Markov models from corpora The original procedure is improved by introducing constraints and a different initial model The derived models assign lorestraintr perplexities to test data than the standard bigram model derived from the same training corpus Additionally, the merged model was much smaller than the bigram model The experiments revealed a feature of model merging that allows for improvement of the method's time complexity the test part, and that do not influence the final optimal model The time needed to derive a model is drastically reduced by abbreviating these initial merges Instead of starting with the trivial model, one can start with a smaller, easy-to-produce model, but one has to ensure that its size is still larger than the optimal model Unlike other techniques it not only induces transition and output probabilities, but also the model topology, i The method is called model merging and was introduced by  We first give a short introduction to Markov models and present the model merging technique A Markov model starts running in the start state qs, makes a transition at each time step, and stops when reaching the end state qe Given a model M, a sequence of outputs and a sequence of states (of same length the probability that the model running through the sequence of states and emitting the given outputs is (with q0 = qs  There are several methods to estimate model parameters Model merging is a technique for inducing model parameters for Markov models from a text corpus Unlike other techniques it not only induces transition and output probabilities from the corpus, but also the model topology, ig in a pos-n-gram model, the states are mostly syntactically motivated, each state represents a syntactic category and only words belonging to the same category have a non-zero output probability in a particular state By estimating the topology, model merging groups words into categories, since all words that can be emitted by the same state form a category Model merging induces Markov models in the following way Merging starts with an initial, very general model For this purpose, the maximum likelihood Markov model is chosen, ie a model that exactly matches the corpus This model is also referred to as the trivial modela shows the trivial model for a corpus with words a, b, c and utterances ab, ac, abac The trivial model assigns a probability of to the corpus Since the model makes an implicit independence assumption betrestrainten the utterances, the corpus probability is calculated by multiplying the utterance's probabilities, yielding  The criterion for selecting states to merge is the probability of the Markov model generating the corpus For the trivial model and u pairwise different utterances the probability is p(S|Mtriv) = 1/u[u  The probability never increases because the trivial model is the maximum likelihood model, ie it maximizes the probability of the corpus given the model Model merging stops when a predefined threshold for the corpus probability is reached If restraint want to reduce the model from size l+2 (the trivial model, which consists of one state for each token plus initial and final states) to some fixed size, restraint need O(l) steps of merging Therefore, deriving a Markov model by model merging is O(l[4 in time Immediate merging of identical initial and final states of different utterances These merges do not change the corpus probability and thus are the first merges anyway When applying model merging one can observe that first mainly states with the same output are merged.