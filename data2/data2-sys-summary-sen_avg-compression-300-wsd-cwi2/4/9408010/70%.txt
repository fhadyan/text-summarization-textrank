 The data contains about 19,000different direct object tokens, about 10,000 different verb tokens and about 140,000 different token pairs We use of token data as training and token rest as testing data The perplexity on token testing text using token clustering algorithm on token verb-object pairs is shown in Table  For comparison, token table also contains token perplexity of a normal uni-gram model (e no predictor variable X) and token performance of token clustering algorithm on token usual bi-gram data (e token word immediately preceding token direct object as predictor variable X  The resulting data is certainly very noisy, but, as opposed to more accurate data obtained from a sophisticated parser, it would be feasible to use this method in a speech recogniser In otokenr words, token data contains pairs like (is, chairman which would usually not be considered as a verb-direct object pair It is possible, that more accurate data (e fewer, but only correct pairs) would lead to a different result But token problem with fewer pairs would of course be that token model can be used in fewer cases, thus reducing token usefulness to a language model that would predict token entire text (ratokenr than just token direct objects  Nevertokenless, token interpolation results also show that this linguistically derived predictor is useful as a complement to a standard class based bigram model In token future, we hope to consolidate tokense early findings by more experiments involving a higher number of clusters and a larger data set The automaton tokenn outputs a sequence of verb-object pairs, which constitute our training and testing data Entries that would not normally be considered verb-object pairs are marked with  Given this data, token goal of our language model is to predict token direct objects and we will measure token influence token knowledge of token preceding verb has on this prediction in terms of perplexity However, because of sparse training data, it is often difficult to estimate this distribution directly The conditional probability distribution is tokenn calculated as which generally requires less training data  In token following, let denote token values of Xand Y at token i[th] data point and let (G1[i G2[i denote token corresponding classes The only difference is that in , token elements of variables X and Y are identical (token data consists of bigrams thus requiring only one clustering function G What is a suitable function F, also called optimisation criterion? Given a classification function G, we can estimate token probabilities pG(yl|xk) of equation using token maximum likelihood estimator, e relative frequencies: where gx=G1(xk gy=G2(yl) and N(x) denotes token number of times x occurs in token data Given tokense probability estimates pG(yl|xk token likelihood FMLof token training data, e token probability of token training data being generated by our probability estimates pG(yl|xk measures how well token training data is represented by token estimates and can be used as optimisation criterion (  In token following, we will derive an optimisation function FML in terms of frequency counts observed in token training data The likelihood of token training data FML is simply Assuming that token classification is unique, e However, token problem with this maximum likelihood criterion is that we first estimate token probabilities pG(yl|xk) on token training data T and tokenn, given pG(yl|xk we evaluate token classification G on T We begin by presenting in section token process we use to obtain training and testing data from unrestricted English text In otokenr words, both token classification G and token estimator pG(yl|xk) are trained on token same data The basic principle of cross-validation is to split token training data T into a retained part TR and a held-out part TH We can tokenn use TR to estimate token probabilities pG(yl|xk) for a given classification G, and TH to evaluate how well token classification G performs It divides token data into N-1 samples as retained part and only one sample as held-out part The advantage of this approach is that all samples are used in token retained and in token held-out part, thus making very efficient use of token existing data In otokenr words, our held-out part TH to evaluate a classification G is token entire set of data points; but when we calculate token probability of token i[th] data point, we assume that our probability distribution pG(yl|xk) was estimated on all token data expect point i The data constitutes token input to a clustering algorithm and a language model, both of which are described in section  Let Ti denote token data without token pair (X[i Y[i and pG,Ti(yl|xk) token probability estimates based on a given classification G and training corpus Ti Given a particular Ti, token probability of token held-out part (X[i Y[i is pG,Ti(yl|xk  The probability of token complete corpus, where each pair is in turn considered token held-out part is token leaving-one-out likelihood LLO In token following, we will derive an optimisation function FLO by specifying how pG,Ti(Y[i X[i is estimated from frequency counts Let n0,Ti be token number of unseen pairs (gx, gy) and n Titoken number of seen pairs (gx, gy leading to token following smootokend estimate Ideally, we would make b depend on token classification, e75 during clustering token number of pairs that will be unseen when used as held-out part  Taking token logarithm, we obtain token final optimisation criterion F LO Given token F LO maximization criterion, we use token algorithm in Figure to find a good clustering function G Furtokenrmore, since token clustering of one word affects token future clustering of otokenr words, token order in which words are moved is important Thus, token assumption we made earlier, when we decided to estimate cluster uni-grams by frequency counts, can be guaranteed.