 ride be coded using the features in Fig The prosodic and cue phrase features were motivated by previous results in the literature Passonneau examined some of the few claims relating discourse anaphoric noun phrases to global discourse structure in the Pear corpus Both the hand tuned and automatically derived algorithms improve over our previous algorithms The segmentation algorithms presented in the next two sections were developed by examining only a training set of narratives We currently use 10 narratives for training and 5 narratives for testing The ratios of test to training data measured in narratives, prosodic phrases and clauses, respectively, are 50 For the machine learning algorithm we also estimate performance using cross-validation , as detailed in Section  To quantify algorithm performance, we use the information retrieval metrics shown in Fig Table shows the average human performance for both the training and test sets of narratives mis-classification of boundaries, often occurred where prosodic and cue features conflicted with NP features The original NP algorithm assigned boundaries wherever the three values coref infer global Table presents the average IR scores across the narratives in the training set for both conditions Condition 1 results, the untuned algorithm with the initial feature set, are very similar to the training set except for worse precision This confirms that the tuned algorithm is over calibrated to the training set5 to automatically develop segmentation algorithms from our corpus of coded narratives, where each potential boundary site has been classified and represented as a set of linguistic features5 specifies the names of the classes to be learned (boundary and non-boundary and the names and potential values of a fixed set of coding features (Fig Our training set of 10 narratives provides 1004 examples of potential boundary sites5 is a classification algorithm expressed as a decision tree, which predicts the class of a potential boundary given its set of feature values5 program This decision tree was learned under the following conditions: all of the features shown in Fig were used to code the training data, boundaries were classified as discussed in section , and C4 The decision tree predicts the class of a potential boundary site based on the features before, after, duration, cue1, word1, coref, infer, and global Note that although not all available features are used in the tree, the included features represent 3 of the 4 general types of knowledge (prosody, cue phrases and noun phrases  The results obtained via machine learning are also somewhat better than the results obtained using hand tuning particularly with respect to precision Condition 2 in Table  We also use the resampling method of cross-validation to estimate performance, which averages results over multiple partitions of a sample into test versus training data We performed 10 runs of the learning program, each using 9 of the 10 training narratives for that run's training set (for learning the tree) and the remaining narrative for testing We have presented two methods for developing segmentation hypotheses using multiple linguistic features The first method hand tunes features and algorithms based on analysis of training errors Both methods rely on an enriched set of input features compared to our previous work Note that quantitatively, the machine learning results are slightly better than the hand tuning results Furthermore, note that the machine learning algorithm used the changes to the coding features that resulted from the tuning methods Discourse structures are derived from subjects' segmentations, then statistical measures are used to characterize these structures in terms of acoustic-prosodic features Hearst presented two implemented segmentation algorithms based on term repetition, and compared the boundaries produced to the boundaries marked by at least 3 of 7 subjects, using information retrieval metrics Kozima had 16 subjects segment a simplified short story, developed an algorithm based on lexical cohesion, and qualitatively compared the results We analyzed linear segmentations of 20 narratives performed by naive subjects (7 new subjects per narrative where speaker intention was the segment criterion Subjects were given transcripts, asked to place a new segment boundary between lines (prosodic phrases) wherever the speaker had a new communicative goal, and to briefly describe the completed segment Subjects were free to assign any number of boundaries We found significant agreement among naive subjects on a discourse segmentation task, which suggests that global discourse units have some objective reality Despite this variation, we found statistically significant agreement among subjects across all narratives on location of segment boundaries (  We used three distinct algorithms based on the distribution of referential noun phrases, cue words, and pauses, respectively NP-A used three features, while CUE-A and PAUSE-A each made use of a single feature However, we also found poor correlation of three untuned algorithms (based on features of referential noun phrases, cue words, and pauses, respectively) with the subjects' segmentations We represent each narrative in our corpus as a sequence of potential boundary sites, which occur between prosodic phrases Agreement among subjects on boundaries was significant at below the  The boxes in the figure show the subjects' responses at each potential boundary site, and the resulting boundary classification Each potential boundary site in our corpus is coded using the set of linguistic features shown in Fig The cue phrase features are also obtained by automatic analysis of the transcripts The third feature, global global illustrates how the first boundary site in Fig.