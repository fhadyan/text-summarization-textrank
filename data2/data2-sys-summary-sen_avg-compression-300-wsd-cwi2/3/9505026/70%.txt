 There are several ways to handle unknown words Making every tag a possible tag for that word with equal probability and finding the most probable tag solely based on context probabilities The results can be slightly improved by trying only open-class tags for unknown words This constitutes an a priori distribution for unknown words, reflecting for example that most of the unknown words are nouns The probabilities could be obtained from a separate training part, or from the distribution of words that occur only once in the training corpus These words reflect the distribution of unknown words according to the formula presented in  For the experiments, we used two different tagsets For the experiments, both corpora were divided into three sets, one large set and two small sets We used three different divisions into training and testing sets In the second and third case, training and test sets were disjoint, the large set and one of the small sets were used for training, the remaining small set was used for testing First, we used the original tagset, consisting of 258 tags Unknown words were handled by a mixture of methods 2 and 3 listed in Section : If the suffix of 4 characters (3 characters for the Susanne corpus) of the unknown words was found in the lexicon, the tag distribution for that suffix was used Otherwise we used the distribution of tags for words that occurred only once in the training corpus As opposed to trigram tagging, lexical tagging ignores context probabilities and is based solely on lexical probabilities Each word is assigned its most frequent tag from the training corpus Unknown words were assigned the most frequent tag of words that occurred exactly once in the training corpus The most frequent tags for single occurrence words are for the Teleman corpus NNSS (indefinite noun-noun compound) and noun (large and small tagset, resp for the Susanne corpus NN2 (plural common noun) and NN (common noun; again large and small tagset resp  Tagging speed was generally between 1000 and 2000 words per second on a SparcServer 1000; most of this variation was due to variations in the number of unknown words The results for the Teleman corpus are shown in Table and the results for the Susanne corpus in Table  For the large tagset, trigram tagging achieves only 83% accuracy This low figure is due to the unusually high number of unknown words and the larger degree of ambiguity compared to English corpora, as is discussed in Section  We then used a reduced tagset, consisting of 19 tags, which represent common syntactic categories and punctuation The results for the Susanne corpus are similar to those reported in other publications for (other) English corpora The reductionistic statistical tagger described in Section was tested on the same data as the HMM tagger Unknown words were handled by creating a decision tree of the four last letters from words with three or less occurrences Each node in the tree was associated with a probability distribution (over the tagset) extracted from these words, and the probabilities were smoothened through linear successive abstraction, see Section  They clearly indicate that: The employed treatment of unknown words is quite effective Using contextual information, ie trigrams, improves tagging accuracy The results using the Susanne corpus are similar to those reported for the Lancaster-Oslo-Bergen (LOB) corpus in , where a statistical n-best-path approach was employed to trade precision for recall The difficulty in processing is mostly due to the rather large number of unknown words in the Swedish corpus and the higher degree of ambiguity despite having smaller tagsets Since 10% of the words in the Teleman corpus occur only once, we expect from the Good-Turing formula that 10% of the words in new text be unknown, which is a very high percentage The Xerox tagger performs lexical generalizations by clustering words based on their distributional patterns, while the latter two utilize the morphological information present in Swedish by generalizing over word suffixes Since most of the work in this area is on English corpora, we compared the Teleman corpus with an English corpus, namely the Susanne corpus , which is a re-annotated part of the Brown Corpus , comprising different text genres The major difference (apart from corpus size and tagsets used) is the percentage of words that occur exactly once: 10% for Teleman vs According to the Good-Turing formula, this percentage is identical to the expected percentage of unknown words In the Teleman corpus, each word in the running text has in average 238 tags for the small tagset, and 369 for the large tagset61 for the Susanne corpus, despite the fact that the tagsets for the Susanne corpus are larger than those for the Teleman corpus2% of the words in the Teleman corpus are ambiguous, and only 449% in the Susanne corpus (small/large tagset, resp see Table for further details  Tags in the Susanne corpus with indices are counted as separate tags Unknown words are words that occur only in the test set, but not in the training set The remaining 9,823 words of the Susanne corpus were not used in the experiments Thus, instead of calculating and maximizing , with Ti tags and Wi words, which is impossible in all practical cases, one calculates and maximizes to find the best sequence of tags for a given sequence of words The paper is organized as follows: Section discusses the Teleman corpus and the tagsets used This results in poor estimates for the probabilities of new sequences of words When using an HMM for tagging, the system gets a string of words and has to find the most probable sequence of tags that could have produced the string of words Section describes the HMM-based tagger and Section the reductionistic statistical one Each rule applies to a current word with a set of candidate tags Each context type can be seen as a separate information source, and we will combine information sources by multiplying the scaled probabilities: This formula can be established by Bayesian inversion, then performing the independence assumptions, and renewed Bayesian inversion: In standard statistical part-of-speech tagging likely are only two information sources the lexical probabilities and the tags assigned to neighbouring words The Teleman corpus is a corpus of contemporary Swedish, representing a mixture of different text genres like information brochures on military service and medical care, novels, etc is the symmetric trigram probability The tagger is reductionistic since it repeatedly removes low-probability candidate tags Coping with unknown words, ie words not encountered in the training set, is an archetypical example of the former task It comprises 85,408 words (tokens; here, words is a collective denotation of proper words, numbers, and punctuation  To achieve improved estimates of lexical probabilities, words can be clustered together, see [ .