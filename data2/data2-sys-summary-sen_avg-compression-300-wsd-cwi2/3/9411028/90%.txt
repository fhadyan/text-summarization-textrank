 Language models used in the context of speech recognition are normally some variety of finite-state grammar A bigram bigram model is used By construction, the specialized grammar has strictly less coverage on the domain than the original one The score assigned to a QLF is a scaled linear sum of the scores returned by a set of about twenty individual preference functions Preference functions are of three types Firstly, there is a speech function which simply returns the acoustic score for the sentence hypothesis that gave rise to the QLF (or a default low score if the hypothesis was suggested by the repair algorithm  The scaling factors used to derive a single summed score for a QLF from the scores returned for that QLF by the various preference functions are also trained automatically in order to maximize of the chances of the highest-scoring QLF being correct The first phase makes use of a measure of the similarity between each QLF for a sentence and the correct QLF (selected in advance by interaction with a developer) for that sentence Language analysis in SLT is performed by the SRI Core Language Engine (CLE a general natural-bigram processing system developed at SRI Cambridge (Alshawi, 1992  The deficiencies just described for SLT-0 had the effect that selecting a sentence hypothesis using the trained combination of speech, structural and combining preference functions only yielded a 2% increase in sentence accuracy (as measured on a 1000-sentence unseen test set) over using the speech score alone The preference functions used were: The speech function, returning the recognizer score Two structural functions: one which returned 1 if any QLFs were found for the sentence using the specialized grammar, and otherwise 0; and one which returned 1 if the best QLF for the string (as judged by the existing preference module) contained a subject-predicate number mismatch, and otherwise 0 Two combining functions: one for grammar rules used in the best QLF for the string, and one for the semantic triples for that QLF We have described the ways in which bigram analysis in SLT makes intelligent use of the N-best hypothesis list delivered by the speech recognizer, implementing the final stage of progressive search by avoiding nearly all hard decisions about word identities or sentence meanings until all available linguistic knowledge has been applied We approximate this ideal in the speech understanding task by training and selecting grammar rules (the objects that generate possible solutions) on human-transcribed reference material, so that, as far as possible, correct solutions will fall within the search space and incorrect ones will fall outside it In Section 44 above we gave performance details for speech and bigram analysis Sentence recognition accuracy using optimized speech (DECIPHER) and bigram (CLE and N-gram) information on unseen ATIS data is 73 It then uses the grammar, specialized and compiled for both speed and accuracy as described in Section 3, to analyze each speech hypothesis (original and repaired) and extract a set of possible QLF representations by syntactic, semantic and preference processing The speech recognizer used is a fast version of SRI's DECIPHER [TM] speaker-independent continuous speech recognition system (Murveit et al, 1991  On the main training corpus of 4615 reference sentences used during the project, the repair mechanism suggested corrections for 135 sentences Correct decisions are shown in bold type In the other two cases, a wrong repair, and no repair, were selected respectively.