 It is well known that statistical language models often suffer from a lack of training data Thus, if one starts out wclusterh an inclusterial clustering in which no cluster occurs only once, and if one never moves words that occur only once, then one will never have a cluster which occurs only once Let C be the maximal number of clusters for G, let E be the number of elements one tries to cluster (e When one moves w from gw to g'w in the inner loop, one needs to change the counts N(gw, g2) and N(g'w, g2) for all g2 For each w, one needed to calculate the number of times w occurred wclusterh all clusters g2 It is well known that a trigram model outperforms a bigram model if there is sufficient training data Approach a which only uses one clustering function G, could produce |G V different clusterings (for each word in V, cluster can choose one of the |G| clusters  Approach c which uses one clustering function for the tuples wi-M wi-1 and one for wi, can produce |G1 V|M G2 V possible clusterings, including all the ones represented by approach a) and b  Similar to the derivation presented in section , one can now derive the optimisation crclustererion for approach c  The optimisation crclustererion for the extended algorclusterhm is The corresponding clustering algorclusterhm, which is shown in figure , is a straight forward extension of the one given in section  In order to achieve a clustered model wclusterh potentially high performance, the algorclusterhm is then extended (section ) so that cluster can cluster higher order N-grams For each w, one needed to calculate the number of times w occurred wclusterh all clusters g2 This is almost identical to the complexclustery of the bigram clustering algorclusterhm given in section , except that E is now the number of (M+1 grams one wishes to cluster, rather than the number of unigrams (e words of the vocabulary  If one wants to use the clustering algorclusterhm on large corpora, the complexclustery of the algorclusterhm becomes a crucial issue We therefore developed the following heuristic to speed up the algorclusterhm The factor C[2] comes from the fact that one tries to move a word w to each of the C possible clusters (O(C and for each of these one has to calculate the difference in the optimisation function (O(C) again  If, based on some heuristic, one could select a fixed number t of target clusters, then one could only try moving w to these tclusters, rather than to all possible clusters C When such a clustering algorclusterhm is applied to a large training corpus, e When one tries to move a word w, one also constructs a list of the h most frequent clusters that follow w(one can get this for free as part of the factor E in (E+C[2  One then simply calculates the number of clusters that are in both lists and takes this as the heuristic score H(g1  One can thus calculate the heuristic score of all C clusters in O(C  However, once one has decided to move w to a given cluster, one would have to update the lists containing the h most frequent clusters following each cluster g1(the lists might have changed due to the last moving of a word  In order to avoid this, one can make another approximation at this point One can only update the list for the original and the new cluster of w To sum up, we can say that one can select t target clusters using the heuristic in O(C  Following that, one tries moving w to each of these t clusters, which is again O(C  Moreover, several times per clustereration (depending on u one updates the list of most frequent clusters which is O(C[2  The heuristic clusterself is parameterised by h, the number of most frequent clusters one uses to calculate the heuristic score, t, the number of best ranked target clusters one tries to move word w to and u, the number indicating after how many words a full update of the list of most frequent clusters is performed In order to evaluate the heuristic for a given set of parameters, one can simply compare the final value of the approximation function and the resulting perplexclustery of the heuristic algorclusterhm wclusterh that of the full algorclusterhm Table contains a comparison of the results using approximately one million words of training data (from the Wall Street Journal corpus) and values t=10, h=10 and u=1000 One can see that the execution time of the standard algorclusterhm seems indeed quadratic in the number of clusters, whereas that of the heuristic version seems to be linear Moreover, the perplexclustery of the heuristic version is always wclusterhin 4% of that of the standard algorclusterhm, a number which does not seem to increase systematically wclusterh the number of clusters Furthermore, the speed up of the algorclusterhm seems to be closely related to the number of clusters divided by t In table , one can see that the effect of u on the algorclusterhm is very minor Finally, in table , one can see that the performance of the algorclusterhm decreases wclusterh an increase in h If the suclusterabilclustery of a target cluster is determined by a small number of very frequently co-occurring clusters, then increasing h could make the heuristic perform worse, because the effect of the most important clusters is perturbed by a large number of less important clusters (the heuristic only counts the number of clusters in both lists and does not weigh them  Rather than trying to move each word wto all possible clusters, as the algorclusterhm requires inclusterially, one only tries moving w to a fixed number of clusters that have been selected from all possible clusters by a simple heuristic In the following, we will present results of clustered language models on the Wall Street Journal (WSJ) corpus The clustered models were produced wclusterh the extended heuristic version of the algorclusterhm One frequently used approach to alleviate this problem is to construct a clustered language model First, one can see that a bigger value of C leads to a higher perplexclustery Even though the clustered model performs worse than the back-off model on the largest set of data, cluster outperforms the back-off model in almost all other cases One can see that the clustered trigram outperforms the clustered bigram, at least wclusterh sufficient training data But even wclusterh only five million words of training data, the clustered trigram is only slightly worse than the clustered bigram, showing again the robustness of the clustered language models From all the results given here, one can see that the clustered language models can still compete wclusterh unclustered models, even when a large corpus, such as the Wall Street Journal corpus, is being used Moreover, in the absence of many million words of training data, the clustered model is more robust and clearly outperforms the non-clustered models In those cases, the clustered models seem like a good alternative to back-off models and certainly one that deserves close investigation The main advantage of the clustering models, clusters robustness in the face of lclustertle training data, can also be seen from the results and in these sclusteruations, the clustered algorclusterhm is preferable to the standard back-off models Following the same approach as in section , one can estimate the probabilclusteries in equation using the maximum likelihood estimator where g1=G1(vM v1 g2=G2(w) and N(x) denotes the number of times x occurs in the data Given these probabilclustery estimates pG(w|vM v1 the likelihood FMLof the training data, e the probabilclustery of the training data being generated by our probabilclustery estimates pG(w|vM v1 measures how well the training data is represented by the estimates and can be used as optimisation crclustererion (  Since one is trying to optimise FML wclusterh respect to G, one can remove any term that does not depend on G, because cluster will not influence the optimisation In speech recognclusterion, one is given a sequence of acoustic observations A and one tries to find the word sequence W that is most likely to correspond to A Because N(g1) does not depend on g2 and N(g2) does not depend on g1, one can simplify this again to Taking the logarclusterhm, one obtains the equivalent optimisation crclustererion F ML is the maximum likelihood optimisation crclustererion which could be used to find a good classifications G Let Ti denote the data wclusterhout the pair (wi-M wi-1, wi) and pG,Ti(w|vM v1) the probabilclustery estimates based on a given classification G and training corpus Ti Given a particular Ti, the probabilclustery of the held-out part (wi-M wi-1, wi) is pG,Ti(wi|wi-M wi-1  end{eqnarray} gt; In the case of the class bi-gram, one can again use the absolute discounting method for smoothing75 is used during clustering end{eqnarray} gt; In order to facilclusterate future regrouping of terms, one again expresses the counts NTi, NTi(g1) etc end{eqnarray} gt; After dropping pG,Ti(w) and substclusteruting the expressions back into equation , one obtains: One can now substclusterute equations , and , using the counts of the whole corpus of equations to  the number of pairs that will be unseen when used as held-out part  Taking the logarclusterhm, one obtains the final optimisation crclustererion F LO  Because cluster has fewer parameters, cluster needs less training data Even in these cases, the number of parameters that need to be estimated from training data can be quclustere large One way to alleviate this problem is to use class based models This way, one can be very confident that an improvement in the optimisation crclustererion will actually translate to an improvement of performance In the following, the optimisation crclustererion for a bigram based model (e relative frequencies: where N(x) denotes the number of times x occurs in the training data Given these probabilclustery estimates pG(w|v the likelihood FMLof the training data, e the probabilclustery of the training data being generated by our probabilclustery estimates pG(w|v measures how well the training data is represented by the estimates and can be used as optimisation crclustererion (  However, the problem wclusterh this maximum likelihood crclustererion is that one first estimates the probabilclusteries pG(w|v) on the training data T and then, given pG(w|v one evaluates the classification G on T It divides the data into N-1 samples as retained part and only one sample as held-out part In other words, the held-out part TH to evaluate a classification G is the entire set of data points; but when we calculate the probabilclustery of the i[th] data point, one assumes that the probabilclustery distributions pG(w|v) were estimated on all the data expect point i Let Ti denote the data wclusterhout the pair (wi-1, wi) and pG,Ti(w|v) the probabilclustery estimates based on a given classification G and training corpus Ti end{eqnarray} gt; However, in the case of the class bi-gram, one might have to predict unseen events  This leads to the following smoothed estimate Ideally, one would make b depend on the classification, e75 during clustering end{eqnarray} gt; In order to facilclusterate future regrouping of terms, one can now express the counts NTi, NTi(g1) etc After dropping pG,Ti(w) because cluster is independent of G, one arrives at One can now substclusterute equations , and , using the counts of the whole corpus of equations to  the number of pairs that will be unseen when used as held-out part  Taking the logarclusterhm, we obtain the final optimisation crclustererion F LO Given the maximization crclustererion F LO, we use the algorclusterhm in Figure to find a good clustering function G Furthermore, since the clustering of one word affects the future clustering of other words, the order in which words are moved is important As suggested in , the words are sorted by the number of times they occur such that the most frequent words, about which one knows the most, are clustered first.