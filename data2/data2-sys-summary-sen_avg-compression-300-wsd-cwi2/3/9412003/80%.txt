 Thus, if one starts out with an initial clustering in which no cluster occurs only once, and if one never moves words that occur only once, then one will never have a cluster which occurs only once Let C be the maximal number of clusters for G, let E be the number of elements one tries to cluster (e When one moves w from gw to g'w in the inner loop, one needs to change the counts N(gw, g2) and N(g'w, g2) for all g2 For each w, one needed to calculate the number of times w occurred with all clusters g2 It is well known that a trigram model outperforms a bigram model if there is sufficient training data Approach a which only uses one clustering function G, could produce |G V different clusterings (for each word in V, it can choose one of the |G| clusters  Approach c which uses one clustering function for the tuples wi-M wi-1 and one for wi, can produce |G1 V|M G2 V possible clusterings, including all the ones represented by approach a) and b  Similar to the derivation presented in section , one can now derive the optimisation criterion for approach c  The optimisation criterion for the extended algorithm is The corresponding clustering algorithm, which is shown in figure , is a straight forward extension of the one given in section  For each w, one needed to calculate the number of times w occurred with all clusters g2 This is almost identical to the complexity of the bigram clustering algorithm given in section , except that E is now the number of (M+1 grams one wishes to cluster, rather than the number of unigrams (e If one wants to use the clustering algorithm on large corpora, the complexity of the algorithm becomes a crucial issue If, based on some heuristic, one could select a fixed number t of target clusters, then one could only try moving w to these tclusters, rather than to all possible clusters C When such a clustering algorithm is applied to a large training corpus, e When one tries to move a word w, one also constructs a list of the h most frequent clusters that follow w(one can get this for free as part of the factor E in (E+C[2  One then simply calculates the number of clusters that are in both lists and takes this as the heuristic score H(g1  One can cluster calculate the heuristic score of all C clusters in O(C  However, once one has decided to move w to a given cluster, one would have to update the lists containing the h most frequent clusters following each cluster g1(the lists might have changed due to the last moving of a word  In order to avoid this, one can make another approximation at this point To sum up, we can say that one can select t target clusters using the heuristic in O(C  Following that, one tries moving w to each of these t clusters, which is again O(C  The heuristic itself is parameterised by h, the number of most frequent clusters one uses to calculate the heuristic score, t, the number of best ranked target clusters one tries to move word w to and u, the number indicating after how many words a full update of the list of most frequent clusters is performed In order to evaluate the heuristic for a given set of parameters, one can simply compare the final value of the approximation function and the resulting perplexity of the heuristic algorithm with that of the full algorithm Table contains a comparison of the results using approximately one million words of training data (from the Wall Street Journal corpus) and values t=10, h=10 and u=1000 One can see that the execution time of the standard algorithm seems indeed quadratic in the number of clusters, whereas that of the heuristic version seems to be linear Moreover, the perplexity of the heuristic version is always within 4% of that of the standard algorithm, a number which does not seem to increase systematically with the number of clusters In table , one can see that the effect of u on the algorithm is very minor Finally, in table , one can see that the performance of the algorithm decreases with an increase in h Rather than trying to move each word wto all possible clusters, as the algorithm requires initially, one only tries moving w to a fixed number of clusters that have been selected from all possible clusters by a simple heuristic The clustered models were produced with the extended heuristic version of the algorithm One frequently used approach to alleviate this problem is to construct a clustered language model First, one can see that a bigger value of C leads to a higher perplexity One can see that the clustered trigram outperforms the clustered bigram, at least with sufficient training data But even with only five million words of training data, the clustered trigram is only slightly worse than the clustered bigram, showing again the robustness of the clustered language models From all the results given here, one can see that the clustered language models can still compete with unclustered models, even when a large corpus, such as the Wall Street Journal corpus, is being used Moreover, in the absence of many million words of training data, the clustered model is more robust and clearly outperforms the non-clustered models In those cases, the clustered models seem like a good alternative to back-off models and certainly one that deserves close investigation Following the same approach as in section , one can estimate the probabilities in equation using the maximum likelihood estimator where g1=G1(vM v1 g2=G2(w) and N(x) denotes the number of times x occurs in the data Given these probability estimates pG(w|vM v1 the likelihood FMLof the training data, e the probability of the training data being generated by our probability estimates pG(w|vM v1 measures how well the training data is represented by the estimates and can be used as optimisation criterion (  Let Ti denote the data without the pair (wi-M wi-1, wi) and pG,Ti(w|vM v1) the probability estimates based on a given classification G and training corpus Ti75 is used during clustering end{eqnarray} gt; In order to facilitate future regrouping of terms, one again expresses the counts NTi, NTi(g1) etc the number of pairs that will be unseen when used as held-out part  Taking the logarithm, one obtains the final optimisation criterion F LO  Because it has fewer parameters, it needs less training data One way to alleviate this problem is to use class based models00002*109  In the following, the optimisation criterion for a bigram based model (e relative frequencies: where N(x) denotes the number of times x occurs in the training data Given these probability estimates pG(w|v the likelihood FMLof the training data, e the probability of the training data being generated by our probability estimates pG(w|v measures how well the training data is represented by the estimates and can be used as optimisation criterion (  However, the problem with this maximum likelihood criterion is that one first estimates the probabilities pG(w|v) on the training data T and then, given pG(w|v one evaluates the classification G on T It divides the data into N-1 samples as retained part and only one sample as held-out part In other words, the held-out part TH to evaluate a classification G is the entire set of data points; but when we calculate the probability of the i[th] data point, one assumes that the probability distributions pG(w|v) were estimated on all the data expect point i Let Ti denote the data without the pair (wi-1, wi) and pG,Ti(w|v) the probability estimates based on a given classification G and training corpus Ti75 during clustering the number of pairs that will be unseen when used as held-out part  Furthermore, since the clustering of one word affects the future clustering of other words, the order in which words are moved is important.