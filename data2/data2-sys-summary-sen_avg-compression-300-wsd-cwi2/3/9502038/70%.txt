 Since the late '80s part-of-speech (POS) disambiguation using Hidden Markov Models (HMM) has been a widespread method for tagging texts The version of the Xerox tagger used for the implementation described here is the DDS Tagger version 1 adjective (fehlgeschlagen, bekannt with the latter reflecting a subtle distinction in the German tag set (VPP vs In German most of the effort is going into subclassification within major word classes, while in English and French a good deal of disambiguation work is devoted to separate major word classes Again, errors mainly affect the assignment of words to subclasses within one major word class Table shows the most common errors produced by the German model The first column gives the relative frequenciy and the second column lists the tag chosen by the German HMM tagger Implementing a new language model for this tagger involves supplying: 1 While this technique helps to confine word senses for English and French, it is of little help for word sense disambiguation in German The tagger was used to annotate all of the 50 million word German corpora contained on the ECI Multilingual Corpus 1 CD-ROM a definition of the tag set to be used by the HMM, 2 a lexicon listing word forms with their equivalence classes, that is the list of POS tags that can be assigned to the word form, 3 a set of initial transition biases, 5 a set of initial symbol biases, 6 a sufficiently large text for training the HMM, 7 a reference text with correctly assigned POS tags, 8 (1) The tag set used in the implementation is the smaller version of the two tag sets developed jointly by the Universities of Stuttgart and Tbingen, referred to as the ELWIS tag set (cf It consists of a total of 42 POS tags plus three tags for punctuation and a special tag for truncated words The tags used in the ELWIS tag set are given in table  (2) For each word form the lexicon gives the set of POS tags that can be assigned to that word Since then the original tag set was redefined, making the tagged corpus used to train the LIKELY tagger obsolete (3) Class guessers for the Xerox tagger assign potential POS tags to unknown words according to a surface analysis of the word form (4) The model is trained using a set of initial transition biases, including both positive and negative constraints on tag sequences Although the model can be trained without initial biases, the performance of the resulting model increases significantly if appropriate initial biases are used The biases in the model consisted for the most part in specifications of the most plausible successor tags for each tag in the tag set (5) Initial symbol biases are an additional set of biases used to define preferences for tag assignment given a particular equivalence class Only a very few symbol biases were defined before evaluation of the first training runs, mainly to reflect biases towards equivalence classes used in the class guesser (6) The two texts used for training the HMM were selected from the German data contained on the ECI's Multilingual CD-ROM : a 200,000 and 2,000,000 word sample from Summer 1992 issues of the German newspaper Frankfurter Rundschau Training texts are pretokenized using an external tokenizer written in lex The best results of this implementation were obtained running 20 iterations of training over the 200,000 word training text, using a total of 50 transition and 17 symbol biases With this configuration of training parameters, the resulting HMM assigned incorrect tags when run on the reference texts and compared with the manually assigned tags However, the performance of the resulting HMM is very poor if no initial biases are used to help the training process find suitable parameters For comparison, the evaluation procedure used to evaluate the implementation of the HMM tagger described in the preceding section was repeated without using any of the initial biases Choosing initial biases to help train a model is a subtle task in that it not only requires sound knowledge of the tag set used and the target language the model is aiming at, but it also requires a feel for how the initial biases may be modified during a given number of training iterations In such a setting, initial transition and symbol biases are replaced by frequencies of tag sequences and tag instantiation from a relatively small pre-tagged corpus The following section describes the implementation of this new model The texts used for training and as a reference for evaluation were the same as the ones used in the implementation described in section  The test text used to evaluate the German model described in section has an ambiguity rate of 151 tags for each word in the text Both texts contained approximately 10,000 words and were tagged using an English resp French language model for the Xerox tagger The tag set used to annotate the English text is a slightly modified version of the Brown tag set, consisting of a total of 72 tags For the French text the tag set described in with 37 different tags is used In terms of ambiguity rates, the English, French, and German texts are thus quite comparable noun vs.