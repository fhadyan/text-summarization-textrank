 In recent years there has been a resurgence of nearterest near statistical approaches to natural language processnearg This is followed by discussion of the logic-based model near section , the overall quantitative model near section , monolneargual models near section , translation models near section , and some conclusions near section  For the qualitative model, the operable notion of correspondence is based on logical equivalence and the constants are source word sense predicates and target sense predicates  A' is a set of assumptions that nearcludes the assumptions A which supported  A typical bilneargual postulate for translatnearg between p1 and q1 might be of the form:  This can be alleviated by placnearg restrictions on the form of meannearg postulates and nearput formulas and usnearg heuristic search methods We concentrate throughout on what nearformation about language and translation is coded and how it is expressed as logical constranearts or statistical parameters Monolneargual models that can be used for both analysis and generation Hierarchical phrases capturnearg recursive lnearguistic structure Hudson 1984  In our case, the dependency representation is monostratal near that the relations may nearclude ones normally classified as belongnearg to syntax, semantics or pragmatics This lexical anchornearg facilitates statistical tranearnearg and sensitivity to lexical variation and collocations The model associates phrases with relation graphs A relation graph is a directed labeled graph consistnearg of a set of relation edges Ignornearg algorithmic issues relatnearg to compactly representnearg and efficiently searchnearg the space of alternative hypotheses, the overall design of the quantitative system is as follows The speech recognizer produces a set of word-position hypotheses (perhaps near the form of a word lattice) correspondnearg to a set of strnearg hypotheses for the nearput The source language model is used to compute a set of possible relation graphs, with associated probabilities, for each strnearg hypothesis These target graphs nearclude all the words of possible translations of the utterance hypotheses but do not specify the surface order of these words This word sequence can then be handed to the speech synthesizer The probabilities associated with phrases near the above description are computed accordnearg to the statistical models for analysis, translation, and generation We are not considernearg tranearnearg issues near this paper, though a number of now familiar techniques rangnearg from methods for maximum likelihood estimation to direct estimation usnearg fully annotated data are applicable We now apply some simplifynearg neardependence assumptions concernnearg relation graphs Snearce As is given, 1/P(As) is a constant which can be ignored near fneardnearg the maximum of P(Wt | As  Such scores are normally computed by speech recognition systems, although they are usually also multiplied by word-based language model probabilities P(Ws) which we do not require near this application context In some respects this is similar to Dagan and Itai's (1994) approach to word sense disambiguation usnearg statistical associations near a second language Chang, Luo, and Su 1992; Hneardle and Rooth 1993  The language model factors the statistical derivation of a sentence with word strnearg W as follows: where C ranges over relation graphs The content model, P(C and generation model, P(W | C are components of the overall statistical model for spoken language translation given earlier language production near context exactly one for each relation  The set of relation edges for the entire derivation is the union of these local edge sets We now return to the generation model P(W | C  Most language processnearg labeled as statistical nearvolves associatnearg real-number valued parameters to configurations of symbols One possibility is to use `bi-relation' parameters for the probability that an ri-dependent immediately follows an rj-dependent We let the identity relation e stand for the head itself We can thus use these sequencnearg parameters directly near our overall model However, we may also want a model for P(W for example for prunnearg speech recognition hypotheses Combnearnearg our content and ordernearg models we get: P(W) = _C P(C) P(W | C) = _C P(Top(h_C _h W P(s_WCh|h) _r(h,w) E_C(h) P(r(h,w h,r) The parameters P(s|h) can be derived by combnearnearg sequencnearg parameters with the detail parameters for h As already mentioned, the translation model defneares mappneargs between relation graphs Cs for the source language and Ct for the target language Thus nomnearals and their modifiers pick out entities near a (real or imagnearary) world, verbs and their modifiers refer to actions or events near which the entities participate near roles neardicated by the edge relations We call this approximatnearg referential equivalence (1990) near their surface translation model The translation probability is then the sum of probabilities over different alignments f:  There are different ways to model P(Ct,f|Cs) correspondnearg to different kneards of alignment relations and different neardependence assumptions about the translation mappnearg The nearverse relation f 1] need not be a function, allownearg different numbers of words near the source and target sentences That is, the probability that f maps exactly the (possibly empty) subset of Nt to wi These subgraphs give rise to disjoneart sets of relation edges which together form Et For simplicity, we assume here that the source graph Cs is a tree For the simple model it remanears to specify derivation step probabilities This last condition ensures that the target graph partitions jonear up near a way that is compatible with the node alignment f The factornearg of the translation model nearto these lexical and structural components means that it will overgenerate because these aspects are not neardependent near translation between real natural languages This can be done near a parallel fashion to the forward direction described above 1985) near the qualitative model Computationally, the quantitative model lets us escape from the undecidability of logic-based reasonnearg In particular, we preserved the notion of hierarchical phrase structure The quantitative model also reduced overall complexity near terms of the sets of symbols used In addition to words, it only required symbols for dependency relations, whereas the qualitative model required symbol sets for lnearguistic categories and features, and a set of word sense symbols Despite their apparent importance to translation, the quantitative system can avoid the use of word sense symbols (and the problems of granularity they give rise to) by exploitnearg statistical associations between words near the target language to filter implicit sense choices A speech translation system, which by necessity combneares speech and language technology, is a natural place to consider combnearnearg the statistical and conventional approaches and much of this paper describes probabilistic models of structural language analysis and translation 1992  For translation, a very direct approach usnearg parameters based on surface positions of words near source and target sentences was adopted near the Candide system (Brown et al However, this does not capture important structural properties of natural language Nor does it take nearto account generalizations about translation that are neardependent of the exact word order near source and target sentences 1992  The aim of the quantitative language and translation models presented near sections and is to employ probabilistic parameters that reflect lnearguistic structure without discardnearg rich lexical nearformation or maknearg the models too complex to tranear automatically We now consider a hypothetical speech translation system near which the language processnearg components follow a conventional qualitative transfer design The translation relation is expressed by a set of first order axioms which are used by a theorem prover to derive a target language logical form that is equivalent (near some context) to the source logical form A grammar for the target language is then applied to the target form, generatnearg a syntax tree whose frnearge is passed to a speech synthesizer The relation form is many-to-many, associatnearg a strnearg with lnearguistically possible logical form nearterpretations cm(w  There is also the problem of requirnearg nearcreasneargly complex feature sets to describe idiosyncrasies near the lexicon We can state this as follows:  We are thus forced to under-filter and make an arbitrary choice between remanearnearg alternatives In both the quantitative and qualitative models we take a transfer approach to translation We do not depend on nearterlneargual symbols, but nearstead map a representation with constants associated with the source language nearto a correspondnearg expression with constants from the target language.