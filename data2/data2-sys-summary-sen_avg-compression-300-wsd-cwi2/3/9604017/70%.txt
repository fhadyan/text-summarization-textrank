 Firstly, since all the phrasal rules are excluded from the specialization process, the coverage loss associated with missing combinations of phrasal rules is eliminated Secondly, and possibly even more importantly, the number of specialized rules produced by a given training corpus is approximately halved Using only non-phrasal rules, compilation of the tables for a 15,000 example training set required less than two CPU-hours on the same machine When utterances had more than one correct reading, a preference heuristic was used to select the most plausible one In the first, increasingly large portions of the training set were used to train specialized grammars The coverage loss due to grammar specialization was then measured on the 1,000 utterance test set The second set of experiments tested more directly the effect of constituent pruning and grammar specialization on the Spoken Language Translator's speed and coverage; in particular, coverage was measured on the real task of translating English into Swedish, rather than the artificial one of producing a correct QLF analysis The specialized grammar used the New scheme, and had been trained on the full training set Even more interestingly, Table 3 shows that real system performance, in terms of producing a good translation, is significantly improved by pruning, and is not degraded by grammar specialization Our interpretation of these results is that the technical loss of grammar coverage due to the specialization and pruning processes is more than counterbalanced by two positive effects Firstly, fewer utterances time out due to slow processing; secondly, the reduced space of possible analyses means that the problem of selecting between different possible analyses of a given utterance becomes easier To sum up, the methods presented here demonstrate that it is possible to use the combined pruning and grammar specialization method to speed up the whole analysis phase by nearly an order of magnitude, without incurring any real penalty in the form of reduced coverage The loss of coverage due to grammar specialization also appears comparable, though we have not yet had time to do the work needed to verify this properly After each level, constituent pruning is used to eliminate unlikely constituents Given a sufficiently large corpus parsed by the original, general, grammar, it is possible to identify the common combinations of grammar rules and chunk them into macro-rules  By the very nature of its construction, a general grammar allows a great many theoretically valid analyses of almost any non-trivial sentence The result is a specialized grammar; this has a larger number of rules, but a simpler structure, allowing it in practice to be parsed very much more quickly using an LR-based method  The coverage of the specialized grammar is a strict subset of that of the original grammar; thus any analysis produced by the specialized grammar is guaranteed to be valid in the original one as well The practical utility of the specialized grammar is largely determined by the loss of coverage incurred by the specialization process The two methods, constituent pruning and grammar specialization, are combined as follows The rules in the original, general, grammar are divided into two sets, called phrasal and non-phrasal respectively Phrasal rules, the majority of which define non-recursive noun phrase constructions, are used as they are; non-phrasal rules are combined using EBL into chunks, forming a specialized grammar which is then compiled further into a set of LR-tables Parsing proceeds by interleaving constituent creation and deletion First, the lexicon and morphology rules are used to hypothesize word analyses Constituent pruning then removes all sufficiently unlikely edges Next, the phrasal rules are applied bottom-up, to find all possible phrasal edges, after which unlikely edges are again pruned Finally, the specialized grammar is used to search for full parses Section describes the constituent pruning method Section describes the grammar specialization method, focusing on how the current work extends and improves on previous results Section describes experiments where the constituent pruning/grammar specialization method was used on sets of previously unseen speech data Before both the phrasal and full parsing stages, the constituent table (henceforth, the chart) is pruned to remove edges that are relatively unlikely to contribute to correct analyses Phrasal parsing then creates a number of new edges, including one for flight D L three one two as a noun phrase As a result, full parsing is very quick, and only one analysis (the correct one) is produced for the sentence That is, our estimate for the probability that an edge with property P is correct is (modulo smoothing) simply the number of times edges with property P occur in correct analyses in training divided by the number of times such edges are created during the analysis process in training If there are several left neighbours, the one giving the highest probability is used One possible solution is of course to dispense with the idea of using a general grammar, and simply code a new grammar for each domain The unigram score: the probability of correctness of an edge considering only the tree of grammar rules, with words or word classes at the leaves, that gave rise to it This pruning is fully interleaved with the parsing process In contrast, our pruning takes place only at certain points: currently before parsing begins, and between the phrasal and full parsing stages As described in Section above, the non-phrasal grammar rules are subjected to two phases of processing For example, train an LR parser based on a general grammar to be able to distinguish between likely and unlikely sequences of parsing actions; automatically infer sortal constraints, that can be used to rule out otherwise grammatical constituents; and describes methods that reduce the size of a general grammar to include only rules actually useful for parsing the training corpus In the first, EBL learning phase, a parsed training corpus is used to identify chunks of rules, which are combined by the EBL algorithm into single macro-rules Most simply, there is the size of the training corpus; a larger training corpus means a smaller loss of coverage due to grammar specialization (Recall that grammar specialization in general trades coverage for speed  At one limit, the whole parse-tree for each training example is turned into a single rule, resulting in a specialized grammar all of whose derivations are completely flat  These grammars can be parsed extremely quickly, but the coverage loss is in practice unacceptably high, even with very large training corpora At the opposite extreme, each rule-chunk consists of a single rule-application; this yields a specialized grammar identical to the original one In , a simple scheme is given, which creates rules corresponding to four possible units: full utterances, recursive NPs, PPs, and non-recursive NPs In both cases, the coverage loss due to grammar specialization was about 10 to 12% using training corpora with about 5,000 examples Note that only the non-phrasal rules are used as input to the chunks from which the specialized grammar rules are constructed.