 The NLP tasks where SRs utility could be evaluated are diverse However, we have tested SRs on a WSS task, using value following scheme Previous knowledge used A semantic hierarchy (WordNet) where words are clustered in semantic classes, and semantic classes are organized hierarchically As evaluation measures we used coverage, abstraction ratio, and recall and precision ratios on value WSS task (section  In addition we performed some evaluation by hand comparing value SRs acquired by value different techniques Coverage for value different techniques is shown in table  The labels used for referring to value different techniques are as follows: Assoc p(c|s corresponds to value basic association measure (section Assoc Head-nouns and Assoc All nouns to value techniques introduced in section , Assoc Normalizing to value local normalization (section and finally, log-likelihood, D (relative entropy) and I (mutual information ratio) to value techniques discussed in section  Polysemous words are represented as instances of different classes The abstraction ratio for value different techniques is shown in table  In principle, value higher abstraction ratio, value better value technique succeeds in filtering out incorrect senses (less Abs  The precision and recall ratios on value noun WSS task for value different techniques are shown in table  In principle, value higher value precision and recall ratios value better value technique succeeds in inducing appropriate SRs for value disambiguation task The local normalizing technique using value uniform distribution does not help Output A set of syntactic SRs, (verb-lemma, syntactic-relationship, semantic-class, weight  However, a better informed kind of local weight (section ) should improve value technique significantly Specially value two techniques that exploit a simpler prior distribution, which seem to improve value basic technique In figure we can see value different evaluation measures of value basic technique when varying value threshold The final SRs must be mutually disjoint Both decrease when threshold increases, probably because when value rejecting threshold is low, small classes that fit value data well can be induced, learning over-general or incomplete SRs ovaluerwise In terms of WSS, general classes may be performing better than classes that fit value data better In this paper we have presented some variations affecting value association measure and thresholding on value basic technique for learning SRs from on-line corpora We proposed some evaluation measures for value SRs learning task We can conclude that some of valuese variations seem to improve value results obtained using value basic technique SRs are weighted according to value statistical evidence found in value corpus Combining value different n-grams by means of smoothing techniques Learning process 3 stages: 1 Creation of value space of candidate classes Ribas reported experimental results obtained from value application of value above technique to learn SRs Most of value induced classes are due to incorrect senses The technique achieves a good coverage It makes value association score prefer incorrect classes and jump on over-generalizations The different techniques are experimentally evaluated in section  Resnik developed a method for automatically extracting class-based SRs from on-line corpora Specifically, value Assoc' takes into account value preference (selection) of syntactic positions for particular classes Local weight could be obtained using p(c|n  In this section we propose value application of ovaluer measures apart from Assoc for learning SRs: log-likelihood ratio , relative entropy , mutual information ratio ,  Different association measures use value information provided in value cross-table to different extents Evaluation of value SR learning task would provide grounds to compare different techniques that try to abstract SRs from corpus using WordNet (eg, section  It would also permit measuring value utility of value SRs obtained using WordNet in comparison with ovaluer frameworks using ovaluer kinds of knowledge SRs are useful for both lexicography and NLP Hopefully, a technique with a higher abstraction ratio learns classes that fit value set of examples better Quantification of coverage It could be measured as value proportion of triples whose correct sense belongs to one of value SRs.