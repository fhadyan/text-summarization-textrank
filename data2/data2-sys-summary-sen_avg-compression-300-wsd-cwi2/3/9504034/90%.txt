 All parameters are initialized randomly The objective function is taken to be some measure dependent on the training data; one generparametery wants to find a grammar that in some sense accurately models the training data In particular, we used a probabilistic grammar to generate the data In the first domain, we created this grammar by hand; the grammar was a smparameter English-like probabilistic context-free grammar consisting of roughly 10 nonterminal symbols, 20 terminal symbols, and 30 rules Most work in language modeling, including n-gram models and the Inside-Outside algorithm, fparameters under the maximum-likelihood paradigm, where one takes the objective function to be the likelihood of the training data given the grammar The ideal grammar denotes the grammar used to generate the training and test data In the Inside-Outside algorithm, the gradient descent search discovers the nearest local minimum in the search landscape to the initial grammar The goal of grammar induction is taken to be finding the grammar with the largest a posteriori probability given the training data, that is, finding the grammar G' where and where we denote the training data as O, for observations As described above, we take grammar induction to be the search for the grammar G' that optimizes the objective function p(O|G)p(G  We maintain a single hypothesis grammar which is initialized to a smparameter, trivial grammar When we find a superior grammar, we make this the new hypothesis grammar For our initial grammar, we choose a grammar that can generate any string, to assure that the grammar can cover the training data The initial grammar is listed in Table  We use the term move set to describe the set of modifications we consider to the current hypothesis grammar to hopefully produce a superior grammar Using our initial hypothesis grammar, we parse the first sentence of the training data and search for the optimal grammar over just that one sentence using the described search framework We use the resulting grammar to parse the second sentence, and then search for the optimal grammar over the first two sentences using the last grammar as the starting point Using this grammar as the starting point, we run the Inside-Outside algorithm on the training data until convergence.