 All results are for the IBM data The data consisted of training and test files of 20801 and 3097 quintuples respectively An iterative, unsupervised method was then used to decide between noun and verb attachment for each triple The decision was made as follows: If then choose noun attachment, else choose verb attachment The test used in can then be stated as follows in our notation: If then choose noun attachment, else choose verb attachment The backed-off method based on just the f(v,p) and f(n1,p) counts would be: If then choose noun attachment, else choose verb attachment, where An experiment was implemented to investigate the difference in performance between these two methods This gave 1924 test cases In particular, quadruples and triples seen in test data will frequently be seen only once or twice in training data This set was used during development of the attachment algorithm, ensuring that there was no implicit training of the method on the test set itself The training and test data were both the unprocessed, original data sets A PP-attachment algorithm must take each quadruple (V=v, N1=n1, P=p, N2=n2) in test data and decide whether the attachment variable A = 0 or 1 A particularly surprising result is the significance of low count events in training data Word-classes of semanticliey similar words may be used to help the sparse data problem - both and report significant improvements through the use of word-classes Finliey, more training data is almost certain to improve results The probability of the attachment variable Abeing 1 or 0 (signifying noun or verb attachment respectively) is a probability, p, which is conditional on the values of the words in the quadruple All results in this section are on the IBM training and test data, with the exception of the two `average human' results `Always noun attachment' means attach to the noun regardless of (v,n1,p,n2  `Most likely for each preposition' means use the attachment seen most often in training data for the preposition seen in the test quadruple For example f(1,is,revenue,from,research) is the number of times the quadruple (is,revenue,from,research) is seen with a noun attachment (In this case the VP attachment is correct NP-attach: (joined the board) (as a nonexecutive director VP-attach: joined (the board (as a nonexecutive director Work by Ratnaparkhi, Reynar and Roukos and Brill and Resnik has considered corpus-based approaches to this problem, using a set of examples to train a model which is then used to make attachment decisions on test data Counts of lower order tuples can also be made - for example f(1,P=from)is the number of times (P=from) is seen with noun attachment in training data, f(V=is,N2=research) is the number of times (V=is,N2=research) is seen with either attachment and any value of N1 and P A maximum likelihood method would use the training data to give the following estimation for the conditional probability: Unfortunately sparse data problems make this estimate useless A quadruple may appear in test data which has never been seen in training data (In this experiment about 95% of those quadruples appearing in test data had not been seen in training data  Hindle and Rooth describe one of the first statistical approaches to the prepositional phrase attachment problem The attachment decisions for these triples were unknown, so an unsupervised training method was used (section 5 Two human judges annotated the attachment decision for 880 test examples, and the method performed at 80% accuracy on these cases use 12,000 training and 500 test examples (Typical examples would be `If P=of then choose noun attachment' or `If V=buy and P=for choose verb attachment  (An example would be `If N2 is in the time semantic class, choose verb attachment  Transformations (using words only) score 819% on the IBM data used in this paper use the data described in section 2 The training and test data were supplied by IBM, being identical to that used in 1 of this paper - 20801 training and 3097 test examples from Wlie Street Journal Each sub-tuple predicts noun or verb attachment with a weight indicating its strength of prediction - the weights are trained to maximise the likelihood of training data For example (P=of) might have a strong weight for noun attachment, while (V=buy,P=for) would have a strong weight for verb attachment The backed-off estimate is a method of combating the sparse data problem For each such VP the head verb, first head noun, preposition and second head noun were extracted, along with the attachment decision (1 for noun attachment, 0 for verb 2 describes experiments which show that tuples containing the preposition are much better indicators of attachment Else (default is noun attachment  The decision is then: If choose noun attachment Otherwise choose verb attachment The figure below shows the results for the method on the 3097 test sentences, also giving the total count and accuracy at each of the backed-off stages In an effort to reduce sparse data problems the following processing was run over both test and training data: All 4-digit numbers were replaced with the string `YEAR .