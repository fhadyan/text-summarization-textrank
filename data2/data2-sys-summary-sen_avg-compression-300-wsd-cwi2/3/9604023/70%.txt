 Generative grammar and formal language theory share a common origin in a procedural notion of grammars: the grammar formalism provides a general mechanism for recognizing or generating languages while the grammar itself specializes that mechanism for a specific language At least initially there was hope that this relationship would be informative for linguistics, that by characterizing the natural languages in terms of language-theoretic complexity one would gain insight into the structural regularities of those languages These principles are simply properties of trees A grammatical theory expressed within such a framework is just the set of logical consequences of those axioms In terms of models, one can understand GB to define a universal language the set of all analyses that can occur in human languages The principles then distinguish particular sub-languages the head-final or the pro-drop languages, for instance Each realized human language is just the intersection of the languages selected by the settings of its parameters In GPSG, in contrast, many universals are, in essence, closure properties that must be exhibited by human languages if the language includes trees in which a particular configuration occurs then it includes variants of those trees in which certain related configurations occur Thus while universals in GB are properties of trees, in GPSG they tend to be properties of sets of trees This makes a significant difference in capturing these theories model-theoretically; in the GB case one is defining sets of models, in the GPSG case one is defining sets of sets of models We have illustrated a general formal framework for expressing theories of syntax based on axiomatizing classes of models in  Secondly, the framework is purely declarative and focuses on those aspects of language that are more or less directly observable their structural properties The abstract properties of the mechanisms that might implement those theories, however, are not beyond our reach The key virtue of descriptive complexity results like the characterizations of language-theoretic complexity classes discussed here and the more typical characterizations of computational complexity classes , is that they allow us to determine the complexity of checking properties independently of how that checking is implemented But the accompanying loss of language-theoretic complexity results is unfortunate The nature of language-theoretic complexity hierarchies is to classify languages on the basis of their structural properties The languages in a class, for instance, will typically exhibit certain closure properties (e Moreover, the fact that language-theoretic complexity classes have dual automata-theoretic characterizations offered the prospect that such results might provide abstract models of the human language faculty, thereby not just identifying these regularities, but actually accounting for themg representation theorems  Moreover, since these classifications are based on structural properties and the structural properties of natural language can be studied more or less directly, there is a reasonable expectation of finding empirical evidence falsifying a hypothesis about language-theoretic complexity of natural languages if such evidence exists Finally, the fact that these complexity classes have automata-theoretic characterizations means that results concerning the complexity of natural languages will have implications for the nature of the human language faculty The regular languages, for instance, can be characterized by finite-state (string) automata these languages can be processed using a fixed amount of memory As a result, while these results do not necessarily offer abstract models of the human language faculty (since the complexity results do not claim to characterize the human languages, just to classify them they do offer lower bounds on certain abstract properties of that faculty Over time, the two disciplines have gradually become estranged, principally due to a realization that the structural properties of languages that characterize natural languages may well not be those that can be distinguished by existing language-theoretic complexity classes In this way, productive grammar in concert with formal language theory offers insight into a deep aspect of human cognition syntactic processing on the basis of observable behavior the structural properties of human languages In this paper we discuss an approach to defining theories of syntax based on , a monadic second-order language that has well-defined productive capacity: sets of finite trees are definable within iff they are strongly context-free in a particular sense While originally introduced as a means of establishing language-theoretic complexity results for constraint-based theories, this language has much to recommend it as a general framework for theories of syntax in its own right Being a monadic second-order language it can capture the (pure) modal languages of much of the existing model-theoretic syntax literature directly; having a signature based on the traditional linguistic relations of domination, immediate domination, linear precedence, etc Thus the insights offered by formal language theory might actually be misleading in guiding theories of syntax This full definition establishes that the theory we capture licenses a strongly context-free language This suggests that the apparent mismatch between formal language theory and natural languages may well have more to do with the unnaturalness of the traditional diagnostics than a lack of relevance of the underlying structural properties Models for the language are labeled tree domains with the natural interpretation of the binary predicates This places it within a hierarchy of results relating language-theoretic complexity classes to the descriptive complexity of their models: the sets of strings definable in S1S are exactly the regular sets , the sets of finite trees definable in SnS, for finite n, are the recognizable sets (roughly the sets of derivation trees of CFGs) , and, it can be shown, the sets of finite trees definable in S S are those generated by generalized CFGs in which regular expressions may occur on the rhs of rewrite rules  Consequently, languages are definable in iff they are strongly context-free in the mildly generalized sense of GPSG grammars Since GPSG is presumed to license (roughly) context-free languages, we are not concerned here with establishing language-theoretic complexity but rather with clarifying the linguistic theory expressed by GPSG FSDs specify conditions on feature values that must hold at a node in a licensed tree unless they are overridden by some other component of the grammar; in particular, unless they are incompatible with either a feature specified by the ID rule licensing the node (inherited features) or a feature required by one of the agreement principles the Foot Feature Principle (FFP Head Feature Convention (HFC or Control Agreement Principle (CAP  The agreement principles require pairs of nodes occurring in certain configurations in local trees to agree on certain classes of features.