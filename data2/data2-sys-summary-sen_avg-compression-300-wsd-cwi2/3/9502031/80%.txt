 One word in this purview is in focus at a time Hence `co-operative' error processing Will shallow processing miss too many of the errors cooperative error processing is aimed at? There are two significant difficulties with collecting test data The central difficulty is finding a representative sample of genuine errors by native speakers, in context, with the correct version of the text attached Apart from anything else, `representative' is hard to decide - spectrum of errors or distribution of errors ? Secondly, any corpus of text usually contains only those errors that were left undetected in the text Cooperative processing deals with errors that view backtracks to catch; if not a different class or range, these at least might have a different distribution of error types This would allow view measure of the (linguistic) feasibility of cooperative error processing: the effectiveness of shallow processing over errors revealed by the keystroke-record data To investigate issues involved in shallow processing and cooperative error handling, the pet (processing errors in text) system is being built.