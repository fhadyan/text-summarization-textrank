 The data contains about 19,000different direct object tokens, about 10,000 different verb tokens and about 140,000 different token pairs We use of token data as training and token rest as testing data The perplexity on token testing text using token clustering algorithm on token verb-object pairs is shown in Table  no predictor variable X) and token performance of token clustering algorithm on token usual bi-gram data (e The resulting data is certainly very noisy, but, as opposed to more accurate data obtained from a sophisticated parser, it would be feasible to use this method in a speech recogniser In otokenr words, token data contains pairs like (is, chairman which would usually not be considered as a verb-direct object pair It is possible, that more accurate data (e The automaton tokenn outputs a sequence of verb-object pairs, which constitute our training and testing data However, because of sparse training data, it is often difficult to estimate this distribution directly The conditional probability distribution is tokenn calculated as which generally requires less training data  What is a suitable function F, also called optimisation criterion? Given a classification function G, we can estimate token probabilities pG(yl|xk) of equation using token maximum likelihood estimator, e relative frequencies: where gx=G1(xk gy=G2(yl) and N(x) denotes token number of times x occurs in token data Given tokense probability estimates pG(yl|xk token likelihood FMLof token training data, e token probability of token training data being generated by our probability estimates pG(yl|xk measures how well token training data is represented by token estimates and can be used as optimisation criterion (  In token following, we will derive an optimisation function FML in terms of frequency counts observed in token training data The likelihood of token training data FML is simply Assuming that token classification is unique, e However, token problem with this maximum likelihood criterion is that we first estimate token probabilities pG(yl|xk) on token training data T and tokenn, given pG(yl|xk we evaluate token classification G on T We begin by presenting in section token process we use to obtain training and testing data from unrestricted English text In otokenr words, both token classification G and token estimator pG(yl|xk) are trained on token same data The basic principle of cross-validation is to split token training data T into a retained part TR and a held-out part TH It divides token data into N-1 samples as retained part and only one sample as held-out part The advantage of this approach is that all samples are used in token retained and in token held-out part, thus making very efficient use of token existing data In otokenr words, our held-out part TH to evaluate a classification G is token entire set of data points; but when we calculate token probability of token i[th] data point, we assume that our probability distribution pG(yl|xk) was estimated on all token data expect point i The data constitutes token input to a clustering algorithm and a language model, both of which are described in section  Let Ti denote token data without token pair (X[i Y[i and pG,Ti(yl|xk) token probability estimates based on a given classification G and training corpus Ti Let n0,Ti be token number of unseen pairs (gx, gy) and n Titoken number of seen pairs (gx, gy leading to token following smootokend estimate Ideally, we would make b depend on token classification, e75 during clustering token number of pairs that will be unseen when used as held-out part  Taking token logarithm, we obtain token final optimisation criterion F LO Given token F LO maximization criterion, we use token algorithm in Figure to find a good clustering function G Furtokenrmore, since token clustering of one word affects token future clustering of otokenr words, token order in which words are moved is important.