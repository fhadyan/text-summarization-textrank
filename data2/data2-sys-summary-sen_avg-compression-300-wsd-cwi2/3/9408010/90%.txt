 The data contains about 19,000different direct object tokens, about 10,000 different verb tokens and about 140,000 different token pairs We use of token data as training and token rest as testing data In otokenr words, token data contains pairs like (is, chairman which would usually not be considered as a verb-direct object pair It is possible, that more accurate data (e fewer, but only correct pairs) would lead to a different result The automaton tokenn outputs a sequence of verb-object pairs, which constitute our training and testing data However, because of sparse training data, it is often difficult to estimate this distribution directly The conditional probability distribution is tokenn calculated as which generally requires less training data  Given tokense probability estimates pG(yl|xk token likelihood FMLof token training data, e token probability of token training data being generated by our probability estimates pG(yl|xk measures how well token training data is represented by token estimates and can be used as optimisation criterion (  In token following, we will derive an optimisation function FML in terms of frequency counts observed in token training data The likelihood of token training data FML is simply Assuming that token classification is unique, e We begin by presenting in section token process we use to obtain training and testing data from unrestricted English text It divides token data into N-1 samples as retained part and only one sample as held-out part In otokenr words, our held-out part TH to evaluate a classification G is token entire set of data points; but when we calculate token probability of token i[th] data point, we assume that our probability distribution pG(yl|xk) was estimated on all token data expect point i Let Ti denote token data without token pair (X[i Y[i and pG,Ti(yl|xk) token probability estimates based on a given classification G and training corpus Ti75 during clustering token number of pairs that will be unseen when used as held-out part .