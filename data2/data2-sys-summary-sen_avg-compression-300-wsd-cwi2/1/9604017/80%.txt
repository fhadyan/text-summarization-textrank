 Firstly, since all the phrasal rules are excluded from the specialization process, the coverage loss associated with missing combinations of phrasal rules is eliminated Secondly, and possibly even more importantly, the number of specialized rules produced by a given training corpus is approximately halved When utterances had more than one correct reading, a preference heuristic was used to select the most plausible one In the first, increasingly large portions of the training set were used to train specialized grammars The coverage loss due to grammar specialization was then measured on the 1,000 utterance test set The second set of experiments tested more directly the effect of constituent pruning and grammar specialization on the Spoken Language Translator's speed and coverage; in particular, coverage was measured on the real task of translating English into Swedish, rather than the artificial one of producing a correct QLF analysis The specialized grammar used the New scheme, and had been trained on the full training set Our interpretation of these results is that the technical loss of grammar coverage due to the specialization and pruning processes is more than counterbalanced by two positive effects After each level, constituent pruning is used to eliminate unlikely constituents Given a sufficiently large corpus parsed by the original, general, grammar, it is possible to identify the common combinations of grammar rules and chunk them into macro-rules  The coverage of the specialized grammar is a strict subset of that of the original grammar; thus any analysis produced by the specialized grammar is guaranteed to be valid in the original one as well The practical utility of the specialized grammar is largely determined by the loss of coverage incurred by the specialization process The two methods, constituent pruning and grammar specialization, are combined as follows The rules in the original, general, grammar are divided into two sets, called phrasal and non-phrasal respectively Phrasal rules, the majority of which define non-recursive noun phrase constructions, are used as they are; non-phrasal rules are combined using EBL into chunks, forming a specialized grammar which is then compiled further into a set of LR-tables First, the lexicon and morphology rules are used to hypothesize word analyses Constituent pruning then removes all sufficiently unlikely edges Next, the phrasal rules are applied bottom-up, to find all possible phrasal edges, after which unlikely edges are again pruned Finally, the specialized grammar is used to search for full parses Section describes the constituent pruning method Section describes the grammar specialization method, focusing on how the current work extends and improves on previous results Section describes experiments where the constituent pruning/grammar specialization method was used on sets of previously unseen speech data Before both the phrasal and full parsing stages, the constituent table (henceforth, the chart) is pruned to remove edges that are relatively unlikely to contribute to correct analyses Phrasal parsing then creates a number of new edges, including one for flight D L three one two as a noun phrase As a result, full parsing is very quick, and only one analysis (the correct one) is produced for the sentence One possible solution is of course to dispense with the idea of using a general grammar, and simply code a new grammar for each domain This pruning is fully interleaved with the parsing process As described in Section above, the non-phrasal grammar rules are subjected to two phases of processing For example, train an LR parser based on a general grammar to be able to distinguish between likely and unlikely sequences of parsing actions; automatically infer sortal constraints, that can be used to rule out otherwise grammatical constituents; and describes methods that reduce the size of a general grammar to include only rules actually useful for parsing the training corpus In the first, EBL learning phase, a parsed training corpus is used to identify chunks of rules, which are combined by the EBL algorithm into single macro-rules Most simply, there is the size of the training corpus; a larger training corpus means a smaller loss of coverage due to grammar specialization (Recall that grammar specialization in general trades coverage for speed  At the opposite extreme, each rule-chunk consists of a single rule-application; this yields a specialized grammar identical to the original one In , a simple scheme is given, which creates rules corresponding to four possible units: full utterances, recursive NPs, PPs, and non-recursive NPs In both cases, the coverage loss due to grammar specialization was about 10 to 12% using training corpora with about 5,000 examples Note that only the non-phrasal rules are used as input to the chunks from which the specialized grammar rules are constructed.