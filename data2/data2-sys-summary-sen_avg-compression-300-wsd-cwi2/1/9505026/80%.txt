 There are several ways to handle unknown words Making every tag a possible tag for that word with equal probability and finding the most probable tag solely based on context probabilities The results can be slightly improved by trying only open-class tags for unknown words This constitutes an a priori distribution for unknown words, reflecting for example that most of the unknown words are nouns The probabilities could be obtained from a separate training part, or from the distribution of words that occur only once in the training corpus These words reflect the distribution of unknown words according to the formula presented in  First, we used the original tagset, consisting of 258 tags Unknown words were handled by a mixture of methods 2 and 3 listed in Section : If the suffix of 4 characters (3 characters for the Susanne corpus) of the unknown words was found in the lexicon, the tag distribution for that suffix was used Otherwise we used the distribution of tags for words that occurred only once in the training corpus As opposed to trigram tagging, lexical tagging ignores context probabilities and is based solely on lexical probabilities Each word is assigned its most frequent tag from the training corpus Unknown words were assigned the most frequent tag of words that occurred exactly once in the training corpus The most frequent tags for single occurrence words are for the Teleman corpus NNSS (indefinite noun-noun compound) and noun (large and small tagset, resp for the Susanne corpus NN2 (plural common noun) and NN (common noun; again large and small tagset resp  The results for the Teleman corpus are shown in Table and the results for the Susanne corpus in Table  Unknown words were handled by creating a decision tree of the four last letters from words with three or less occurrences The difficulty in processing is mostly due to the rather large number of unknown words in the Swedish corpus and the higher degree of ambiguity despite having smaller tagsets Since 10% of the words in the Teleman corpus occur only once, we expect from the Good-Turing formula that 10% of the words in new text be unknown, which is a very high percentage Since most of the work in this area is on English corpora, we compared the Teleman corpus with an English corpus, namely the Susanne corpus , which is a re-annotated part of the Brown Corpus , comprising different text genres The major difference (apart from corpus size and tagsets used) is the percentage of words that occur exactly once: 10% for Teleman vs In the Teleman corpus, each word in the running text has in average 238 tags for the small tagset, and 369 for the large tagset61 for the Susanne corpus, despite the fact that the tagsets for the Susanne corpus are larger than those for the Teleman corpus2% of the words in the Teleman corpus are ambiguous, and only 449% in the Susanne corpus (small/large tagset, resp see Table for further details  Tags in the Susanne corpus with indices are counted as separate tags Unknown words are words that occur only in the test set, but not in the training set The remaining 9,823 words of the Susanne corpus were not used in the experiments Thus, instead of calculating and maximizing , with Ti tags and Wi words, which is impossible in all practical cases, one calculates and maximizes to find the best sequence of tags for a given sequence of words The paper is organized as follows: Section discusses the Teleman corpus and the tagsets used This results in poor estimates for the probabilities of new sequences of words When using an HMM for tagging, the system gets a string of words and has to find the most probable sequence of tags that could have produced the string of words Section describes the HMM-based tagger and Section the reductionistic statistical one Each rule applies to a current word with a set of candidate tags The Teleman corpus is a corpus of contemporary Swedish, representing a mixture of different text genres like information brochures on military service and medical care, novels, etc The tagger is reductionistic since it repeatedly removes low-probability candidate tags Coping with unknown words, ie words not encountered in the training set, is an archetypical example of the former task It comprises 85,408 words (tokens; here, words is a collective denotation of proper words, numbers, and punctuation  To achieve improved estimates of lexical probabilities, words can be clustered together, see [ .