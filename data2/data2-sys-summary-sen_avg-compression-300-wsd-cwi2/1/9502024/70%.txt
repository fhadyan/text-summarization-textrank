 Therefore, extragrammatical sentences should be handled by some recovery mechanism(s) rather than by a set of additional rules At first, an input sentence is processed by the normal parser Otherwise, the normal parser fails, and then the robust parser starts to execute with edges generated by the normal parser The result of the robust parser is the parse trees which are within the grammatical coverage of the system Rule We can derive 4,958 rules and their frequencies out of 14,137 sentences in the Penn treebank tree-tagged corpus, the Wall Street Journal These removed rules are almost for peculiar sentences and the left rules are very general rules We can show that our robust parser can compensate for lack of rules using only 192 rules with the recovery mechanism and heuristics Test set First, 1,000 sentences are selected randomly from the WSJ corpus, which we have referred to in proposing the robust parser Of these sentences, 410 are failed in normal parsing, and are processed again by the robust parser To show the validity of these heuristics, we compare the result of the robust parser using heuristics with one not using heuristics Second, to show the adaptability of our robust parser, same experiments are carried out on 1,000 sentences from the ATIS corpus in Penn treebank, which we haven't referred to when we propose the robust parser Among 1,000 sentences from the ATIS, 465 sentences are processed by the robust parser after the failure of the normal parsing Table shows the results of the robust parser on WSJ With heuristics, our robust parser can enhance the processing time and reduce the number of edges1% even if the heuristics differentiate edges and prefer some edges It shows that the proposed heuristics is valid in parsing the real sentences The experiment says that our robust parser with heuristics can recover perfectly about 23 sentences out of 100 sentences which are just failed in normal parsing, as the percentage of no-crossing sentences is about 23 However, the percentage of sentences with constituents crossing less than 2 is higher than the WSJ, as sentences of ATIS are more or less simple The experimental results of our robust parser show high accuracy in recovery even though 96% of total rules are removed So, parsing systems are likely to have extragrammatical sentences which cannot be analyzed by the systems Our robust parser can recover these extragrammatical sentences with 68 77% accuracy In this paper, we have presented the robust parser with the extended least-errors recognition algorithm as the recovery mechanism This robust parser can easily be scaled up and applied to various domains because this parser depends only on syntactic factors To enhance the performance of the robust parser for extragrammatical sentences, we proposed several heuristics The heuristics assign the error values to each error-hypothesis edge, and edges which has less error values are processed first So, not all the generated edges are processed by the robust parser, but the most plausible parse trees can be generated first The accuracy of the recovery in our robust parser is about 68% 77  Mellish introduced some chart-based techniques using only syntactic information for extragrammatical sentences Also, because the recovery process runs when a normal parser terminates unsuccessfully, the performance of the normal parser does not decrease in case of handling grammatical sentences For any input, including grammatical and extragrammatical sentences, this algorithm can generate the resultant parse tree In this paper, we present a robust parser with a recovery mechanism We extend the general algorithm for least-errors recognition to adopt it as the recovery mechanism in our robust parser Because our robust parser handle extragrammatical sentences with this syntactic information oriented recovery mechanism, it can be independent of a particular system or particular domain In addition, people are prone to mistakes in writing sentences Then we present the extension of this algorithm, and the heuristics adopted by the robust parser The general algorithm for least-errors recognition , which is based on Earley's algorithm, assumes that sentences may have insertion, deletion, and mutation errors of terminal symbols Each final state means the recognition of a nonterminal The extended least-errors recognition algorithm can handle not only terminal errors but also nonterminal errors The robust parser using the extended least-errors recognition algorithm overgenerates many error-hypothesis edges during parsing process Edges with more error values are regarded as less important ones, so that those edges are processed later than those of less error values Heuristics 1: error types The analysis on 3,538 sentences of the Penn treebank corpus WSJ shows that there are 498 sentences with phrase deletions and 224 sentences with phrase insertions When handling sentences, the robust parser assings more error values( ) to the error hypothesis edge occurring within a fiducial nonterminal So, the robust parser assigns less error values( ) to the error hypothesis edges with these symbols than to the other terminal symbols All error values are additive By these heuristics, our robust parser can process only plausible edges first, instead of processing all generated edges at the same time, so that we can enhance the performance of the robust parser and result in the great reduction in the number of resultant trees Our robust parsing system is composed of two modules One module is a normal parser which is the bottom-up chart parser The other is a robust parser with the error recovery mechanism proposed herein.