 27 were ie correctly, ie 50 were not ie at all, ie can be represented as: \t\tThe \t\tS \t\t[ ] man \t\tN \t\t[VP] gave \t\tVP \t\t[ ] (1a t\tthe \t\tNP \t\t[NP] dog \t\tN \t\t[NP] a \t\tNP \t\t[ ] bone \t\tN \t\t[ ] Intuitively, syntactic links between non-adjacent words, impossible in a standard finite-state grammar, are here established by passing categories along on the stack through the state of intervening words \t\tFido \t\tS \t\t[ ] had \t\tVP \t\t[ ] a \t\tNP \t\t[NP(t (7a t\tbone \t\tN \t\t[NP(t yesterday \t\tNP(t) \t\t[N [NP(t and \t\tN \t\t[NP(t biscuit \t\tN \t\t[NP(t today \t\tNP(t) \t\t[ ] In this analysis instead of a regular transition for `bone' of: N [NP(t NP(t) [ ] there is instead a transition introducing coordination: N [NP(t NP(t) [N [NP(t Allowing categories on the stack to themselves have non-empty stacks moves the formalism one step further from being an indexed grammar \t\t  Therefore, when tuned to any particular language corpus the resulting grammar will be effectively finite-state2 N [ ] S(rel) [ t\t02 \t\tN [ ] S(rel) [ t\t0 Factor out other features which are merely passed from state to state Establish word paradigms, ie classes of words which occur with similar transitions These paradigms will correspond to a great extent to the word classes of rule-based grammars threw \t\tVP NP, X(out) \t\tprob: p1 VP X(out NP \t\tprob: p2 Even if p1 were considerably greater than p2, the cumulative negative effect of the longer states in (10) would eventually lead to the model giving the sentence with the shifted NP (11) a higher probability It would not be difficult to make a small extension to the present model to capture such information, namely by introducing an additional feature containing the lexical value of the head of a phrase.