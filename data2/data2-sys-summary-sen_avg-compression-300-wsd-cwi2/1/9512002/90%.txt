 Together, terminals and nonterminals are called words  Given that our only unit of representation is the word, compression of the input or a nonterminal reduces to writing out a sequence of word indices For simplicity, these words are drawn from a probability distribution over a single dictionary; this language model has been called a multigram  Figure presents a complete description of thecatinthehat, in which the input and six words used in the description are decomposed using a multigram language model The input is represented by four words, thecat+i+n+thehat The total number of times the word is indexed in the combined description of the input and the dictionary is c(w  At the end, our algorithm produces a lexicon, a statistical language model, and a segmentation of the input36 bits to the description length, whereas under the empty grammar the description length doubles Thus, it has diverse application in speech recognition, lexicography, text and speech compression, machine translation, and the segmentation of languages with continuous orthography The language model in figure looks suspiciously like a stochastic context-free grammar, in that a word is a nonterminal that expands into other words This paper presents acquisition results from text and phonetic transcripts, and preliminary results from raw speech First, because each word is decomposable into its representation, adding or deleting a word does not drastically alter the character of the grammar Finally, because the representation of a word serves as a prior that discriminates against unnatural words, search tends not to get bogged down in linguistically implausible grammars In stage 1, the Baum-Welch procedure is applied to the input and word representations to estimate the probabilities of the words in the current dictionary In stage 2 new words are added to the dictionary if this is predicted to reduce the combined description length of the dictionary and input So far as we know, these are the first reported results on learning words directly from speech without prior knowledge Given a dictionary, we can compute word probabilities over word and input representations using EM; for the language model described here this is simply the Baum-Welch procedure Normalizing these counts produces the next round of word probabilities So some partial ordering must be imposed on words Under the concatenative model that has been discussed, this is easy enough, since the representation of a word can only contain shorter words The general strategy for building new words is to look for a set of existing words that occur terminal more often than independent chance would predict Similarly, words are deleted when doing so would reduce the combined description length This generally occurs as shorter words are rendered irrelevant by longer words that model more of the input One interesting addition needed for processing speech is the ability to merge changes that occur in the phoneme-to-phone mapping into existing words The algorithm was run on the Brown corpus , a collection of approximately one million words of text drawn from diverse sources, and a standard test of language models The final dictionary contains 30,347 words92 bits/character A slight adjustment of the conditions for creating words produces a larger dictionary, of 42,668 words, that has a slightly poorer compression rate of 2 The goal, as in Cartright and Brent , is to segment the speech into words After ten iterations of training on the phoneme sequences, the algorithm produces a dictionary of 6,630 words, and a segmentation of the input A phone-to-speech model was created using supervised training on the TIMIT continuous speech database The final dictionary contains 1097 words after training on the transcriptions, and 728 words after training on the speech Most of the difference is in the longer words: as might be expected, performance is much poorer on the raw speech Then since there is no chance of a word with meaning moccurring in the input, all words with that meaning can effectively be removed from the dictionary and probabilities renormalized Since word meanings are tied to compression, they can be learned by altering the meaning of a word when such a move reduces the combined description length The algorithm also occasionally produces words that cross real-word boundaries, like ed by the (see figure  Then words are sequences of terminals and abstract categories that are represented by concatenating words and 's The algorithm we have described for learning words has several properties that make it a particularly good tool for solving language engineering problems Using our algorithm for text compression, for instance, enables the compressed text to be searched or indexed in terms of intuitive units like words no wanna like words  The components of that representation are also words with representations Ellison has used three-level compression schemes to acquire intermediate representations in a manner similar to how we acquire words The unsupervised acquisition of words from continuous speech has received relatively little study What is more, during speech production sounds blend across word boundaries, and words undergo tremendous phonological and acoustic variation: what are you doing is often pronounced /wc  The optimal dictionary is the one that produces the shortest description of both the speech stream and the dictionary itself.