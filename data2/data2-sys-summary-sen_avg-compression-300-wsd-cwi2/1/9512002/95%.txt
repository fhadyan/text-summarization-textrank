 Together, terminals and nonterminals are called words  Given that our only unit of representation is the word, compression of the input or a nonterminal reduces to writing out a sequence of word indices For simplicity, these words are drawn from a probability distribution over a single dictionary; this language model has been called a multigram  Figure presents a complete description of thecatinthehat, in which the input and six words used in the description are decomposed using a multigram language model The input is represented by four words, thecat+i+n+thehat At the end, our algorithm produces a lexicon, a statistical language model, and a segmentation of the input The language model in figure looks suspiciously like a stochastic context-free grammar, in that a word is a nonterminal that expands into other words In stage 1, the Baum-Welch procedure is applied to the input and word representations to estimate the probabilities of the words in the current dictionary In stage 2 new words are added to the dictionary if this is predicted to reduce the combined description length of the dictionary and input So far as we know, these are the first reported results on learning words directly from speech without prior knowledge Given a dictionary, we can compute word probabilities over word and input representations using EM; for the language model described here this is simply the Baum-Welch procedure So some partial ordering must be imposed on words Similarly, words are deleted when doing so would reduce the combined description length This generally occurs as shorter words are rendered irrelevant by longer words that model more of the input The final dictionary contains 30,347 words After ten iterations of training on the phoneme sequences, the algorithm produces a dictionary of 6,630 words, and a segmentation of the input The final dictionary contains 1097 words after training on the transcriptions, and 728 words after training on the speech The algorithm also occasionally produces words that cross real-word boundaries, like ed by the (see figure  Then words are sequences of terminals and abstract categories that are represented by concatenating words and 's The algorithm we have described for learning words has several properties that make it a particularly good tool for solving language engineering problems Using our algorithm for text compression, for instance, enables the compressed text to be searched or indexed in terms of intuitive units like words Secondly, the algorithm is unsupervised no wanna like words  The components of that representation are also words with representations The unsupervised acquisition of words from continuous speech has received relatively little study.