 One way to do this is to let the right context vector record which classes of left context vectors occur to the right of a word The rationale is that words with similar left context characterize words to their right in a similar way Generalized left context vectors were derived by an analogous procedure using word-based right context vectors This differs from previous approaches , in which left and right context vectors of a word are always used in vector concatenated vector The generalized context vectors were input to the tag induction procedure described above for word-based context vectors: 20,000 word triplets were selected from the corpus, encoded as 1,000-dimensional vectors (consisting of four generalized context vectors decomposed by a singular value decomposition and clustered into 200 classes Tables present results for word type-based induction and induction based on word type and context Table shows that performance for generalized context vectors is better than for word-based context vectors (00 if two words share many neighbors, and 0 We refer to the vector of left neighbors of a word as its left context vector, and to the vector of right neighbors as its right context vector Neighbors with highest similarity according to both left and right context are listed These examples demonstrate the importance of representing generalizations about left and right context separately The left and right context vectors are the basis for four different tag induction experiments, which are described in detail below: induction based on word type only induction based on word type and context induction based on word type and context, restricted to natural contexts induction based on word type and context, using generalized left and right context vectors The two context vectors of a word characterize the distribution of neighboring words to its left and right Here, we use the raw 250-dimensional context vectors and apply the SVD to the 47,025-by-500 matrix (47,025 words with two 250-dimensional context vectors each  An occurrence of word w is represented by a concatenation of four context vectors: The right context vector of the preceding word The left context vector of w The right context vector of w The left context vector of the following word Contexts with rare words (less than ten occurrences) were also excluded for similar reasons: If a word only occurs nine or fewer times its left and right context vectors capture little information for syntactic categorization The context vectors used so far only capture information about distributional interactions with the 250 most frequent words.