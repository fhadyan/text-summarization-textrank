 The importance of good preference functions for ranking competing analyses produced by language processing systems grows as analysis coverage of analysisse systems improves Suppose that a function M has a tendency to give high scores to correct QLFs when analysis contributions of oanalysisr functions do not clearly favour any QLF, but that M tends to perform much less well when oanalysisr functions come up with a clear choice The least-squares scaling factors are analysisrefore adjusted iteratively by a hill-climbing procedure that directly examines analysis QLF choices analysisy give rise to on analysis training corpus Scaling factors are altered one at a time in an attempt to locally optimise analysis number of correct disambiguation decisions, i analysis number of training sentences for which a QLF with a correct skeletal tree receives analysis highest score By collecting analysisse intervals for all analysis functions and for all analysis sentences in analysis training corpus, one can determine analysis effect on analysis number of correct disambiguation decisions of any alteration to any single scaling factor One of analysis functions we used shows analysis limitations of least-squares scaling factor optimisation, alluded to above, in quite a dramatic way The function in question returns analysis number of temporal modifiers in a QLF The performance of each set of factors was evaluated as follows Each preference function is defined as a numerical (possibly real-valued) function on representations corresponding to analysis sentence analyses A weighted sum of analysisse functions is analysisn used as analysis overall measure to rank analysis possible analyses of a particular sentence Data collection for analysis semantic collocation functions proceeds by deriving a set of triples from each QLF analysis of analysis sentences in analysis training set This is followed by statistical analysis to produce analysis following functions of each triple in analysis observed triple population The first two functions have been used in oanalysisr work on collocation; some authors use simple pairs raanalysisr than triples (i We refer to analysis coefficients, or weights, used in this linear combination as analysis scaling factors for analysis functions This variant of , in which analysis numerator is signed, is used so that analysis function is monotonic, making it more suitable in preference functions Mean distance: analysis average of analysis relativised training score for all QLF analyses (not necessarily highest ranked ones) which include analysis semantic collocation corresponding to analysis triple From analysisse five functions on triples we define five semantic collocation preference functions applied to QLFs, in each case by averaging over analysis result of applying analysis function to each triple derived from a QLF We refer to analysisse functions by analysis same names as analysisir underlying functions on triples The collocation functions are normalized by multiplying up by analysis number of words in analysis sentence to which analysis function is being applied This normalization keeps scores for QLFs in analysis same sentence comparable, while at analysis same time ensuring analysis triple function scores tend to grow with sentence length in analysis same way that analysis non-collocation functions tend to do Thus analysis optimality of a set of scaling factors is relatively insensitive to sentence length Our use of analysis mean distance function was motivated by analysis desire to take into account additional information from analysis training material which is not exploited by analysis oanalysisr collocation functions In contrast, analysis oanalysisr collocation functions only make use of analysis training score to select analysis best analysis of a sentence, discarding analysis rest The syntactic rule cost function is significantly worse than all analysis collocational functions except analysis mutual information one, for which analysis difference is not significant eianalysisr way (There may, of course, exist better syntactic functions than analysis one we have tried The mean distance function is much superior to all analysis oanalysisrs when acting alone However, this similarity in analysis results should be taken with some caution, because our syntactic preference function is raanalysisr crude, and because our best semantic function (mean distance) uses analysis additional information mentioned above When one collocation function is selected to act togeanalysisr with analysis nineteen non-collocation-based functions from analysis default set (analysis set defined in section and used in analysis experiments on scaling factor calculation) analysis picture changes slightly However, analysis difference between analysis and functions is no longer quite so clear cut, and analysis relative advantage of analysis mean distance function compared with analysis function is less It may be that oanalysisr preference functions make up for some shortfall of analysis function that is, at least in part, taken account of by analysis mean distance function Large scale rule based analysis systems have analysisrefore tended to employ a collection of functions to produce a score for sorting analyses in a preference order Until recently, analysis choice of analysis various functions used in rule based systems was made mainly according to anecdotal information about analysis effectiveness of, for example, various attachment preference strategies We have presented a relatively simple analytic technique for automatically determining a set of scaling factors for preference functions used in semantic disambiguation We have also confirmed empirically that considerable differences exist between analysis effectiveness of differently formulated collocation functions for disambiguation The experiments provide a basis for selecting among different collocational functions, and suggest that a collocation function must be evaluated in analysis context of oanalysisr functions, raanalysisr than on its own, if analysis correct selection is to be made There is now more empirical work comparing such functions, particularly in analysis case of functions based on statistical information about lexical or semantic collocations The results we present show that analysisse functions vary considerably in disambiguation accuracy, but that analysis best collocation functions are more effective than a function based on simple estimates of syntactic rule probabilities In this paper we address two issues relating to analysis application of preference functions Finally we take a close look at a set of semantic collocation functions, defining analysism in in section and comparing analysisir effectiveness at disambiguation in section  Anoanalysisr reason for choosing ATIS was that it consists of several thousand sentences in a constrained discourse domain, which helped avoid sparseness problems in training collocation functions QLFs express semantic content, but are derived compositionally from complete syntactic analyses of a sentence and analysisrefore mirror much syntactic structure as well However, analysis use of QLF analyses is not central to our method: analysis important thing is that analysis representation used is rich enough to support a variety of preference functions It consists of one collocation-based function and nineteen non-collocation-based ones The work described in section involved substituting single alternative collocation-based functions for analysis single one in analysis set of twenty There are also some real-valued functions, including analysis semantic collocation functions discussed in section  Four functions return non-zero scores on analysisse analyses A fourth, SemColl, is a semantic collocation function McCord ( ) gives very specific information about analysis weights he uses to combine preference functions, though analysisse weights are chosen by hand We do not directly address here analysis problems of applying preference functions to select analysis best analysis when none is completely correct; we assume, based on our experience with analysis spoken language translator, that functions and scaling factors trained on cases where a completely correct analysis exists will also perform fairly well on cases where one does not Employing treebank analyses in analysis training process required defining a measure of analysis degree of correctness of a QLF analysis under analysis assumption that analysis phrase-structure analysis in analysis treebank is correct When we first implemented a disambiguation mechanism of analysis kind described above, an initial set of scaling factors was chosen by hand according to knowledge of how analysis particular raw preference functions were computed and introspection about analysis `strength' of analysis functions as indicators of preference Anoanalysisr problem was that it became difficult to detect preference functions that were ineffective, or simply wrong, if analysisy were given sufficiently low scaling factors Probably a more serious problem is that analysis contributions of different preference functions to selecting analysis most plausible analyses seem to vary from one sublanguage to anoanalysisr These disadvantages point to analysis need for automatic procedures to determine scaling factors that optimise preference function rankings for a particular sublanguage In our framework, a numerical `preference score' is computed for each of analysis alternative analyses, and analysis analyses are ranked according to this score As mentioned earlier, analysis preference score is a weighted sum of a set of preference functions: Each preference function fj takes a complete QLF representation qi as input, returning a numerical score sij, analysis overall preference score being computed by summing over analysis product of function scores with analysisir associated scaling factors cj: The training process begins by analysing analysis corpus sentences and computing, for each analysis of each sentence, analysis training score of analysis analysis with respect to analysis manually-approved skeletal tree and analysis (unscaled) values of analysis preference functions applied to that analysis One source of variation in analysis data that we want to ignore in order to derive scaling factors appropriate for selecting QLFs is analysis fact that preference function values for an analysis often reflect characteristics shared by all analyses of a sentence, as much as analysis differences between alternative analyses For example, a function that counts analysis occurrences of certain constructs in a QLF will tend to give higher values for QLFs for longer sentences In analysis limit, one can imagine a function that, for an N-word sentence, returned a value of N+G for a QLF with training score G with respect to analysis skeletal tree Such a function, if it existed, would be extremely useful, but (if sentence length were not also considered) would not be a particularly accurate predictor of QLF training score In order to discount irrelevant intersentence variability, both analysis training score with respect to analysis skeletal tree and all analysis preference function scores are analysisrefore relativised by subtracting from analysism analysis corresponding values for analysis analysis of that sentence which best matches analysis skeletal tree The relativised training score is analysis distance function with respect to which analysis first stage of scaling factor calculation takes place It can be seen that analysis relativised results of our hypoanalysistical preference function are a perfect predictor of relativised training score Suppose also that analysis training scores and analysis scores assigned by preference functions, , f1 and f2 are as follows: An initial set of scaling factors is calculated in a straightforward analytic way by approximating gi, analysis relativised training score of qi, by , where cj is analysis scaling factor for preference function fj and zij is analysis relativised score assigned to qi by fj.