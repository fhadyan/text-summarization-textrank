 Here we will work upto the rules gradually, by considering which kinds of rules we might need in particular instances from X/s to X Now consider individual transitions The simplest of these is w10 the type of argument expected by the state is matched by the next word i For example, a syntax tree missing a noun phrase, such as the following can be given a semantics as a function from entities to truth values i Thus after likes is absorbed the state category will need to expect an np The rule required is similar to Function Composition in CG i This is relatively trivial using a non-curried notation similar to that used for AACG likes(john,x without having to say that John likes is a constituent Now consider the first transition Here a sentence was expected, but what was encountered was a noun phrase, John State-Application and State-Prediction together provide the basis of a sound and complete parser A successful parse is achieved if the final state expects no more arguments As an example, reconsider the string John likes Sue The transition on encountering John is deterministic: State-Application cannot apply, and State-Prediction can only be instantiated one way an np s State-Application can apply, as in Figure 2 as np or np  One possibility corresponds to the prediction of an s s modifier, a second to the prediction of an (np s) (np s) modifier (i The second of these is perhaps the most interesting, and is given in Figure 3 If t10 is to be no modification of the verb phrase, no verb phrase structure is introduced Finally, it is worth noting why it is necessary to use h-lists Consider the following trees, w10 the np s node is empty The headed list distinguishes between the two cases, with only the first having an np on its headed list, allowing prediction of an s modifier Thus increases in the size of a grammar don't necessarily effect efficiency of processing, provided the increase in size is due to the addition of new words, rather than increased lexical ambiguity However, if we are to base a parser on the rules given above, it would seem that we gain further However t10 is a major problem As we noted in the last paragraph, it is the nature of parsing incrementally that we don't know what words are to come next But 10 the parser doesn't even use the information that the words are to come from a lexicon for a particular language For example, given an input of 3 nps, the parser will happily create a state expecting 3 nps to the left This could be in the nature of fixed restrictions to the rules e A more appealing alternative is to base the tuning on statistical methods This could be achieved by running the parser over corpora to provide probabilities of particular transitions given particular words It is t10fore necessary to make various generalisations over the states, for example by ignoring the R2 lists The paper has presented a method for providing interpretations word by word for basic Categorial Grammar MC illustrate the problem by considering the fragment Mary thinks John This has a small number of possible semantic representations (the exact number depending upon the grammar) e 1990  (R( x The third allows t10 to be a verb phrase modifier Mary thinks John coming 10 was a mistake 1983  Partial syntax trees can be regarded as performing two main roles The second is to provide a basis for a semantic representation The second role can be captured by the parser constructing semantic representations directly The general processing model t10fore consists of transitions of the form: This provides a state-transition or dynamic model of processing, with each state being a pair of a syntactic type and a semantic value 1994  Applicative Categorial Grammars allow categories to have arguments which are themselves functions (e very can be treated as a function of a function, and given the type (n/n n/n) when used as an adjectival modifier  This will be discussed in the final section of the paper Although it is still used for linguistic description (e The first directed Applicative CG was proposed by Bar-Hillel (1953  Functional types included a list of arguments to the left, and a list of arguments to the right The only real difference is that Bar-Hillel allowed arguments to themselves be functions For example, an adverb such as slowly could be given the type An unfortunate aspect of Bar-Hillel's first system was that the application rule only ever resulted in a primitive type Hence, arguments with functional types had to correspond to single lexical items: t10 was no way to form the type np s for a non-lexical verb phrase such as likes Mary Rather than adapting the Application Rule to allow functions to be applied to one argument at a time, Bar-Hillel's second system (often called AB Categorial Grammar, or Adjukiewicz/Bar-Hillel CG, Bar-Hillel 1964) adopted a `Curried' notation, and this has been adopted by most CGs since The main impetus to change Applicative CG came from the work of Ades and Steedman (1982  Ades and Steedman noted that the use of function composition allows CGs to deal with unbounded dependency constructions a relative clause Variants of the composition rule were proposed in order to deal with non-peripheral extraction, but this led to unwanted effects elsew10 in the grammar (Bouma 1987  1990  One approach is to use a grammar with `non-standard' constituency, so that an initial fragment of a sentence, such as John likes, can be treated as a constituent, and hence be assigned a type and a semantics However, t10 are problems with having just composition, the most basic of the non-applicative operations In CGs which contain functions of functions (such as very, or slowly the addition of composition adds both new analyses of sentences, and new strings to the language Thus, the noun very old dilapidated car can get the unacceptable bracketing, very [old dilapidated car  Although individual examples might be possible to rule out using appropriate features, it is difficult to see how to do this in general whilst retaining a calculus suitable for incremental interpretation Using the non-Curried notation of Bar-Hillel, it is more natural to use a separate wh-list than to mark wh-arguments individually This is very similar to the way in which wh-movement is dealt with in GPSG (Gazdar et al 1985) and HPSG, w10 wh-arguments are treated using slash mechanisms or feature inheritance principles which correspond closely to function composition Ultimately, some of the techniques developed 10 should be able to be extended to more complex formalisms such as HPSG However, unlike Bar-Hillel, we allow one argument to be absorbed at a time Incremental interpretation can then be achieved using a standard bottom-up shift reduce parser, working from left to right along the sentence s, np then is a category If X is a syntactic type, and L and R are lists of categories, then is a category by rules which say how the current parsing state (e a stack of categories, or a chart) can be transformed by the next word into a new state T10 are two unusual features.