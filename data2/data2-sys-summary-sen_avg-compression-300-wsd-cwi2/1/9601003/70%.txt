 rating task has connections with SU 1 One might wish e independent evaluations for spoken and written language engineering However, spoken and written language have a lot to learn from each other Then there's handwriting as well  at the level of user-significant components only 2 Accuracy and robustness are often also attributes of components or systems implementing tasks But evaluation requires modularity to be enforced everywhere, and that will just block innovation 2: We need to bear in mind that finer granularity may differ substantially between systems, so that in may cases comparative evaluation may not be possible generation ? I don't know what 'willing' is supposed to mean here - this is a very resource-dependent thing Efficiency: The speed, and the resources consumed in performing a task are often important attributes relating to the user adequacy of systems word sense identification ? 7 phone lattice production ? 3 prosodic marking ? many spoken language systems wont produce any of these text type ? 4 native-non-native speakers ? 26 Implementation of Comparative TA SE Two possible scenarios for developing an evaluation programme are the following In this scenario the type of evaluation evolves out of actual funded pilot applications This would be done in the 2nd and final calls the bottom-up scenario ? 2 bottom up  homogeneous What does are required, where appropriate, to participate I am afraid that this will not lead anywhere 2 is more oriented to technology push, and should provide a more general framework 2: I don't think either of theses is an effective way to achieve this goal, but 2 comes closest Task Decomposition: There may be more than one way of decomposing a task into constituent sub-tasks7 Other Comments If you have any other comments you would like to make about the way you think TA SE should be carried out in the EC, or about aspects of this questionnaire (omissions or commissions) then please make them here User validation is very important for near to product projects: it should also include technological evaluation to quantify with different objective measures the user validation process general business letter dictation Technology development project are important in order to have a good basis for future more ambitious application oriented projects Different task decompositions may be more or less appropriate to different constraints on the linguistic features, the depth and accuracy, and the environment within which a task is to be performed - My answers are very much those of someone interested in developing, rather than using, written language technology From that point of view, evaluation is a waste of time unless it can provide diagnostic information pointing out gaps in the technology Strategic Marked and Innovation Watch, as well as with sector-based tasks concerned with rating and evaluation you set up false dichotomies, where one actually has a gradation eg for 2 Clearly some mix is required Proper performance of these tasks may be as essential to the success of the system as carrying out its linguistic tasks Not all respondents replied to all questions Not all respondents answered all questions5 Content of Comparative TA SE a translation system for weather reports and a translation system for airline enquiries For user-transparent sub-tasks within a system, the environment is determined by the sub-tasks surrounding it For user-visible tasks, the environment is determined both by the other user-visible and -transparent system tasks surrounding it, but also by the properties of the users, e Environments thus display a concentric, onion-like organisation, where what constitutes a system, task or component at one level may correspond to part of an environment at a lower level One can consider an entire setup (computational system plus end-users) within the wider environment or corporate or national economies One can consider a computational system within the environment constituted by its end-users One can look at a major task components in a system (e translation, document retrieval, speech recognition) in the context of other major task components in the system One can view lower level tasks (parsing, coreference resolution) within the environment of the broader linguistic tasks they contribute to (e Specifically it concentrates on the preparation of a proposal for setting up infrastructure and guidelines for technology rating and performance analysis, including a comparative evaluation where possible, taking into account the broader context of project rating, user validation and field testing and the user driven paradigm of the TELEMATICS APPLICATIONS in general and the LE sector in particular Any task or component (including non-linguistic ones) within a system also has an environment Evaluative principles stressing the need to match a task or its implementation to its environment do not apply just at system level It is therefore a mistake to draw a completely hard line between user-centered / system rating on the one hand, and technology / individual component rating on the other To match a task to its environment, we need to compare task attributes and environment attributes It is useful to identify environment and task attributes that can vary, and those that (for a given evaluation or setup) are fixed A grid evaluation methodology naturally emerges This involves carrying out different runs of a system or component, varying environmental attributes (e language, subject domain, text type) against different settings of task parameters if one wishes to investigate how a system of component would fare outside its intended environment Proposals must outline methods for verifying and validating project results, and dedicate a certain amount of effort to evaluation effort during project cycles Thus a system environment attribute concerning speed of processing will typically surface as a similar environmental attribute for components or sub-tasks within the system There may be two reasons why environmental attributes are not matched by system attributes First, the components implementing the system task have not been coded with sufficient generality or flexibility; in this case the system may be doing the right thing, but not well enough Second, the task itself does not have the right kind of attributes; in this case, the system may be trying to do the wrong thing The people who are paying for the system (procuring users) The two, and their respective interests, may or may not coincide Users are the prime determinants of system environment variables It is therefore necessary to obtain user profile data, and in this regard we should distinguish further categories of user present users, accessible for collection of user data prospective users, from whom such data may or may not be obtainable idealised users, to who future users will hopefully correspond individual users: if a system is being tailored to a specific individual their idiosyncratic needs should be taken into account collective users (e Assessment comprises verification and validation Within project clusters: Project clusters may be formed on the basis of related objectives and applications (e The difference between verification and validation is succinctly summarised as Verification: are we building the system right? Validation: are we building the right system? Verification is thus a form of rating that gauges how far a system or system component fulfills its functional specification, i Validation is a form of rating that checks that a system actually meets genuine needs and requirements Validation ultimately comes down to user validation; namely that a system, the tasks it aims to perform, and the components implementing those tasks all contribute to meeting the needs of its intended users Having determined these, one way of subjecting a system to user validation is by carrying out a grid evaluation, varying environment variables alongside system parameters Ultimately, a task or technology only has validity if it can produce results that are beneficial to some user So one cannot have `pure' task validation, independent of any user requirements A technology is a task for which there exists one or more alternative components implementing it, and where substantial variation of internal task parameters to match possible environmental variables is permitted A technology has user validity if it can be deployed within some system meeting genuine user needs Shared tasks within project clusters could include, amongst others, market research and user requirement definitions, the establishment of user forums, data gathering, the testing and rating of generic technologies, and the development of user validation guidelines There is a tendency to draw a strong contrast between `technology' and user-centered rating What is usually meant by technology rating here is the evaluation of mainly user-transparent tasks or components of a system, in a way that either abstracts away from (or more likely ignores) user-centered factors Not all sub-tasks or components within a system meet the criteria above for being a technology; some can be quite specific to a particular application or system Therefore, the term technology rating can be somewhat misleading: (user-transparent) task rating might be better terminology A task or technology has to be assessed in the light of environmental factors, and these will include factors ultimately stemming from system users To be sure, many previous efforts at technology rating have proposed abstract measures (such as matching labelled bracketings, predicate argument structure, word error rates etc) and applied them to system components without regard to user issues For the rating to enjoy proper validity, one also needs to match the results of these measures up against identified environment attributes for the task Technology validation is just as important as technology verification Internal rating refers to user and technology verification and validation that is carried out solely with regard to the particular needs of an individual project Internal rating typically has a diagnostic as well as an evaluative element Internal rating will normally make use of evaluation data specific to the needs of the project: either the user needs, or the technological needs Assessment issues can be divided into those concerning user-centered rating, and those concerning technology rating Given that applications will vary from project to project, this user-specific data may not be directly amenable for use as comparative evaluation data Technological evaluation data will depend on the precise instances of technology used within the project So again, the technological data may not be directly amenable for use as comparative evaluation data Changes in environment might result from: different user groups, different domains, addition of further task requirements, and so on Assessment of these properties gauges how the present system compares to slight variants of itself, and is a form of comparative rating Comparative rating involves taking different systems and comparing either their system-wide performance, and/or the performance of individual components / task competencies Systems and components can vary according to their task, domain, user profiles/environments, the technologies they employ, and their implementation and task/system decomposition User-centered rating involves the testing the operational feasibility and functional adequacy of system, with regard to its intended user population, and to gauge user acceptance and the socio-economic impact of the system in a real-life situation More generally Environment: This has several sub-variables, applicable in different cases: User profile (system rating) Overall surrounding task (component/sub-task rating) Domain, text type, etc Channel conditions, speaker accent, etc Other non-linguistic factors Task: One can vary the task or system goals Task Attributes: These include Depth, accuracy, robustness and efficiency Grammars, lexicons, language models etc Task decomposition Implementation: Which covers such things as hardware platforms, operating systems, programming language, code, etc One can fix, vary or ignore combinations of these factors to get different kinds of evaluation, not all of which are strictly comparative Varying things at the system level, one can have, e ARPA: The ARPA MUC and TREC evaluations are characterised by having the system environment and task fixed, while varying task attributes and implementation Flexibility: For an individual system one can vary environment factors such as user profile, domain, etc to see how flexible a system is Assessing domain independence may be important here Usefulness: By fixing just the user profile, one can assess how useful various systems are for a particular user group Portability: e by varying the hardware platform, operating system A second question is what benefits such a comparative evaluation will bring in the near and/or long term to system users Briefly, the benefits of comparative evaluation are Cross-Fertilization: By comparing different systems, technologies and implementations, projects can learn from and exploit best practices adopted in other projects The core evaluation activity involves providing a system or system component with some input and comparing its output with the results expected But for all these modes of comparison, evaluation data is required Evaluation data comes in three forms 1 Test, or input data 2 Answer, or output data 3 Training data Training data may not always be required, but for any system employing statistical methods it is likely to be essential Very often, old input and output data can be used as training data Requirements on evaluation data are that it be realistic, and representative (Sparck Jones 1994  For test data to be realistic, it must be the kind of input data that the system or component would actually receive in real use A distinction should also be drawn between diagnostic data and adequacy data Diagnostic data may be set up very carefully to pinpoint particular points at which a system is failing In collecting evaluation data for rating, a decision must be made about the level of: 1g do we evaluate the system against data from different languages, different domains etc  One needs to be able to assess, and predict, what kinds and what properties of technology lend themselves best to different applications and user groups Assessment metrics need to be specified at three levels 1 This section addresses user-centered rating As such it is primarily, though not exclusively, concerned with project internal evaluation; the next section on technology rating takes up the issue of comparative rating We will take this controlled evolutionary development model as the starting point for discussion of user centered rating The model is especially designed to promote the ease and timeliness of user rating One of the primary aims of technology rating is to provide the means to abstract away from individual users, and to gauge the applicability of systems or system components to other types of user But in all cases, information profiling users is a necessary starting point for user-centered rating One can look at it on the basis of different types of data: Factual, objective data about users These determine the implied needs of the users The system attributes that should be matched against environment attributes should be identified, and success criteria and measures for meeting the system attributes must be set up Two very broad system attributes are usability and integratability The trigger papers, and one reaction paper, are Sparck Jones Crouch: General Technology Assessment Netter: Technology Assessment for Written NL Applications Steeneken: Speech Technology Assessment Adriaens: User-Centered Assessment for RTD in Language Engineering King: Reactions to G Such a test could very well figure on the list of acceptance tests of the buyer/procurer of the system is an important attribute that pervades all stages of the development cycle of a system or system increment integration in a complex user environment could be investigated The following test types occur during development of a system or system increment; they are ordered chronologically, by decreasing involvement of system creators and increasing involvement of users, and by increasing importance of integration matters in ever-broadening contexts: 1 alpha test (system or system increment test by other people than the developers, typically prospective end users, possibly still in a controlled environment) 4 acceptance test (a formal test initiated by the prospective procurer/buyer of a system or system increment, typically at the installation site; for custom-built solutions, this is a final test in the test chain for a system or system increment) 5 Regression testing presupposes a rigid approach in which test data are well-defined, and evaluation tools exist to determine changes from one test run to another However good the underlying system, a poor user interface can make it practically unusable Within an interactive system, the user's knowledge and circumstances inevitably change over time, so that one cannot usefully repeat individual tests Since different sets of users are involved at the project cluster level, it is not immediately obvious what role user-centered rating has there, or even whether it is possible But most of the reasons for pursuing comparative evaluation do persist at the user level: e One possible mode of user-centered comparative evaluation would be to try the user group from one project out on a system from another project having a broadly similar, though not necessarily identical, functionality Another mode of evaluation would be to exchange subject domains between projects in the cluster (assuming that suitable evaluation data can be made available to assess the flexibility and portability of systems The problem with user-centered comparative evaluation comes down to the fact that user groups may simply be too heterogeneous to permit much meaningful comparison This being so, there is a danger that user groups within projects will be resistant to this form of comparative evaluation What could be distilled from these data are abstractions from comparable evaluation scenarios, evaluation metrics etc In the medium and long run this could result in a kind of evaluation library containing user models, abstract scenarios, evaluation methods, test suites, tools, metrics etc which have been successfully applied in user validation However, if one looks at the structure of different systems, one usually finds that they have tasks and components in common Of course the same task/component in two different systems might have quite different attributes in terms of user-visibility, depth, accuracy, robustness, efficiency, language, text-type, etc etc Any comparison between the task performance in the two systems will need to take these differences into account, and also try to relate performance to user attributes task and sub-task performance) is a natural way of pursuing comparative evaluation However, just because some of the tasks being compared are user-transparent does not mean that user considerations are absent from technology rating Technology rating permits a natural abstraction away from specific user requirements, but abstracting is not the same thing as ignoring The second discusses user-centered rating, followed by an extensive discussion of technology rating `Pure' technology evaluation metrics require user-centered validation There is little benefit to be gained if the technology metrics bear no relation to the ability of a system to perform its chosen task for its chosen user group User-centered validation of technology evaluation metrics has received scant attention In section 5 some recommendations are given about the kind of organisation and infrastructure required for carrying out user-centered and technology rating, both at a project internal level and for the purposes of comparative evaluation If user-centered validation is to be taken seriously, this means that the initial stages of any comparative technology evaluation exercise would need to be devoted to it The aim would be to propose a number of candidate technology metrics, and to measure them on a variety of systems in parallel with carrying out direct user-centered evaluation On the assumption that technology rating can be carried out using standardised test data, users and systems designers can identify appropriate technological tools without first having to perform user-specific evaluation, where the collection of test data is likely to be expensive Another benefit is that users and systems designers would have a much better idea of the ways in which a system can be modified and adapted to deal with different tasks Comparative rating is of potential value to user groups because it promotes: Cross-fertilization One of the bye-products of the ARPA MUC and TREC comparative evaluations has been cross-fertilization between different systems If one system employs a technique that is shown perform particularly well on a certain sub-task, then the other systems in the evaluation have tended to make use of that technique to improve their own performance Comparative evaluation means that funding user groups can be more confident that the most appropriate components have been employed to perform various sub-tasks One way of promoting this kind of flexibility is to test and evaluate a system by consideration of tasks and application domains lying outside its immediately intended areas A properly set up comparative evaluation can do this, since components of a given system will be run on data from different domains, and where the data arise in response to different sets of user requirements But for developing future systems, the information may be valuable Additionally Comparative rating involves a degree of abstraction away from specific tasks, domains and user groups Some kind of general technology rating is required to achieve this abstraction Performing meaningful technology rating entails the identification of relevant environment and user attributes Any technology measures used must have a user-centered rationale Therefore, an important initial component of any comparative evaluation exercise for the FP-4 should be this kind of validation This involves proposing general technology metrics, applying them to individual systems, and comparing the results with user-centered rating of the systems One possibility would be to set up a small ARPA-like evaluation project under FP-4 However, this would (a) exclude remaining FP-4 projects from comparative evaluation, and (b) does not sit well with emphasis on users in FP-4 the evaluation task would doubtless involve a large degree of artificiality Another way of tackling things would be to build an evaluation exercise bottom-up from existing FP-4 projects, hopefully building on the work done for internal project evaluation Especially so, given that in its initial stages the benefits of participating in a comparative evaluation may be slow to accrue The evaluation should address tasks with multilingual and multimodal (spoken and written language) aspects, falling within the areas covered by approved LE projects A language engineering application system or just LE system is a set of software components constructed to permit a user to carry out some language-related task or function in a specific real-world environment corpora and, more importantly, the evaluation data with `answers' for chosen tasks The evaluation structure should allow both technological and user-centered evaluation As far as possible the comparative evaluation exercise should sit on top of, and make use of, project internal evaluation A braided evaluation structure is a candidate meeting the requirements set out above The input and output objects provide obvious material for black-box evaluation of the sub-task (which will become glass-box evaluation if the sub-task is further decomposed  The more complex structure reflects the fact that in a complex task (like creating a document, say there are different routes through the task structure, and different options that one might employ or ignore on different occasions Different overall tasks will often have overlapping sub-tasks The point of decomposing tasks and putting them together in a braid structure is that it allows one to identify natural common evaluation points These evaluation points allow for a variety of different kinds of evaluation When a sub-task is user significant, then user-centered evaluation metrics can be applied Generally, we can talk of a system or system component as implementing a task The tasks define competences to be evaluated and not system components; it is possible for a task to be distributed across several components, and also for several tasks to make use of the same system component A braided evaluation structure allows for comparative and individual evaluation of different systems at different levels (user-centered, task-specific, general technology  However, it is important to realise that this need not always be the case, and that task structure need not always exactly match system structure While every attempt should be made to bring project internal and comparative evaluation as close together as possible, it would be unrealistic to expect a comparative evaluation to run itself, with the only impetus coming from within individual projects Task specific rating of technologies will thus be feasible only if there is a large enough number of systems performing the same tasks Task oriented rating of technologies would require that the contributions of a module to an application could be factored out in a reliable way Evaluation data has to be chosen that is sufficiently representative for the task or application to be assessed To our knowledge, none of these test suites have been applied to evaluation as opposed to diagnosis; test suites are generally designed to meet criteria other than user-centered evaluation From one point of view, say that of someone designing a modular language analyser, the system still performs these separate tasks The problem is that most reference corpora and annotations of this kind are tailored towards the rating of some specific technology or task Trivially, parallel multi-lingual corpora could play a role in the rating of MT systems, although adequacy of MT systems is hardly ever tested by comparing system output with a predefined translation on the level of corpora Parallel test corpora in different languages with comparable annotations could become relevant, if portability of systems from one language to another is to be assessed But one could well imagine the connectionist system designer arguing that these do not correspond to any genuine tasks, as witnessed by the lack of any separate system components Even if it would not necessarily provide the rating basis for some specific application, the technology of some subcomponents or modules could still be evaluated But in any case, more than corpora will be required to support comparative evaluation using common data Software tools for scoring results, and possibly also the semi-automatic annotation of answer data would also be required Any project that tackles internal evaluation seriously is likely to build up evaluation data in the form of layered corpora, lexicons, terminological databases, databases, etc, as described above If at all possible, it makes sense to build on this kind of test data to provide material for comparative rating By separating tasks from components, we have the option of super-imposing different task structures on top of a single system structure in order to promote a variety of meaningful comparisons and evaluations The problem is that the test data will only be appropriate for certain tasks and sub-tasks For example, a mono-lingual information access system for airline reservations simply will not provide the kind of test data required for evaluating the multi-lingual aspects of a multi-lingual access system for airline reservations Test data provided by individual projects is thus a starting point for building up shared evaluation resources, but it has to be built upon It needs to be extended to cover tasks not relevant to the system from which the data originated Thus, identifying task structure rather than system architecture is the first step towards defining an evaluation framework To further facilitate the re-use of project internal test data, a degree of top-down imposition is desirable labelled bracketings, predicate-argument structure, co-reference relations, word sense identification then projects can be encouraged to produce internal test data appropriate to these kinds of metric One needs to retain flexibility and room for development in what should constitute common test data Standards for technology rating of written NL applications are only gradually emerging For MUC and TREC the main cornerstones of the rating methods were black box evaluation, the usage of naturally occurring corpora as the test material, and the employment of recall and precision as evaluation metrics System architecture will of course normally provide much useful information about the appropriate task structures to employ What appear to be the main limitations of current evaluation methods, if viewed from the point of technology rating are the following aspects Many of the performance evaluation methods, such as employed in MUC and TREC, are task specific This partially results from the concentration on application specific rating, but partly also from the limitation to pure task-independent rating In order to involve as many LE projects as possible in evaluation, it is better to wait and see what tasks and projects are actually to be worked upon, and then build the task structure up from that The task scenario centres on document retrieval and translation, assuming that document requests couched in one language may need to be searched against document files in other languages, and that retrieved document titles in one language may need to be translated to others However, it will provide material for a task specific evaluation, which e This task-specific evaluation needs to be treated with care What counts as the correct index terms may very well depend on the nature of the retrieval system carrying out Task 2 So direct, task-specific comparisons between systems employing different retrieval systems may very well not be possible This illustrates one of the problems with task-specific evaluation: the nature of a task may depend on its surrounding context, so that even two apparently identical tasks may not be directly comparable when their contexts differ Depending on the nature of the systems being tested, Task 2 may not in fact be regarded as an SLP task, although it may very well include some sort of language processing The pair L-OBJ 1 and L-OBJ 2 provides material for evaluation of the retrieval system The transition between the two is very likely to constitute a user-visible task, so that user-centered rating is possible One could envisage user, task and general technology measures being applied to Task 3 In addition, the combination of Tasks 1-3 is similar to that of Tasks 1 and 2, permitting task-specific and user centered rating The coarse task decomposition above provides little space for technology rating User-visible (or user-significant) tasks are those where both the input and output objects have some kind of direct functional significance to a system user Since most of the tasks on the path (1 5) correspond to standard linguistic functionalities, one can apply general technology rating measures, e User-transparent tasks are ones where the input and/or output are of no direct interest to the user, so that typically a user-transparent task is part of a wider user-visible task However, different systems may perform these tasks to different depths of analysis and levels of detail Own-language retrieval would be done only to provide evaluation data In a machine translation system, the translation task will be user-visible: the input and output texts are both significant objects from the user's point of view In relation to speech processing evaluation in particular, various speech input conditions are possible for Task 1, and there are also speech input and output possibilities at Task 3 It is possible to enter and exit the chain at different points, but never without a rational job or task and evaluation step for this Since the illustrative evaluation scenario above is somewhat biassed towards text processing, this section raises some issues more specific to the evaluation of spoken language systems Three topics of particular interest in the evaluation of the present state-of-the-art speech technology: user appreciation of spoken language systems; rating of technology modules; and the interaction between spoken language and natural language systems This requires a continuous stream of new test data The major issue is total system performance and error correction Whether a task is user-visible or transparent can depend on the user as much as on the task This adaptation requires specific rating methods Total system performance (does the user get the required information efficiently user appreciation and the performance of different technologies are main issues for rating The rating is very much application related and generally requires specific data bases m2 Speech output: for pre-recorded speech may rely on existing speech intelligibility tests As the parsing task illustrates, input-output mappings do not completely define tasks Multilingual use requires a variety of identical (language specific) data bases s2 Data bases: should be (within commercial confidence) accessible by other projects within the same frame work Possible rating methods include: User appreciation: Human factor studies normally require subjective measures based on queries opinion scores, rating, (pair-wise) comparison, etc Total system performance : benefit vs human performance successful trials handling time error analysis ease of use Total system performance quantifies the technical success of a system (does it work well  Additionally the benefit of a system versus human performance (do we really need such a system) are items to include Human performance also offers a bench mark for system evaluation Technology rating: Progress during development, competitive comparison and progress estimation are normally quantified within a specific application or technology (e For technology rating the conditions are carefully controlled (laboratory conditions) in order to obtain reproducible results Interaction between a human and a system may fail Specific requirements of a certain application may lead to a different rating methods, and at least to different metrics and criteria Therefore, it is impossible to give detailed examples of possible rating projects In general a robust evaluation experiment should include experiments on various items of user appreciation, system performance or technology Three examples are given: Application oriented: The evaluation of a system using speech input and speech output in a dialogue concept, such as used for a travel information system, allows for the rating of the total system (user appreciation and system performance  Also the rating of individual modules may be relevant, consider the robustness of the recognizer for telephone quality speech and the intelligibility of the (text-to speech output system However, for application oriented ratings one can use representative evaluation data where variables are uncontrolled but more or less representative for the application In many cases a representative set of test data may be too big and therefore not realistic But with a limited set of test data valuable results can still be obtained Also selective analysis of the results (individual module responses) may produce diagnostic information on the performance of the system A competitive project in which various systems are compared will push the technology It is also recommended that test data are made available to the community A possible example may be the results of the SQALE project Combination of Spoken Language and Natural Language Projects: In general SL and NLP use different rating methods and scoring metrics However, the use of common data bases for testing of a combined system (eg a translation system with speech input and/or output, a spelling checker with voice control, or a information system with an interpreter) may be a useful first step The role of project internal technology rating is likely to be primarily one of producing diagnostic information This can be used to facilitate progress in system development, by identifying gaps in the technology that need to be filled in order to improve system performance Internal technology rating may or may not form part of progress evaluation, which would in any case require a large element of user-centered rating To a large degree, internal technology rating is a matter of internal project policy First, the specifics of the technologies employed may vary quite widely from project to project, with a concomitant variety in the forms of rating producing useful diagnostic information Second, some projects may aim to take speech or language processing modules off the shelf and fit them into the system with minimal adaptation Other projects may aim at more substantial refinement and development of existing technologies Perhaps the most one can say is that projects should specify a progress evaluation plan, and that where applicable this should include some form of diagnostic technological rating Technological rating is appropriate where a project envisages either (i) refining or developing speech and/or language processing techniques, or (ii) adapting data such as grammars, language models, lexicons for a particular domain, task or language Namely, (a) that the form of project internal technology evaluation can vary from project to project, and (b) that technology evaluation provides the core of comparative rating The second claim would seem to demand uniformity between projects in technology evaluation, while the first denies it This suggests that internal and comparative technology rating are very different kinds of animal But what one would like is for comparative technology rating to build on the back of project internal rating While internal technology evaluation will tend to be more specific and detailed than the comparative version, this is still compatible with there being an a core of internal rating that is common to a variety of projects In particular, different projects might very well use standard annotations, such as labelled syntactic bracketings, basic predicate argument structures, etc (as defined under Parseval and SemEval to formulate test data But if the standard components can be identified and kept separable, then project internal test data may, with appropriate extra effort, be re-usable for comparative purposes Another obvious point to make is that if an evolutionary system development cycle is followed, then snapshots of a system at different times can profitably be regarded, for the purposes of comparative evaluation as different systems for the same domain, task and user group Thus, in order to facilitate comparison and reusability of evaluation data, a limited degree of standardisation is required for project internal technology rating The first question was intended to determine the attitude of researchers towards two forms that technology rating might take: project-internal and comparative The overwhelming majority of respondents expressed keenness to conduct both sorts of technology rating (some comments indicated a confusion between project-internal technology rating, and other forms of project internal rating, such as simple progress evaluation - the questionnaire may have failed to make this distinction sufficiently clearly  There were worries expressed, however, about the cost of comparative technology rating and the possible effect that it might divert resources away from other research and rating activities The second, multipart question sought to explore the possible content of project-internal technology rating The first subpart asked whether there should be quantitative technology rating of projects, in addition to any (quantitative or non-quantitative) user validation that might be carried out Again, the overwhelming response was in favour of quantitative technology rating, independently of what user rating might or might not take place The comments indicated that user rating alone was not sufficient to drive the technology or to promote reusability The final subpart asked whether in project-internal rating systems should be assessed against test data that differed in various linguistic features (e The third multipart question asked about implementing project-internal technology rating The question aimed at determining whether researchers were in favour of the evaluation exercise being largely run by the projects themselves or whether it would be better for persons independent of the project to be involved as evaluators and as evaluation data collectors However, there was almost an even split for and against the evaluation data collectors being members of projects The fourth multipart question aimed at determining views on the content of comparative evaluation The first subpart asked for preferences concerning an integrated speech and language comparative evaluation exercise versus separate speech and language evaluations The bulk of the respondents supported finer levels of rating, again arguing that it would advance the technology; however fears were expressed that development could be slowed by evaluation forcing a uniform modularity across systems The next three subparts of the fourth question asked respondents to prioritise user-significant system components they thought were candidates for evaluation, sub-user-significant components suitable for evaluation, and dimensions along which generality should be evaluated (and hence promoted  projects themselves should carry out project internal technology rating, though there may be a case for having evaluation data assembled by independent `experts 3 The ARPA evaluations have shown that there is much to be gained from comparative evaluation, and the preceding discussion has indicated how one can set up an evaluation exercise that can accommodate a variety of different applications While technology evaluation must form the core of the exercise, user-centered issues may readily be catered for by paying due attention to the environmental aspects of technology rating A flexible evaluation structure capable of accommodating a variety of tasks and systems, and with an emphasis on environmental / user-centered validation of technology measures should be employed Provision would need to be made for a small group of people independent of any specific project to oversee the collection, refinement and dissemination of common evaluation material, as well as the specification of evaluation architectures, standards and metrics The profile of expertise of this group should reflect both the needs of technology rating and of user validation To a large degree, methods of project internal evaluation will be a matter for negotiation between users and system developers within individual projects However, there are a number of points that are not only desirable for internal evaluation, but which would also facilitate comparative evaluation: Projects should be encouraged to develop a well defined, evolutionary evaluation strategy Projects should identify from the outset the kinds of evaluation data that they will require, and the means by which this data is to be acquired Early acquisition (and use) of evaluation data should be encouraged Projects should provide functional specifications that not only identify tasks and sub-tasks, but also the environmental attributes pertinent to those tasks Wherever possible, projects should be encouraged to make their internal evaluation strategies, test data, user profiles etc Section amplifies on some of these points, and suggests some practices for project internal evaluation The comparative evaluation exercise needs to be set up in a way that is part top-down and part bottom-up For LE tasks, linguistic features that can vary (for different tasks) include: The language of the input or output (e Clear agreement must be reached at the outset about public accessibility of the evaluation data that projects will be supplying A braided task structure should be identified to accommodate the participating projects Natural evaluation points in the braided structure should be identified Appropriate corpus annotations for the selected evaluation points should be specified Projects whose task structure includes a particular evaluation point are expected to employ the annotation relevant to that point as part of their internal rating regime (They may of course also apply additional rating measures  One should aim to exploit existing tools where possible, and otherwise distribute development effort over different projects and the evaluation coordinators Using these tools, individual projects should produce annotated answer data from their own evaluation corpora These should be passed on to the evaluation coordinators Producing common evaluation data will typically involve the coordinators, perhaps in conjunction with other participating projects, refining and extending the range of annotations on the material delivered Individual projects should make available as much additional linguistic and non-linguistic data pertinent to evaluation as possible This involves correlating metric scores with the adequacy or inadequacy of task performance under the different environmental attributes imposed by different systems This correlation would be greatly assisted if systems were assessed at different stages of development This would allow comparison of technology measures and system adequacy under relatively fixed environmental constraints Even a small comparative evaluation exercise cannot be expected to be self-running and self-regulating The function of this coordinating organisation would be to negotiate with the projects participating in the comparative exercise, to collect, administer and disseminate evaluation material, and to survey and synchronise the evaluation exercise The profile of these experts should include industrial and academic expertise in the areas of evaluation and standardisation, language technology, linguistic and software engineering, as well as knowledge of sectors and application areas where user centered rating is concerned QUESTIONNAIRE ON ASSESSMENT AND EVALUATION IN LANGUAGE ENGINEERING 1uni-sb user validation; and 20 Questionnaire 21 Introduction and Terminology Terms later used in the specific sense introduced here are capitalised spontaneous speech channel conditions (telephone vs how much do we vary the linguistic features of the input/output data we provide/expect relative to the features of the data in the intended application; eg do we evaluate the system against data from different languages, different domains etc  Example To make these distinctions clearer consider this example The linguistic functions of the overall system, and those which define user-significant components, may be described as transcription and translation But we may suppose the system contains linguistic processing subcomponents which are not user-significant and which accomplish such functions as word lattice interpretation, part-of-speech tagging, parsing, English-French syntax tree transformation, etc Relevant linguistic features of the input data include: English language, single speaker, read speech, limited domain, short passages, formulaic style; relevant linguistic features of the output are French language, formulaic style, limited domain seeing if the techniques employed may be generalised other languages, other sorts of brief report, etc  Would be willing to conduct it 4 Maybe you should have an option 5 For an applied project, it ought to be part of the basic research methodology For more theoretical work its more problematic since you might not know what to measure, and it may change as the project progresses Would be willing to participate 4 4: We already actively participate in common comparative evaluations Not convinced how useful it would be, but that's not what you asked comparative rating is not going to divert substantial resources from internal rating and development; 2 that comparative rating is likely to bring about project internal improvements, in the same way that internal evaluation should; 3 4: Concern that too much research time would be directed towards setting up evaluations (but this had good results in the ARPA community) 2 whatever validation the users deem sufficient) ? 2 2: qualitative rating is not enough Once you can do that, the considerations change I actually think it may be dangerous to place too much emphasis on UV, because depending on users' existing preconceptions introduces a huge inertia into technological development Test data at least provides some moves in this direction 2: User validation alone won't promote generality and reusability of components at the level of user-significant components only 2 Therefore technological rating levels are undetournable For semi-objective UV it only makes to look a user-significant components But clearly, the measures would not be of interest to users 2: User validation alone won't promote generality and reusability of components only on unseen input-output pairings whose linguistic features match those of the project application 2 You will achieve different things depending which you do 2: This answer needs to be qualified This reflects a system's maintainability, portability and flexibility internally by project participants (developers and users) ? 2 It should just be part of the research team's methodology and they should do it primarily for project-management purposes internally by project participants (developers and users) ? 2 2: but these specialists must do so in close coordination with the partners to ensure compatibility/applicability 1: See previous comment3 above Depends on cases again 2 will involve a lot of organising and doesn't make sense for every project an integrated speech and language evaluation.