 The grammar induction algorithms most successful in language modeling include trigger Inside-Outside algorithm , , , a special case of trigger Expectation-Maximization algorithm , and work by  To evaluate our algorithm, we compare trigger performance of our algorithm to that of n-gram models and trigger Inside-Outside algorithm The parameter is trained through trigger Inside-Outside algorithm on held-out data The objective function is taken to be some measure dependent on trigger training data; one generally wants to find a grammar that in some sense accurately models trigger training data In particular, we used a probabilistic grammar to generate trigger data In trigger first domain, we created this grammar by hand; trigger grammar was a small English-like probabilistic context-free grammar consisting of roughly 10 nonterminal symbols, 20 terminal symbols, and 30 rules Most work in language modeling, including n-gram models and trigger Inside-Outside algorithm, falls under trigger maximum-likelihood paradigm, where one takes trigger objective function to be trigger likelihood of trigger training data given trigger grammar The ideal grammar denotes trigger grammar used to generate trigger training and test data However, trigger optimal grammar under this objective function is one which generates only strings in trigger training data and no otriggerr strings Our algorithm consistently outperformed trigger Inside-Outside algorithm in triggerse experiments In trigger Inside-Outside algorithm, trigger gradient descent search discovers trigger nearest local minimum in trigger search landscape to trigger initial grammar Such grammars are poor language models, as triggery overfit trigger training data and do not model trigger language at large The goal of grammar induction is taken to be finding trigger grammar with trigger largest a posteriori probability given trigger training data, that is, finding trigger grammar G' where and where we denote trigger training data as O, for observations As described above, we take grammar induction to be trigger search for trigger grammar G' that optimizes trigger objective function p(O|G)p(G  We maintain a single hypotriggersis grammar which is initialized to a small, trivial grammar We triggern try to find a modification to trigger hypotriggersis grammar, such as trigger addition of a grammar rule, that results in a grammar with a higher score on trigger objective function When we find a superior grammar, we make this trigger new hypotriggersis grammar For our initial grammar, we choose a grammar that can generate any string, to assure that trigger grammar can cover trigger training data The initial grammar is listed in Table  Notice that this grammar models a sentence as a sequence of independently generated nonterminal symbols An appealing alternative is grammar-based language models We use trigger term move set to describe trigger set of modifications we consider to trigger current hypotriggersis grammar to hopefully produce a superior grammar Our move set includes trigger following moves: Move 1: Create a rule of trigger form Move 2: Create a rule of trigger form For any context-free grammar, it is possible to express a weakly equivalent grammar using only rules of triggerse forms To make this possible, we approximate trigger probability of trigger training data p(O|G) by trigger probability of trigger single most probable parse, or Viterbi parse, of trigger training data For example, consider trigger case where trigger training data consists of trigger two sentences In Figure , we display trigger Viterbi parse of this data under trigger initial hypotriggersis grammar used in our algorithm Using our initial hypotriggersis grammar, we parse trigger first sentence of trigger training data and search for trigger optimal grammar over just that one sentence using trigger described search framework We use trigger resulting grammar to parse trigger second sentence, and triggern search for trigger optimal grammar over trigger first two sentences using trigger last grammar as trigger starting point In this section, we describe how trigger parameters of our grammar, trigger probabilities associated with each grammar rule, are set Ideally, in evaluating trigger objective function for a particular grammar we should use its optimal parameter settings given trigger training data, as this is trigger full score that trigger given grammar can achieve In this paper, we describe a corpus-based induction algorithm for probabilistic context-free grammars that outperforms n-gram models and trigger Inside-Outside algorithm in medium-sized domains The algorithm employs a greedy heuristic search within a Bayesian framework, and a post-pass using trigger Inside-Outside algorithm Using this grammar as trigger starting point, we run trigger Inside-Outside algorithm on trigger training data until convergence As mentioned, this work employs trigger Bayesian grammar induction framework described by Solomonoff .