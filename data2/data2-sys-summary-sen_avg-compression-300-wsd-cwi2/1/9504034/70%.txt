 The grammar induction algorithms most successful in language modeling include trigger Inside-Outside algorithm , , , a special case of trigger Expectation-Maximization algorithm , and work by  To evaluate our algorithm, we compare trigger performance of our algorithm to that of n-gram models and trigger Inside-Outside algorithm For n-gram models, we tried for each domain The smoothing parameters are trained through trigger Forward-Backward algorithm on held-out data The parameter is trained through trigger Inside-Outside algorithm on held-out data The objective function is taken to be some measure dependent on trigger training data; one generally wants to find a grammar that in some sense accurately models trigger training data In trigger first two domains, we created trigger training and test data artificially so as to have an ideal grammar in hand to benchmark results In particular, we used a probabilistic grammar to generate trigger data In trigger first domain, we created this grammar by hand; trigger grammar was a small English-like probabilistic context-free grammar consisting of roughly 10 nonterminal symbols, 20 terminal symbols, and 30 rules In trigger second domain, we derived trigger grammar from manually parsed text Most work in language modeling, including n-gram models and trigger Inside-Outside algorithm, falls under trigger maximum-likelihood paradigm, where one takes trigger objective function to be trigger likelihood of trigger training data given trigger grammar The ideal grammar denotes trigger grammar used to generate trigger training and test data We achieve a moderate but significant improvement in performance over n-gram models and trigger Inside-Outside algorithm in trigger first two domains, while in trigger part-of-speech domain we are outperformed by n-gram models but we vastly outperform trigger Inside-Outside algorithm However, trigger optimal grammar under this objective function is one which generates only strings in trigger training data and no otriggerr strings The first pass row refers to trigger main grammar induction phase of our algorithm, and trigger post-pass row refers to trigger Inside-Outside post-pass Notice that our algorithm produces a significantly more compact model than trigger n-gram model, while running significantly faster than trigger Inside-Outside algorithm even though we use an Inside-Outside post-pass Our algorithm consistently outperformed trigger Inside-Outside algorithm in triggerse experiments In trigger Inside-Outside algorithm, trigger gradient descent search discovers trigger nearest local minimum in trigger search landscape to trigger initial grammar Such grammars are poor language models, as triggery overfit trigger training data and do not model trigger language at large Hence, our algorithm should be less prone to suboptimal local minima than trigger Inside-Outside algorithm In n-gram models and trigger Inside-Outside algorithm, this issue is evaded by bounding trigger size and form of trigger grammars considered, so that trigger optimal grammar cannot be expressed A factor in trigger objective function that favors smaller grammars over large can prevent trigger objective function from preferring grammars that overfit trigger training data The goal of grammar induction is taken to be finding trigger grammar with trigger largest a posteriori probability given trigger training data, that is, finding trigger grammar G' where and where we denote trigger training data as O, for observations As described above, we take grammar induction to be trigger search for trigger grammar G' that optimizes trigger objective function p(O|G)p(G  We maintain a single hypotriggersis grammar which is initialized to a small, trivial grammar We triggern try to find a modification to trigger hypotriggersis grammar, such as trigger addition of a grammar rule, that results in a grammar with a higher score on trigger objective function When we find a superior grammar, we make this trigger new hypotriggersis grammar For our initial grammar, we choose a grammar that can generate any string, to assure that trigger grammar can cover trigger training data The initial grammar is listed in Table  Notice that this grammar models a sentence as a sequence of independently generated nonterminal symbols An appealing alternative is grammar-based language models We use trigger term move set to describe trigger set of modifications we consider to trigger current hypotriggersis grammar to hopefully produce a superior grammar Our move set includes trigger following moves: Move 1: Create a rule of trigger form Move 2: Create a rule of trigger form For any context-free grammar, it is possible to express a weakly equivalent grammar using only rules of triggerse forms Consider trigger task of calculating trigger objective function p(O|G)p(G) for some grammar G We cannot afford to parse trigger training data for each grammar considered; indeed, to ever be practical for data sets of millions of words, it seems likely that we can only afford to parse trigger data once Language models expressed as a probabilistic grammar tend to be more compact than n-gram language models, and have trigger ability to model long-distance dependencies , ,  To make this possible, we approximate trigger probability of trigger training data p(O|G) by trigger probability of trigger single most probable parse, or Viterbi parse, of trigger training data For example, consider trigger case where trigger training data consists of trigger two sentences In Figure , we display trigger Viterbi parse of this data under trigger initial hypotriggersis grammar used in our algorithm Now, let us consider trigger move of adding trigger rule to trigger initial grammar (as well as trigger concomitant rule  To minimize triggerse effects, we process trigger training data incrementally Using our initial hypotriggersis grammar, we parse trigger first sentence of trigger training data and search for trigger optimal grammar over just that one sentence using trigger described search framework We use trigger resulting grammar to parse trigger second sentence, and triggern search for trigger optimal grammar over trigger first two sentences using trigger last grammar as trigger starting point In this section, we describe how trigger parameters of our grammar, trigger probabilities associated with each grammar rule, are set Ideally, in evaluating trigger objective function for a particular grammar we should use its optimal parameter settings given trigger training data, as this is trigger full score that trigger given grammar can achieve In this paper, we describe a corpus-based induction algorithm for probabilistic context-free grammars that outperforms n-gram models and trigger Inside-Outside algorithm in medium-sized domains To address this issue, we use an Inside-Outside algorithm post-pass We create n new nonterminal symbols , and create all rules of trigger form: denotes trigger set of nonterminal symbols acquired in trigger initial grammar induction phase, and X1 is taken to be trigger new sentential symbol The algorithm employs a greedy heuristic search within a Bayesian framework, and a post-pass using trigger Inside-Outside algorithm Using this grammar as trigger starting point, we run trigger Inside-Outside algorithm on trigger training data until convergence As mentioned, this work employs trigger Bayesian grammar induction framework described by Solomonoff .