 Thus, every STSG tree would be produced by the PCFG with equal probability We call the best parse tree under this criterion the Maximum Constituents Parse We also ran experiments using Bod's data, 75 sentence test sets, and no limit on sentence length Since the probability of the most probable parse decreases exponentially in sentence length, the number of random samples needed to find this most probable parse increases exponentially in sentence length In the DOP model, a sentence cannot be given an exactly correct parse unless all productions in the correct parse occur in the training set Examining Bod's data, we find he removed productions There are two existing ways to parse using the DOP model One could try to find the most probable parse tree For a given sentence and a given parse tree, there are many different derivations that could lead to that parse tree The probability of the parse tree is the sum of the probabilities of the derivations Given our example, there are two different ways to generate the parse tree x 1E x 1B 2S each with probability , so that the parse tree has probability  This parse tree is most probable Bod shows how to approximate this most probable parse using a Monte Carlo algorithm Parsing using the DOP model is especially difficult Thus for every STSG derivation, there would be an isomorphic PCFG derivation, with equal probability Thus every STSG tree would be produced by the PCFG with equal probability.