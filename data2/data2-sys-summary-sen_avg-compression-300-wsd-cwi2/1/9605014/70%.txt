 Recently various methods for automatically constructing a thesaurus (hierarchically clustering words) based on corpus data have been proposed , , ,  Since some words never occur in a corpus, and thus cannot be reliably classified by a method solely based on corpus data, we propose to combine the use of an automatically constructed thesaurus and a hand made thesaurus in disambiguation We then constructed a number of thesauri based on these data, using our method We also evaluated our method by using a constructed thesaurus in a pp-attachment disambiguation experiment We used as training data the same 180,000 case frames in Experiment 1 We also extracted as our test data 172 (verb,noun1,prep,noun2) patterns from the data in the same corpus, which is not used in the training data We then applied the learning method proposed in to learn case frame patterns with the constructed thesaurus as input using the same training data Our experimental results indicate that combining an automatically constructed thesaurus and a hand made thesaurus widens the coverage of our disambiguation method, while maintaining high accuracy `Word-Based `MLE-Thesaurus and `MDL-Thesaurus' respectively stand for using word-based estimates, using a thesaurus constructed by employing MLE, and using a thesaurus constructed by our method We also tested the method proposed in of learning case frames patterns using an existing thesaurus In particular, we used this method with WordNet and using the same training data, and then conducted pp-attachment disambiguation experiment using the obtained case frame patterns A method of constructing a thesaurus based on corpus data usually consists of the following three steps: (i) Extract co-occurrence data (e We have proposed a method of clustering words based on large corpus data case frame data, adjacency data) from a corpus, (ii) Starting from a single class (or each word composing its own class divide (or merge) word classes based on the co-occurrence data using some similarity (distance) measure Our method of hierarchical clustering of words based on the MDL principle is theoretically sound Using a thesaurus constructed by our method can improve pp-attachment disambiguation results Suppose available to us are data like those in Figure , which are frequency data (co-occurrence data) between verbs and their objects extracted from a corpus (step (i  We then view the problem of clustering words as that of estimating a probabilistic model (representing probability distribution) that generates such datae Figure shows two example models which might have given rise to the data in Figure  In this paper, we assume that the observed data are generated by a model belonging to the class of models just described, and select a model which best explains the data Thus selecting a model which best explains the given data is equivalent to finding the most appropriate classification of words based on their co-occurrence MDL stipulates that the best probability model for given data is that model which requires the least code length for encoding of the model itself, as well as the given data relative to it We refer to the code length for the model as the `model description length' and that for the data the `data description length We apply MDL to the problem of estimating a model consisting of a pair of partitions as described above In contrast, a model with more clusters, such as Model 1 in Figure , is more complex, but tends to have a better fit to the data Thus, there is a trade-off relationship between the simplicity of clustering (a model) and the goodness of fit to the data The model description length quantifies the simplicity (complexity) of a model, and the data description length quantifies the fit to the data We will now describe how the description length for a model is calculated Then, there are free parameters in a model Given a model M and data S, its total description length L(M) is computed as the sum of the model description length Lmod(M the description length of its parameters Lpar(M and the data description length Ldat(M  (We often refer to Lmod(M) + Lpar(M) as the model description length  Hence Lmod(M) is calculated as Lpar(M) is calculated by where |S| denotes the input data size, and is the number of (free) parameters in the model With the description length of a model defined in the above manner, we wish to select a model having the minimum description length and output it as the result of clustering The description lengths for the data in Figure using the two models in Figure are shown in Table  (Table shows some values needed for the calculation of the description length for Model 1 These calculations indicate that according to MDL, Model 1 should be selected over Model 2 We could in principle calculate the description length for each model and select a model with the minimum description length, if computation time were of no concern Figure shows our (divisive) clustering algorithm MLE, as its name suggests, selects a model which maximizes the likelihood of the data, that is,  This is equivalent to minimizing the `data description length' as defined in Section 3, i In the presence of models with varying complexity, MLE tends to overfit the data, and output a model that is too complex and tailored to fit the specifics of the input data We artificially constructed a true model of word co-occurrence, and then generated data according to its distribution We then used the data to estimate a model (clustering words and measured the KL distance between the true model and the estimated model (The algorithm used for MLE was the same as that shown in Figure , except the `data description length' replaces the (total) description length' in Step 2 Figure (a) plots the relation between the number of obtained noun clusters (leaf nodes in the obtained thesaurus tree) versus the input data size, averaged over 10 trials (The number of noun clusters in the true model is 4 Figure (b) plots the KL distance versus the data size, also averaged over the same 10trials The results indicate that MDL converges to the true model faster than MLE Also, MLE tends to select a model overfitting the data, while MDL tends to select a model which is simple and yet fits the data reasonably well.