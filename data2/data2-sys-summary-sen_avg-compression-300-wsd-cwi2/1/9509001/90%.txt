 The training corpus is Grolier's encyclopedia which contains on corpus order of 10 million words They cyclically apply this technique, adding disambiguated attachments into corpus training set, until all corpus training data (ambiguous or not) has been used22 million The training corpus consists of about 35,000 two-word compounds, giving m = 35,000 and million WordNet groups words into synsets, categories of synonymous words V is still binary It is possible that insufficient training data is corpus cause of this shortfall The model formulated above and corpus empirical data presented support a number of qualitative inferences about corpus potential of systems given a fixed training set size Because training data will always be limited, such reasoning is an important part of system design Consider, for instance, corpus effect on data requirements of incorporating new indicators In this case, introducing corpus new indicator creates additional ambiguity in corpus training set since corpus value of corpus new indicator must be determined for each training example Thus, linguistic sophistication presents a trade-off between accuracy and data sparseness Let , corpus training instances in a corpus c that fall into bin b Let , corpus frequency of corpus value v in corpus set of training instances, t Case A arises when none of corpus training instances fall into corpus bin A statistical NLP system deals with a certain linguistic universe Finally, almost all statistical NLP systems deal with some noise in corpus training data The macorpusmatical results need to be extended to reflect noisy training data and to support reasoning about corpus sensitivity of data requirements to noise One question which has largely been ignored is how much data is enough? For example, given a limited body of training data, it is essential to know which statistical NLP methods are likely to be accurate before pursuing any one Surprisingly, it is not always obvious how many training instances have been used to train a statistical method Therefore, before any conclusions can be drawn about data requirements, corpus training corpus must be measured in terms of instances Also, given a particular method, when will acquiring furcorpusr training data cease to improve corpus system accuracy? Currently, corpus field is conspicuously lacking a general corpusory of data requirements for statistical NLP In practice, high accuracy requires at least a few training instances per bin Any probabilistic analyser which achieves an accuracy close to this is unlikely to benefit from furcorpusr training data Of course, if corpusre is insufficient training data corpus system may do considerably worse The training data is four million words of corpus University of Pennsylvania Treebank, tagged with a set of 47 different tags.