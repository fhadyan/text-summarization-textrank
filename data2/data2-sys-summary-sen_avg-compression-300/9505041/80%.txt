 Because traditional notions of language complexity are generally defined in terms of rewriting mechanisms, complexity of the languages licensed by these formalisms can be difficult to determine A node is in the set assigned to Xi in iff it is labeled with Xi Rabin showed that, for any in the language of SnS, the set of trees encoding the satisfying assignments for in is accepted by a particular type of finite-state automaton on infinite trees We say that the set is Rabin recognizable 100] But formal language theory still has much to offer to generative linguistics It follows that a set of trees is definable in iff they are Rabin recognizable Every set of trees that is accepted in this way is the projection of a local set It is easy to show that, given a recognizable set of trees, one can construct a CFG to generate the corresponding set of trees labeled with pairs as in  Language complexity provides one of the most useful measures with which to compare languages and language formalisms This characterization provides a powerful tool for establishing strong context-freeness of classes of languages that are defined by constraints on the structure of the trees analyzing the strings in the language If one can show that the constraints defining such a set, or perhaps that any constraints in the class employed by a given formalism, can be defined within then the corresponding language or class of languages is strongly context-free Much of the value of standard language complexity classes, on the other hand, comes from results that allow one to show that a given language or class of languages is not included in a particular complexity class If one can show that a given predicate, when added to allows definition of known non-CF languages, then clearly that predicate properly extends the power of the language and cannot be definable In this way, one can show that the predicate which holds between two nodes iff the yields of the subtrees rooted at those nodes are labeled identically wrt P is not definable in , for if it were one could define the copy language  Let be the monadic second-order theory of G Lewis showed that this theory is undecidable by showing how one could define the set of terminating computations of an arbitrary Turing machine within it In particular, we will show that the theory of such a structure includes an undecidable fragment of the monadic second-order theory of the grid More importantly, characterization results for language complexity classes tend to be in terms of the structure of languages, and the structure of natural language, while hazy, is something that can be studied more or less directly In essence, the indexation is an equivalence relation, one that distinguishes unboundedly many equivalence classes among the nodes of the tree Let S2S+CI be the monadic second-order theory of this class of structures Let ALIGN= right where This requires that every node is co-indexed with itself, that the left children of co-indexed nodes are co-indexed as are the right children of co-indexed nodes, and that the left child of the right child and right child of the left child of co-indexed nodes are co-indexed (Although such evidence may well be difficult to find, as witnessed by the history of less successful attempts to establish results such as Shieber's , Further, language complexity classes characterize, along one dimension, the types of resources necessary to parse or recognize a language We thus establish that this account licenses a strongly context-free language In this paper we discuss a flexible and quite powerful approach to establishing language complexity results for formalisms based on systems of constraints on trees The principles embodying a GB theory of language are collected into modules which apply at various levels of this analysis The principles we capture include basic X-bar Theory, Theta Theory, the Case Filter, Binding Theory, Control Theory and various constraints on movement, in particular the Empty Category Principle In Section we introduce a logical language, , capable of encoding such constraints lucidly In this section we focus on the Empty Category Principle and the definition of chains One argument against such a view is that in some cases (such as head-raising) chains formed by one movement can be disrupted by subsequent movement A particular example, one that will be a focus of this paper, is Government and Binding Theory The fundamental issue we must address in defining chains within is how to identify the antecedent of a trace without reference to indices Thus definability in characterizes the strongly context-free languages This puts specific constraints on the structural relationship between a trace and its antecedent But under Rizzi's criteria chains can also be identified by referential indices Manzini , in fact, argues for an account of A-movement (movements, like these we have been considering, to non-argument positions) which implies that no more than two such chains one referential and one non-referential may ever overlap Relativized Minimality theory distinguishes a number of distinct varieties of antecedent-government, one for each class of movement We look at one representative case -antecedent-government The idea, now, is to define chains as any set of nodes that are linearly ordered by  Thus we are able to show that the language licensed by this particular GB theory is strongly context-free The fact that we can exhibit a definition in of the class of trees licensed by a specific GB account of English provides a strong complexity result for that class of trees it is strongly context-free The account works for English because we can classify chains in English into a bounded set of types in such a way that no two chains of the same type ever cross Our approach to chains will work for any account of language that satisfies this principle While this is often modeled as a specific range of Transformational Grammars, the connection between the underlying grammar mechanism and the language a given GB theory licenses is quite weak In this paper we have introduced a kind of descriptive complexity result for the strongly Context-Free Languages a language is strongly context-free iff the set of trees analyzing the syntax of its strings is definable in (modulo a projection  Using this result we have sketched a couple of language complexity results relevant to GB, namely, that free-indexation cannot, in general, be enforced by CFGs, and that a specific GB account of English licenses a strongly context-free language The fact that this is context-free says nothing about the nature of human language faculty, since the principle it depends upon is unlikely to be a principle of Universal Grammar Extensions of our descriptive complexity result to larger language complexity classes could provide formal restrictions on the principles employed by GB theories that would be sufficient to provide non-trivial generative capacity results for those theories without losing the ability to capture the full range of human language With such extended characterizations one might establish upper bounds on the complexity of human language in general This suggests that the regularities of human languages that are the focus of the linguistic studies are perhaps reflections of properties of the human language faculty that can be characterized, at least to some extent, by language complexity classes The fact that the formalization is possible, then, establishes a relatively strong language complexity result for the theory we capture We have, then, two conflicting criteria for our language In addition, it includes an arbitrary array of monadic predicate constants constants naming specific subsets of the nodes in the tree In an extreme view, one can take the underlying mechanism simply to generate the set of all finite trees (labeled with some alphabet of symbols) while the grammatical theory is actually embodied in a set of principles that filter out the ill-formed analyses The formula , for instance, is true at every node labeled N There are two sorts of variables as well those that range over nodes in the tree and those that range over arbitrary subsets of those nodes (thus this is is monadic second-order language  Crucially, though, this is all the language includes To be precise, the actual language we use in a given situation depends on the sets of constants in use in that context We are concerned then with a family of languages, parameterized by the sets of individual and set constants they employ We use lower-case for individual variables and constants, and upper-case for set variables and predicate constants As a result, it has been difficult to establish language complexity results for GB theories, even at the level of the recursive , or context-sensitive languages So, for instance, asserts that the set assigned to X includes every node dominated by the node assigned to x Truth, for these languages, is defined relative to a specific class of models The intended class of these models are, in essence, labeled tree domains Tree domains, then, are particular subsets of  That language complexity results for GB should be difficult to come by is hardly surprising If is a set of sentences in a language , we will use the notation to denote the set of trees, i is just a monadic predicate; it is within the language of (for suitable P) and its definition is simply a biconditional formula This followed, at least in part, from the recognition of the fact that the structural properties that characterize natural languages as a class may well not be those that can be distinguished by existing language complexity classes We can now give an example of a class of sets of trees that is definable in the local sets (ie the sets of derivation trees generated by Context-Free Grammars  Given an arbitrary Context-Free Grammar, we can treat its terminal and non-terminal symbols as monadic predicate constants The proof hinges on the fact that one can translate formulae in into the language of SnS the monadic second-order theory of multiple successor functions