 We consider constraints that divide the states of the current model into equivalence classes The relative frequencies of pairs or triples of groups (categories, clusters) are used as model parameters, each group is represented by a state in the model The standard n-gram approaches are special cases of using model merging and constraintsg if we use the unigram constraint, and merge states until no further merge is possible under this constraint, the resulting model is a standard bigram model, regardless of the order in which the merges were performed The initial model of the original model merging procedure is the maximum likelihood or trivial model This model has the advantage of directly representing the corpus A lot of computation time can be saved by choosing an initial model with fewer states The initial model must have two properties: 1 The trivial model has both properties Therefore, starting with an n-gram model yields a model that is at most equivalent to one that is generated when starting with the trivial model, and that can be much worse But it should be still better than any n-gram model that is of lower of equal order than the initial model The first experiment compares model merging with a standard bigram model The bigram model yields a Markov model with 1,440 states Model merging starts with the maximum likelihood model for the training parte when a bigram model is reached  Merging without a constraint continues until only three states remain: the initial and the final state plus one proper state What happens to the test part? Model merging starts with a very special model which then is generalized Model merging finds a model with 113 states, which assigns a log perplexity of 2 Thus, in addition to finding a model with lower log perplexity than the bigram model (2 1,440  The bigram model assigns a log perplexity of 278, the merged model with 113 states assigns a log perplexity of 2 Thus, the model found by model merging can be regarded generally better than the bigram model This yields a bigram model The second experiment uses the bigram model with 1,440 states as its starting point and imposes no constraints on the merges39, thus again lower than the perplexity of the bigram model (see table  The derived models are not in any case equivalent (with respect to perplexity regardless whether we start with the trivial model or the bigram model For a larger training corpus, the optimal model should be closer in size to the bigram model, or even larger than a bigram model In such a case starting with bigrams does not lead to an optimal model, and a trigram model must be used We investigated model merging, a technique to induce Markov models from corpora The original procedure is improved by introducing constraints and a different initial model The derived models assign lower perplexities to test data than the standard bigram model derived from the same training corpus Additionally, the merged model was much smaller than the bigram model the test part, and that do not influence the final optimal model The time needed to derive a model is drastically reduced by abbreviating these initial merges Instead of starting with the trivial model, one can start with a smaller, easy-to-produce model, but one has to ensure that its size is still larger than the optimal model The method is called model merging and was introduced by  We first give a short introduction to Markov models and present the model merging technique There are several methods to estimate model parameters Model merging is a technique for inducing model parameters for Markov models from a text corpus Model merging induces Markov models in the following way Merging starts with an initial, very general model For this purpose, the maximum likelihood Markov model is chosen, ie a model that exactly matches the corpus This model is also referred to as the trivial model Figure a shows the trivial model for a corpus with words a, b, c and utterances ab, ac, abac The trivial model assigns a probability of to the corpus The criterion for selecting states to merge is the probability of the Markov model generating the corpus The probability never increases because the trivial model is the maximum likelihood model, ie it maximizes the probability of the corpus given the model Model merging stops when a predefined threshold for the corpus probability is reached If we want to reduce the model from size l+2 (the trivial model, which consists of one state for each token plus initial and final states) to some fixed size, we need O(l) steps of merging Therefore, deriving a Markov model by model merging is O(l[4 in time When applying model merging one can observe that first mainly states with the same output are merged