 This model of speech production determines , the probability of a phone sequence given a phoneme sequence , computed by summing over all possible derivations of from  The probability of an utterance u given a phoneme sequence is then , and the combined description length of a grammar G and a set of utterances becomes  Using this formula, any language model that assigns probabilities to phoneme sequences can be evaluated with respect to the MDL principle As a consequence, any algorithm that can build a lexicon by modeling unsegmented text is a good part of the way towards learning words from speech; the principal difference is that in the case of speech there are two hidden layers that must be summed over, namely the phoneme and phone sequences This provides an incentive to learn as much about the general structure of language as possible, and results in a prior that serves to discriminate against words and phrases with unnatural structure In language, regularities exist at many different scales, from common sound sequences that are words, to intricate patterns of grammatical categories constrained by syntax, to the distribution of actions and objects unique to a conversation Together, terminals and nonterminals are called words  This suggests that fundamental processes can be extracted by looking for patterns within the uninteresting coincidences, and implies a recursive learning scheme: extract patterns from the input (creating words and extract patterns from those words, and so on They lead us to a class of grammars in which both the input and nonterminals are represented in terms of words Given that our only unit of representation is the word, compression of the input or a nonterminal reduces to writing out a sequence of word indices For simplicity, these words are drawn from a probability distribution over a single dictionary; this language model has been called a multigram  Figure presents a complete description of thecatinthehat, in which the input and six words used in the description are decomposed using a multigram language model The input is represented by four words, thecat+i+n+thehat The surface form of a word w is given by , and its representation by  The total number of times the word is indexed in the combined description of the input and the dictionary is c(w  At the end, our algorithm produces a lexicon, a statistical language model, and a segmentation of the input Using maximum-likelihood estimation, the probability p(w) of a word is computed by normalizing these counts Assuming a clever coding, the length of a word w's index is  The cost of representing a word in the dictionary is the total length of the indices in its representation; this is denoted by above (Terminals have no representation or cost The total description length is this example is 60 This is longer than the empty description, containing no nonterminals: for this short input the words we contrived were not justified36 bits to the description length, whereas under the empty grammar the description length doubles Thus, it has diverse application in speech recognition, lexicography, text and speech compression, machine translation, and the segmentation of languages with continuous orthography The language model in figure looks suspiciously like a stochastic context-free grammar, in that a word is a nonterminal that expands into other words This paper presents acquisition results from text and phonetic transcripts, and preliminary results from raw speech First, because each word is decomposable into its representation, adding or deleting a word does not drastically alter the character of the grammar Finally, because the representation of a word serves as a prior that discriminates against unnatural words, search tends not to get bogged down in linguistically implausible grammars In stage 1, the Baum-Welch procedure is applied to the input and word representations to estimate the probabilities of the words in the current dictionary In stage 2 new words are added to the dictionary if this is predicted to reduce the combined description length of the dictionary and input So far as we know, these are the first reported results on learning words directly from speech without prior knowledge Stage 3 is identical to stage 1, and in stage 4 words are deleted from the dictionary if this is predicted to reduce the combined description length Given a dictionary, we can compute word probabilities over word and input representations using EM; for the language model described here this is simply the Baum-Welch procedure Summing the posterior probability of a word w over all possible locations produces the expected number of times w is used in the combined description Normalizing these counts produces the next round of word probabilities The above equations are for complete-likelihood estimates, but if one adopts the philosophy that a word has only one representation, a Viterbi formulation can be used Each of our tests is on complex input: the TIMIT speech collection, the Brown text corpus , and the CHILDES database of mothers' speech to children  The first is quite interesting For a description to be well-defined, the graph of word representations can not contain cycles: a word can not be defined in terms of itself So some partial ordering must be imposed on words Under the concatenative model that has been discussed, this is easy enough, since the representation of a word can only contain shorter words First, a word can with some probability match anywhere in an utterance, not just where characters align perfectly The final words and segmentations accord well with our linguistic intuitions (this is quantified and the language models compare very favorably to other results with respect to statistical efficiency And third, the probability of the word-to-phone mapping is now dependent on the first phoneme of the next word Even so, running the algorithm on speech is two orders of magnitude slower than on text The governing motive for changing the dictionary is to reduce the combined description length of U and G, so any improvement a new word brings to the description of an utterance must be weighed against its representation cost The general strategy for building new words is to look for a set of existing words that occur together more often than independent chance would predict The addition of a new word with the same surface form as this set will reduce the description length of the utterances it occurs in Similarly, words are deleted when doing so would reduce the combined description length This generally occurs as shorter words are rendered irrelevant by longer words that model more of the input One interesting addition needed for processing speech is the ability to merge changes that occur in the phoneme-to-phone mapping into existing words The other possible approach, to build words based on the surface forms found in the input rather than the concatenation of existing words, is less attractive, both because it is computationally more difficult to estimate the effect of adding such words, and because surface forms are so variable The remainder of this paper is divided into nine sections, starting with the problem as we see it ( ) explains how we link speech to the symbolic representation of language described in ( ) discusses how the learning framework extends to the acquisition of word meanings and syntax The first is that it captures regularities in the input, using as efficient a model as possible This is tested by using it to compress unsegmented phonetic transcriptions and then verifying that its internal representation follows word boundaries This is tested by applying the algorithm to a multi-speaker corpus of continuous speech The algorithm was run on the Brown corpus , a collection of approximately one million words of text drawn from diverse sources, and a standard test of language models5% of the description is devoted to the parameters (the words and other overhead and the rest to the text The final dictionary contains 30,347 words A slight adjustment of the conditions for creating words produces a larger dictionary, of 42,668 words, that has a slightly poorer compression rate of 2 This is impressive, considering that the simple language model used in this work has no access to context, and naively reproduces syntactic and morphological regularities time and time again for words with similar behavior Even with no special knowledge of the space character, the algorithm adopts a policy of placing spaces at the start of words Many of the rarer words are uninteresting coincidences, useful for compression only because of the peculiarities of the source The goal, as in Cartright and Brent , is to segment the speech into words After ten iterations of training on the phoneme sequences, the algorithm produces a dictionary of 6,630 words, and a segmentation of the input Because the text-to-phoneme engine we use is particularly crude, each word in the original text is mapped to a non-overlapping region of the phonemic input2 fully 96 They indicate that given simple input, the program very reliably extracts the fundamental linguistic units The experiments we have performed on raw speech are preliminary, and included here principally to demonstrate that our algorithm does learn words even in the very worst of conditions A phone-to-speech model was created using supervised training on the TIMIT continuous speech database Unfortunately, there is no single useful definition of a word: the term encompasses a diverse collection of phenomena that seems to vary substantially from language to language (see Spencer  The final dictionary contains 1097 words after training on the transcriptions, and 728 words after training on the speech Most of the difference is in the longer words: as might be expected, performance is much poorer on the raw speech Yet our algorithm has learned a number of long words well enough that they can be reliably found in the data, even when the underlying form does not exactly match the observed input In many cases (witness sometime, maybe, set aside in figure ) these words are naturally and properly represented in terms of others Furthermore, as will be described in the next section, the algorithm can be extended to make use of side information, which has been shown to make the word learning problem enormously easier In the case of child language acquisition this is plainly absurd, and even in engineering applications the motivation for learning a sound pattern is usually to pair it with something else (text, if one is building a dictation device, or words from another language in the case of machine translation  If the meaning of a sentence is a function of the meanings of the words in the sentence (such as the union then the meaning of a word should likewise be that function applied to the meanings of the words in its representation Then since there is no chance of a word with meaning moccurring in the input, all words with that meaning can effectively be removed from the dictionary and probabilities renormalized The probabilities of all other words will increase, and their code lengths shorten Since word meanings are tied to compression, they can be learned by altering the meaning of a word when such a move reduces the combined description length We have fleshed out this extension more fully and conducted some initial experiments, and the algorithm seems to learn word meanings quite reliably even given substantial noise and ambiguity At this time, we have not conducted experiments on learning word meanings with speech, though the possibility of learning a complete dictation device from speech and textual transcripts is not beyond imagination The algorithm also occasionally produces words that cross real-word boundaries, like ed by the (see figure  This approach accords well with traditional theories of morphology that assume words have structure at many levels (and in particular with theories that suppose word formation to obey similar principles to syntax, see Halle and Marantz  Thus, we have a means of representing sequences of terminals and abstract categories by concatenating context-free rules that look very much like words This suggests merging the notion of rule and word Then words are sequences of terminals and abstract categories that are represented by concatenating words and 's The first difference disappears if each word has its own category; in essence, the category takes the place of the word index The algorithm we have described for learning words has several properties that make it a particularly good tool for solving language engineering problems Peeking ahead, after analyzing the Brown corpus our algorithm parses the phrase the government of the united states as a single entity, a word that has a representation Using our algorithm for text compression, for instance, enables the compressed text to be searched or indexed in terms of intuitive units like words Secondly, the algorithm is unsupervised no wanna like words  The components of that representation are also words with representations Olivier and Wolff , were among the first who implemented algorithms that attempt to learn words from text using techniques based on prediction Ellison has used three-level compression schemes to acquire intermediate representations in a manner similar to how we acquire words The unsupervised acquisition of words from continuous speech has received relatively little study The speech recognition community has generally assumed that segmentations of the input are available for the early stages of training This is the first work to present a complete specification of an unsupervised algorithm that learns words from speech, and we hope it will lead researchers to study unsupervised language-learning techniques in greater detail The generality of our algorithm makes it a valuable tool for language engineering tasks ranging from the construction of speech recognizers to machine translation There is a word united states that might be assigned a meaning independently of united or states In contrast to several prior proposals, our algorithm makes no assumptions about the presence of facilitative side information, or of cleanly spoken and segmented speech, or about the distribution of sounds within words What is more, during speech production sounds blend across word boundaries, and words undergo tremendous phonological and acoustic variation: what are you doing is often pronounced /wc  Thus, before reaching the language learner, unknown sounds from an unknown number of words drawn from an unknown distribution are smeared across each other and otherwise corrupted by various noisy channels This may seem like an impossible task- after all, every utterance the listener hears could be a new word Put again in terms of language, a grammar G is a stochastic theory of language production that assigns a probability pG(u) to every conceivable utterance u Speech is encoded as a sequence of articulatory feature bundles, and compressed using a hierarchical dictionary-based coding scheme The optimal dictionary is the one that produces the shortest description of both the speech stream and the dictionary itself Thus, the principal motivation for discovering words and other facts about language is that this knowledge can be used to improve compression, or equivalently, prediction A working unsupervised algorithm would both dispel many of the learnability-argument myths surrounding child language acquisition, and be a valuable tool to the natural language engineering community There is a conceptual difficulty with using a minimum description-length learning framework when the input is a speech signal: speech is continuous, and can be specified to an arbitrary degree of precision When writing a phone, the distribution over phones is a function of the features of the current input phoneme, the next input phoneme, and the most recently written phone