 This model of speech production determines , the probability of a phone sequence given a phoneme sequence , computed by summing over all possible derivations of from  As a consequence, any algorithm that can build a lexicon by modeling unsegmented text is a good part of the way towards learning words from speech; the principal difference is that in the case of speech there are two hidden layers that must be summed over, namely the phoneme and phone sequences In language, regularities exist at many different scales, from common sound sequences that are words, to intricate patterns of grammatical categories constrained by syntax, to the distribution of actions and objects unique to a conversation Together, terminals and nonterminals are called words  This suggests that fundamental processes can be extracted by looking for patterns within the uninteresting coincidences, and implies a recursive learning scheme: extract patterns from the input (creating words and extract patterns from those words, and so on They lead us to a class of grammars in which both the input and nonterminals are represented in terms of words Given that our only unit of representation is the word, compression of the input or a nonterminal reduces to writing out a sequence of word indices For simplicity, these words are drawn from a probability distribution over a single dictionary; this language model has been called a multigram  Figure presents a complete description of thecatinthehat, in which the input and six words used in the description are decomposed using a multigram language model The input is represented by four words, thecat+i+n+thehat The surface form of a word w is given by , and its representation by  The total number of times the word is indexed in the combined description of the input and the dictionary is c(w  At the end, our algorithm produces a lexicon, a statistical language model, and a segmentation of the input Using maximum-likelihood estimation, the probability p(w) of a word is computed by normalizing these counts Assuming a clever coding, the length of a word w's index is  This is longer than the empty description, containing no nonterminals: for this short input the words we contrived were not justified36 bits to the description length, whereas under the empty grammar the description length doubles Thus, it has diverse application in speech recognition, lexicography, text and speech compression, machine translation, and the segmentation of languages with continuous orthography The language model in figure looks suspiciously like a stochastic context-free grammar, in that a word is a nonterminal that expands into other words This paper presents acquisition results from text and phonetic transcripts, and preliminary results from raw speech First, because each word is decomposable into its representation, adding or deleting a word does not drastically alter the character of the grammar Finally, because the representation of a word serves as a prior that discriminates against unnatural words, search tends not to get bogged down in linguistically implausible grammars In stage 1, the Baum-Welch procedure is applied to the input and word representations to estimate the probabilities of the words in the current dictionary In stage 2 new words are added to the dictionary if this is predicted to reduce the combined description length of the dictionary and input So far as we know, these are the first reported results on learning words directly from speech without prior knowledge Given a dictionary, we can compute word probabilities over word and input representations using EM; for the language model described here this is simply the Baum-Welch procedure Summing the posterior probability of a word w over all possible locations produces the expected number of times w is used in the combined description Normalizing these counts produces the next round of word probabilities Each of our tests is on complex input: the TIMIT speech collection, the Brown text corpus , and the CHILDES database of mothers' speech to children  The first is quite interesting For a description to be well-defined, the graph of word representations can not contain cycles: a word can not be defined in terms of itself So some partial ordering must be imposed on words Under the concatenative model that has been discussed, this is easy enough, since the representation of a word can only contain shorter words Even so, running the algorithm on speech is two orders of magnitude slower than on text The general strategy for building new words is to look for a set of existing words that occur together more often than independent chance would predict Similarly, words are deleted when doing so would reduce the combined description length This generally occurs as shorter words are rendered irrelevant by longer words that model more of the input One interesting addition needed for processing speech is the ability to merge changes that occur in the phoneme-to-phone mapping into existing words The other possible approach, to build words based on the surface forms found in the input rather than the concatenation of existing words, is less attractive, both because it is computationally more difficult to estimate the effect of adding such words, and because surface forms are so variable This is tested by applying the algorithm to a multi-speaker corpus of continuous speech The algorithm was run on the Brown corpus , a collection of approximately one million words of text drawn from diverse sources, and a standard test of language models5% of the description is devoted to the parameters (the words and other overhead and the rest to the text The final dictionary contains 30,347 words A slight adjustment of the conditions for creating words produces a larger dictionary, of 42,668 words, that has a slightly poorer compression rate of 2 This is impressive, considering that the simple language model used in this work has no access to context, and naively reproduces syntactic and morphological regularities time and time again for words with similar behavior Even with no special knowledge of the space character, the algorithm adopts a policy of placing spaces at the start of words The goal, as in Cartright and Brent , is to segment the speech into words After ten iterations of training on the phoneme sequences, the algorithm produces a dictionary of 6,630 words, and a segmentation of the input The experiments we have performed on raw speech are preliminary, and included here principally to demonstrate that our algorithm does learn words even in the very worst of conditions A phone-to-speech model was created using supervised training on the TIMIT continuous speech database The final dictionary contains 1097 words after training on the transcriptions, and 728 words after training on the speech Most of the difference is in the longer words: as might be expected, performance is much poorer on the raw speech Yet our algorithm has learned a number of long words well enough that they can be reliably found in the data, even when the underlying form does not exactly match the observed input If the meaning of a sentence is a function of the meanings of the words in the sentence (such as the union then the meaning of a word should likewise be that function applied to the meanings of the words in its representation Then since there is no chance of a word with meaning moccurring in the input, all words with that meaning can effectively be removed from the dictionary and probabilities renormalized The probabilities of all other words will increase, and their code lengths shorten Since word meanings are tied to compression, they can be learned by altering the meaning of a word when such a move reduces the combined description length At this time, we have not conducted experiments on learning word meanings with speech, though the possibility of learning a complete dictation device from speech and textual transcripts is not beyond imagination The algorithm also occasionally produces words that cross real-word boundaries, like ed by the (see figure  Thus, we have a means of representing sequences of terminals and abstract categories by concatenating context-free rules that look very much like words This suggests merging the notion of rule and word Then words are sequences of terminals and abstract categories that are represented by concatenating words and 's The first difference disappears if each word has its own category; in essence, the category takes the place of the word index The algorithm we have described for learning words has several properties that make it a particularly good tool for solving language engineering problems Using our algorithm for text compression, for instance, enables the compressed text to be searched or indexed in terms of intuitive units like words Secondly, the algorithm is unsupervised no wanna like words  The components of that representation are also words with representations Olivier and Wolff , were among the first who implemented algorithms that attempt to learn words from text using techniques based on prediction Ellison has used three-level compression schemes to acquire intermediate representations in a manner similar to how we acquire words The unsupervised acquisition of words from continuous speech has received relatively little study This is the first work to present a complete specification of an unsupervised algorithm that learns words from speech, and we hope it will lead researchers to study unsupervised language-learning techniques in greater detail In contrast to several prior proposals, our algorithm makes no assumptions about the presence of facilitative side information, or of cleanly spoken and segmented speech, or about the distribution of sounds within words What is more, during speech production sounds blend across word boundaries, and words undergo tremendous phonological and acoustic variation: what are you doing is often pronounced /wc  The optimal dictionary is the one that produces the shortest description of both the speech stream and the dictionary itself Thus, the principal motivation for discovering words and other facts about language is that this knowledge can be used to improve compression, or equivalently, prediction A working unsupervised algorithm would both dispel many of the learnability-argument myths surrounding child language acquisition, and be a valuable tool to the natural language engineering community There is a conceptual difficulty with using a minimum description-length learning framework when the input is a speech signal: speech is continuous, and can be specified to an arbitrary degree of precision