 Together, terminals and nonterminals are called words  Figure presents a complete description of thecatinthehat, in which the input and six words used in the description are decomposed using a multigram language model The input is represented by four words, thecat+i+n+thehat Given a dictionary, we can compute word probabilities over word and input representations using EM; for the language model described here this is simply the Baum-Welch procedure The first is quite interesting Similarly, words are deleted when doing so would reduce the combined description length This generally occurs as shorter words are rendered irrelevant by longer words that model more of the input The final dictionary contains 30,347 words The final dictionary contains 1097 words after training on the transcriptions, and 728 words after training on the speech Then words are sequences of terminals and abstract categories that are represented by concatenating words and 's no wanna like words  The components of that representation are also words with representations The unsupervised acquisition of words from continuous speech has received relatively little study