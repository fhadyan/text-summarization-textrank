 we cannot measure the effect of Hobbs' syntax assumption since it is difficult to say how likely one is to get the wrong parse Although both algorithms are part of theories of discourse that posit the interaction of the algorithm with an inference or intentional component, we will not use reasoning in tandem with the algorithm's operation A comparison of the two algorithms on each data set individually and an overall analysis on the three data sets combined revealed no significant differences in the performance of the two algorithms ( , not significant  In addition for each algorithm alone we tested whether there were significant differences in performance for different textual types Both of the algorithms performed significantly worse on the task dialogues ( for Hobbs, for BFP, p [ 0 ERROR CHAINING refers to the fact that once an algorithm makes an error, other errors can result However one can and should measure the effects of error chaining, since reporting numbers that correct for error chaining is misleading, but if the error that produced the error chain can be corrected then the algorithm might show a significant improvement In this analysis, error chains contributed 22 failures to Hobbs' algorithm and 19 failures to BFP Figure also fails to indicate whether the two algorithms missed the same examples or are covering a different set of phenomena, i Since the main purpose of evaluation must be to improve the theory that we are evaluating, the most interesting cases are the ones on which the algorithms' performance varies and those that neither algorithm gets correct This analysis gives us some indication of the contribution of syntactic constraints, task structure and global focus to anaphoric processing The Hobbs algorithm will correctly choose the end as the antecedent for the second it The BFP algorithm on the other hand will get two interpretations, one in which the second it co-specifies the red piece and one in which it co-specifies the end All of the examples on which BFP succeed and Hobbs fails have to do with extended discussion of one discourse entity Since an executive vice-president is centered in the first sentence, and continued in each following sentence, the BFP algorithm will correctly choose the cospecifier Among the examples that neither algorithm gets correctly are 20 examples from the task dialogues of it referring to the global focus, the pump Neither Hobbs algorithm nor BFP attempt to cover these examples When both parties contribute discourse entities to the common ground, both algorithms may fail (n=4  Thus the relevant factor here may be the switching of control among discourse participants  Task structure in the pump dialogues is an important factor especially as it relates to the use of global focus This lets BFP get the two examples of event anaphora Consider the goal of adding a discourse component to a system, or evaluating and improving one that is already in place In other two-person dialogues, both parties may contribute discourse entities to the conversation on a more equal basis Hobbs discusses the fact that his algorithm cannot be modified to get event anaphora in  Another interesting fact is that in every case in which Hobbs' algorithm gets the correct co-specifier and BFP didn't, the relevant factor is Hobbs' preference for intrasentential co-specifiers One view on these cases may be that these are not discourse anaphora, but there seems to be no principled way to make this distinction He argues that intra-sentential candidates (ISCs) should be preferred over candidates from the previous utterance, ONLY in the cases where no discourse center has been established or the discourse center is rejected for syntactic or selectional reasons He then uses Hobbs algorithm to produce an ordering of these ISCs This is compatible with the centering framework since it is underspecified as to whether one should always choose to establish a discourse center with a co-specifier from a previous utterance Of these Carter's rule clearly gets 5, and another 3 seem to rest on whether one might want to establish a discourse entity from a previous utterance Since the addition of this constraint does not allow BFP to get any examples that neither algorithm got, it seems that this combination is a way of making the best out of both algorithms See the Figure  The only significant difference as a result of the modifications is that the BFP algorithm now performs significantly better on the pump dialogues alone (  In 59% to 82% of the cases both algorithms get the correct result In addition, this analysis has shown, that at least for task-oriented dialogues global focus is a significant factor, and in general discourse structure is more important in the task dialogues We analyse the performance of the algorithms on three types of data Two of the samples are those that Hobbs used when developing his algorithm A discourse module might combine theories on, e In this case the BFP centering algorithm and Hobbs algorithm both make ASSUMPTIONS about other system components There are two major sets of assumptions, based on discourse segmentation and syntactic representation We attempt to make these explicit for each algorithm and pinpoint where the algorithms might behave differently were these assumptions not well-founded For example, since the BFP salience hierarchy for discourse entities is based on grammatical relation, an implicit assumption is that an utterance only has one subject In this situation a system might flag the utterance as ambiguous and draw in support from other discourse components This arises in the present analysis for two reasons: (1) the constraints given by do not always allow one to choose a preferred interpretation, (2) the BFP algorithm proposes equally ranked interpretations in parallel This doesn't happen with the Hobbs algorithm because it proposes interpretations in a sequential manner, one at a time We chose to count as a failure those situations in which the BFP algorithm only reduces the number of possible interpretations, but Hobbs algorithm stops with a correct interpretation We also have not needed to make a decision on how to score an algorithm that only finds one interpretation for an utterance that humans find ambiguous The centering algorithm as defined by Brennan, Friedman and Pollard, (BFP algorithm is derived from a set of rules and constraints put forth by Grosz, Joshi and Weinstein ,  We shall not reproduce this algorithm here (See  There are two main structures in the centering algorithm, the CB, the BACKWARD LOOKING CENTER, which is what the discourse is `about and an ordered list, CF, of FORWARD LOOKING CENTERS, which are the discourse entities available to the next utterance for pronominalization In the centering framework, the order of the forward-centers list is intended to reflect the salience of discourse entities The BFP algorithm orders this list by grammatical relation of the complements of the main verb, i This captures the intuition that subjects are more salient than other discourse entities The BFP algorithm added linguistic constraints on CONTRA-INDEXING to the centering framework The BFP algorithm depends on semantic processing to precompute these constraints, since they are derived from the syntactic structure, and depend on some notion of c-command  subject The BFP algorithm assumes that some other mechanism can structure both written texts and task-oriented dialogues into hierarchical segments Speaker's intentions, task structure, cue words like O We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues ,  In the task-oriented dialogues, the action PICK-UP marks task boundaries hence segment boundaries Segment initial utterances therefore are the only situation where the BFP algorithm will prefer a within-sentence noun phrase as the cospecifier of a pronoun The Hobbs algorithm is based on searching for a pronoun's co-specifier in the syntactic parse tree of input sentences  Hobbs algorithm operates on one sentence at a time, but the structure of previous sentences in the discourse is available Thus there are two parts to this paper: we present the quantitative results of hand-simulating these algorithms (henceforth Hobbs algorithm and BFP algorithm but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general Hobbs does not assume a segmentation of discourse structure in this algorithm; the algorithm will go back arbitrarily far in the text to find an antecedent In more recent work, Hobbs uses the notion of COHERENCE RELATIONS to structure the discourse  The order by which Hobbs' algorithm traverses the parse tree is the closest thing in his framework to predictions about which discourse entities are salient This amounts to a claim that different discourse entities are salient, depending on the position of a pronoun in a sentence When seeking an intersentential co-specification, Hobbs algorithm searches the parse tree of the previous utterance breadth-first, from left to right Turning to the assumptions about syntax, we note that Hobbs assumes that one can produce the correct syntactic structure for an utterance, with all adjunct phrases attached at the proper point of the parse tree Hobbs also assumes that his algorithm can somehow collect discourse entities mentioned alone into sets as co-specifiers of plural anaphors A major component of any discourse algorithm is the prediction of which entities are salient, even though all the factors that contribute to the salience of a discourse entity have not been identified , , ,  So an obvious question is when the two algorithms actually make different predictions The main difference is that the choice of a co-specifier for a pronoun in the Hobbs algorithm depends in part on the position of that pronoun in the sentence In the centering framework, no matter what criteria one uses to order the forward-centers list, pronouns take the most salient entities as antecedents, irrespective of that pronoun's position