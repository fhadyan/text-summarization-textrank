 Although both algorithms are part of theories of discourse that posit the interaction of the algorithm with an inference or intentional component, we will not use reasoning in tandem with the algorithm's operation In addition for each algorithm alone we tested whether there were significant differences in performance for different textual types Both of the algorithms performed significantly worse on the task dialogues ( for Hobbs, for BFP, p [ 0 ERROR CHAINING refers to the fact that once an algorithm makes an error, other errors can result In this analysis, error chains contributed 22 failures to Hobbs' algorithm and 19 failures to BFP This analysis gives us some indication of the contribution of syntactic constraints, task structure and global focus to anaphoric processing The Hobbs algorithm will correctly choose the end as the antecedent for the second it The BFP algorithm on the other hand will get two interpretations, one in which the second it co-specifies the red piece and one in which it co-specifies the end All of the examples on which BFP succeed and Hobbs fails have to do with extended discussion of one discourse entity Since an executive vice-president is centered in the first sentence, and continued in each following sentence, the BFP algorithm will correctly choose the cospecifier Among the examples that neither algorithm gets correctly are 20 examples from the task dialogues of it referring to the global focus, the pump Neither Hobbs algorithm nor BFP attempt to cover these examples When both parties contribute discourse entities to the common ground, both algorithms may fail (n=4  Thus the relevant factor here may be the switching of control among discourse participants  This lets BFP get the two examples of event anaphora In other two-person dialogues, both parties may contribute discourse entities to the conversation on a more equal basis Hobbs discusses the fact that his algorithm cannot be modified to get event anaphora in  Another interesting fact is that in every case in which Hobbs' algorithm gets the correct co-specifier and BFP didn't, the relevant factor is Hobbs' preference for intrasentential co-specifiers One view on these cases may be that these are not discourse anaphora, but there seems to be no principled way to make this distinction He then uses Hobbs algorithm to produce an ordering of these ISCs This is compatible with the centering framework since it is underspecified as to whether one should always choose to establish a discourse center with a co-specifier from a previous utterance Of these Carter's rule clearly gets 5, and another 3 seem to rest on whether one might want to establish a discourse entity from a previous utterance In addition, this analysis has shown, that at least for task-oriented dialogues global focus is a significant factor, and in general discourse structure is more important in the task dialogues We analyse the performance of the algorithms on three types of data Two of the samples are those that Hobbs used when developing his algorithm A discourse module might combine theories on, e In this case the BFP centering algorithm and Hobbs algorithm both make ASSUMPTIONS about other system components There are two major sets of assumptions, based on discourse segmentation and syntactic representation We attempt to make these explicit for each algorithm and pinpoint where the algorithms might behave differently were these assumptions not well-founded For example, since the BFP salience hierarchy for discourse entities is based on grammatical relation, an implicit assumption is that an utterance only has one subject This doesn't happen with the Hobbs algorithm because it proposes interpretations in a sequential manner, one at a time We chose to count as a failure those situations in which the BFP algorithm only reduces the number of possible interpretations, but Hobbs algorithm stops with a correct interpretation We shall not reproduce this algorithm here (See  There are two main structures in the centering algorithm, the CB, the BACKWARD LOOKING CENTER, which is what the discourse is `about and an ordered list, CF, of FORWARD LOOKING CENTERS, which are the discourse entities available to the next utterance for pronominalization The BFP algorithm orders this list by grammatical relation of the complements of the main verb, i This captures the intuition that subjects are more salient than other discourse entities The BFP algorithm added linguistic constraints on CONTRA-INDEXING to the centering framework The BFP algorithm depends on semantic processing to precompute these constraints, since they are derived from the syntactic structure, and depend on some notion of c-command  subject The BFP algorithm assumes that some other mechanism can structure both written texts and task-oriented dialogues into hierarchical segments Speaker's intentions, task structure, cue words like O We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues ,  Segment initial utterances therefore are the only situation where the BFP algorithm will prefer a within-sentence noun phrase as the cospecifier of a pronoun The Hobbs algorithm is based on searching for a pronoun's co-specifier in the syntactic parse tree of input sentences  Hobbs algorithm operates on one sentence at a time, but the structure of previous sentences in the discourse is available Thus there are two parts to this paper: we present the quantitative results of hand-simulating these algorithms (henceforth Hobbs algorithm and BFP algorithm but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general Hobbs does not assume a segmentation of discourse structure in this algorithm; the algorithm will go back arbitrarily far in the text to find an antecedent In more recent work, Hobbs uses the notion of COHERENCE RELATIONS to structure the discourse  The order by which Hobbs' algorithm traverses the parse tree is the closest thing in his framework to predictions about which discourse entities are salient This amounts to a claim that different discourse entities are salient, depending on the position of a pronoun in a sentence When seeking an intersentential co-specification, Hobbs algorithm searches the parse tree of the previous utterance breadth-first, from left to right Hobbs also assumes that his algorithm can somehow collect discourse entities mentioned alone into sets as co-specifiers of plural anaphors A major component of any discourse algorithm is the prediction of which entities are salient, even though all the factors that contribute to the salience of a discourse entity have not been identified , , ,  So an obvious question is when the two algorithms actually make different predictions