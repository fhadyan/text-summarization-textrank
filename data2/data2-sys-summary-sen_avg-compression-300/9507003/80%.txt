 The notion of robustness in natural language processing is a rather broad one and lacks a precise definition Initially, it assigns a set of morphologically justified syntactic labels to each word form in the input sentence In the latter case, a total disambiguation cannot be achieved by purely syntactic means, as in the following attachment example: In contrast to traditional grammars of the phrase structure type which license well-formed structures according to their rule system, constraint grammar rather happens to be an eliminative approach information decrease The mutual compatibility of modifying relations is checked against a set of constraints and thus the set of possible modifications is successively reduced by a constraint propagation mechanism Whereas Constraint Grammar restricts itself to purely syntactic means, an integration of simple semantic criteria into Constraint Dependency Grammar has been proposed recently  In order to facilitate functional independence it will become necessary to establish separate layers for structural description and constraint propagation a syntactic layer relating word forms according to functional surface structure notions e The following small and rather rigid sample grammar illustrates the different types of constraints needed: 1 linear ordering constraints sy3: synlab(X SUBJ pos(dep(X pos(syndom(X The subject precedes the finite verb sy1 through sy3 are unary constraints, sy4 is a binary one Note that constraints refer to modifying relations instead of word forms In a very similar fashion semantic constraints comprise 1 Semantic constraints need not be restricted to linguistically motivated (i So far, one of the most striking shortcomings has been the strictly binary nature of constraint satisfaction Penalty factors may range from zero to one where pf=0 specifies a strict constraint in the classical sense and 0[pf[1 indicates a soft constraint accepting contradictory cases with a confidence value proportional to pf Obviously, a value of one is meaningless because it neutralizes the constraint compatibility matrices within the constraint satisfaction problem no longer contain binary categories but confidence scores also ranging from zero (for impossible combinations) up to one (for combinations not even violating a single constraint  Inappropriate readings are excluded only if they violate strict constraints On the semantic layer only the licensing constraint se1 is declared as a strict one the calculation of initial confidence scores for all combinations of syntactic and semantic modification relations and 2 Hence, structural interpretations violating a high number of rather strong constraints are pruned first Using the toy grammar specified above together with its penalty scores the arbitration process between syntactic and semantic evidence in simple disambiguation problems can be studied It switches to the alternative interpretation only in the case of combined syntactic distortions which, if desired, could be taken as a headline-style utterance, syntactic evidence will gain the upper hand against the violation of two selectional constraints This interpretation, however, happens to be a rather fragile one and breaks immediately under arbitrary syntactic variation On the other hand, constraint relaxation techniques rely on a systematic variation of existing grammar rules written for standard input Whether syntactic evidence is propagated from the syntactic to the semantic layer or vice versa depends only on the available information Eliminating implausible interpretations by locally pruning less favoured modification relations represents only one, though fundamental method for the disambiguation of natural language utterances Preference-induced constraints consist of implications which, given enough evidence for the unary precondition P, require the possibly binary constraint C to hold Constraints of this type can be used to model e agreement conditions in syntactic rules Preference-induced constraints can also be used to modify value assignments at certain nodes in the constraint network without the necessity to copy them Combining the eliminative nature of a disambiguation procedure with a system architecture supporting bidirectional arbitration between syntactic and semantic evidence has turned out to be a key factor for achieving a higher level of robustness in language understanding It allows to treat syntactic ill-formedness and semantic deviations by providing a mechanism for mutual compensation Insufficient modelling information on any one of the processing layers might well result in the selection of an odd interpretation but will not cause the language processing unit to break down entirely Since structural disambiguation by constraint satisfaction likewise lends itself to the creation of time sensitive parsing procedures , in the long run it might provide a unifying foundation to build language processing systems upon which embody aspects of robustness against such different disruptive factors as syntactically ill-formed input, metaphorical use and dynamic time constraints Even a superficial comparison with human processing principles shows the fundamental deficit of these approaches Robustness in human language processing does not amount to an additional effort, but instead facilitates both, insensitivity to ill-formed input as well as a flexible adaptation to temporal restrictions Psycholinguistic evidence provides a contradictory picture of human language processing Some observations clearly support a rather strong modular organization with processing units of great autonomy like syntax and semantics  On the other hand there is a considerable semantic influence on the assignment of syntactic structure which suggests a highly integrated processing architecture It allows to yield an at least vague interpretation even in cases of extremely distorted input: 1 Syntactically ill-formed utterances are interpreted based on semantic and background knowledge even if subcategorization regularities or other grammatical constraints are violated Parallel and autonomous structures in language processing have not only evolved between syntactic and semantic aspects of language Here, expectations come to play at two different dimensions: Syntactic, semantic and pragmatic predictions about future input derived from previous parts of the utterance or dialogue Expectations exchanged between parallel and autonomous processing structures for syntax and semantics Certain syntactic constructions may trigger specific semantic interpretations, a view which is strongly supported by the traditional perspective on the relation between syntax and semantics In the opposite direction, semantic relations, e Hence, there is a third principle of robust language processing upon which the human model builds One of the more popular examples surely is Head-Driven Phrase Structure Grammar (HPSG where syntactic and semantic descriptions are uniquely related to each other by coreferential pointers within the framework of typed feature structures from subcategorization information Since syntactic and semantic restrictions are conjunctively combined the overall vulnerability against arbitrary impairment of the input utterances even increases: An analysis may now fail due to syntactic as well as due to semantic reasons It combines syntactic, semantic and even pragmatic information in a single representation named construction Although the parser is guided in its decisions by different kinds of preferences, the mapping between syntactic and semantic representations seems to be a strict one