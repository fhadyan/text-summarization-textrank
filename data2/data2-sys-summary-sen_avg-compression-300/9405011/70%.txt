 In collaborative expert-consultation dialogues, two participants (executing agent and consultant) work together to construct a plan for achieving the executing agent's domain goal The executing agent and the consultant bring to the plan construction task different knowledge about the domain and the desirable characteristics of the resulting domain plan The proposed additions now consist of actions agreed upon by both agents and will therefore be incorporated into the existing model The process can be successfully applied to the problem-solving level because both the domain and problem-solving levels represent actions that the agents propose to do (at a later point in time for the domain level and at the current time for the problem-solving level however, the discourse level actions are actions that are currently being executed, instead of proposed for execution Hence, once a set of actions is proposed by an agent, the other agent must first evaluate the proposal based on his own private beliefs and determine whether or not to accept the proposal Problems will arise if the system convinces the user that Dr Therefore, we argue that instead of applying the arbitration process to the discourse level, it should be applied to the beliefs proposed by the discourse actions The belief level captures domain-related beliefs proposed by discourse actions as well as the relationship amongst them Figure outlines the dialogue model for utterances ( ) with the additional belief level Note that each Inform action at the discourse level proposes a mutual belief, and that supports relationships (inferred from Address-Acceptance) are proposed between the mutual beliefs The evaluation process starts at the proposed domain level However, an examination of the proposed belief level causes the proposal to be rejected because the system does not believe that Dr Allen proposed different plan modalities that capture the shared and individual beliefs during collaboration, and Grosz, Sidner and Lochbaum , proposed a SharedPlan model for capturing intentions during a collaborative process Thus their meta-plans do not handle correction of proposed additions to the dialogue model, since this generally does not involve adding a step to the proposal However, they only handle cases in which the user fails to understand the system, instead of cases in which the user disagrees with the system Notice that this model is essentially a recursive one: the Modify action in itself contains a full collaborative process an agent's proposal of a modification, the other agent's evaluation of the proposal, and potential modification to the modification! We capture this theory in a plan-based system for response generation in collaborative task-oriented interactions Our system can initiate subdialogues to negotiate implicitly proposed additions to the shared plan, can appropriately respond to user queries that are motivated by ill-formed or suboptimal solutions, and handles in a unified manner the negotiation of proposed domain actions, proposed problem-solving actions, and beliefs proposed by discourse actions The system is presented with the existing dialogue model and the actions proposed by the user's new utterances We assume that the current status of the interaction is represented by a tripartite dialogue model that captures intentions on three levels: domain, problem-solving, and discourse The domain level contains the domain plan being constructed for later execution The problem-solving level contains the agents' intentions about how to construct the domain plan, and the discourse level contains the communicative plan initiated to further their joint problem-solving intentions Each utterance by a participant constitutes a proposal intended to affect the shared model of domain, problem-solving, and discourse intentions These inferred actions explain why the user asked the question and are actions that the user is implicitly proposing be added to the plan shared plans in a collaborative planning process, we separate the dialogue model into an existing model, which consists of a shared plan agreed upon by both agents, and the proposed additions, which contain newly inferred actions A proposal consists of a chain of actions for addition to the shared plan Thus, the evaluator should check for two types of discrepancies in beliefs: one that causes the proposal to be viewed by the system as invalid , and one in which the system believes that a better alternative to the user's proposal exists ,  Based on this evaluation, the system determines whether it should accept the user's proposal, causing the proposed actions to be incorporated into the existing model, or should reject the proposal, in which case a negotiation subdialogue will be initiated The processes for detecting conflicts and better alternatives start at the top-level proposed action, and are interleaved because we intend for the system to address the highest-level action disagreed upon by the agents This is because it is meaningless to suggest, for example, a better alternative to an action when one believes that its parent action is infeasible Pollack argues that a plan can fail because of an infeasible action or because the plan itself is ill-formed  A plan is considered ill-formed if child actions do not contribute to their parent action as intended; hence, the evaluator performs a well-formedness check to examine, for each pair of parent-child actions in the proposal, whether the contributes relationship holds between them If the system knows of a substantially superior alternative to the proposal, but does not suggest it to the user, it cannot be said to have fulfilled its responsibility as a collaborative agent; hence the system must model user characteristics in order to best tailor its identification of sub-optimal plans to individual users Our system maintains a user model that includes the user's preferences The preferences are represented in the form, prefers user, _attribute object, _value _action, _strength which indicates that _user has a _strength preference that the attribute _attribute of _object be _value when performing _action Suppose that the evaluator must determine whether an action Ai (in a chain of proposed actions ) is the best way of performing its parent action Ai+1 We demonstrate the ranking advisor by showing how two different instantiations, CS601 and CS621, of the Take-Course action are ranked Figure shows the relevant domain knowledge and user model information The ranking advisor matches the user's preferences against the domain knowledge for each of CS601 and CS621 The model treats utterances as proposals open for negotiation and only incorporates a proposal into the shared plan under construction if both agents believe the proposal to be appropriate Each specialization eventually decomposes into some primitive action which modifies the proposal If the system does not accept a user proposal, the system attempts to modify it, and natural language utterances are generated as a part of this process If the user accepts the system's beliefs, thus satisfying the precondition of Modify-Relation, the original dialogue model can be modified; however, if the user rejects the system's beliefs, he will invoke the Modify-Proposal action to revise the system's suggested modification of his original proposal The former is selected if the action itself is inappropriate, and will cause the action to be removed from the dialogue model Figure illustrates the dialogue model that would result from the following utterances (5) Who is teaching AI? The evaluation process, which determines whether or not to accept the proposal, starts at the top-level proposed domain action, Satisfy-Seminar-Course(U,CS  The evaluator then checks its child action Take-Course(U,AI  The modifier performs the Modify-Proposal action, which selects as its specialization Correct-Relation, because the rejected proposal is ill-formed Notice that the arbitration process (the problem-solving level in Figure ) operates on the entire dialogue model in Figure , and therefore is represented as meta-level problem-solving actions The Inform action further decomposes into two actions, one which tells the user of the belief, and one which provides support for the claim