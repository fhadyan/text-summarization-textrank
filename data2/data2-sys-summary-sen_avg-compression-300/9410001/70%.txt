 In speech recognition and understanding systems, many kinds of language model may be used to choose between the word and sentence hypotheses for which there is evidence in the acoustic data04 for a unigram language model, 2 Specifically, each clustering was tested against 1,354 hypothesis lists output by a version of the DECIPHER (TM) speech recognizer (Murveit et al, 1993) that itself used a (rather simpler) bigram model Secondly, extending the model may greatly increase the amount of training data required if sparseness problems are to be kept under control, and additional data may be unavailable or expensive to collect The unigram and bigram scores show a steady and, in fact, statistically significant increase with the number of clusters Using twenty clusters for bigrams (score 43 In the second experiment, each training sentence and each test sentence hypothesis was analysed by the Core Language Engine (Alshawi, 1992) trained on the ATIS domain (Agns et al, 1994  Unanalysable sentences were discarded, as were sentences of over 15 words in length (the ATIS adaptation had concentrated on sentences of 15 words or under, and analysis of longer sentences was less reliable and slower  For the purpose of the experiment, clustering and hypothesis selection were performed on the basis not of the words in a sentence but of the grammar rules used to construct its most preferred analysis A sentence was modeled simply as a bag of rules, and no attempt (other than the clustering itself) was made to account for dependencies between rules The training corpus consisted of the 4,279 sentences in the 5,873-sentence set that were analysable and consisted of fifteen words or less These results show that clustering gives a significant advantage for both the 1-rule and the 2-rule types of model, and that the more clusters are created, the larger the advantage is, at least up to K=20 clusters I have suggested that training corpus clustering can be used both to extend the effectiveness of a very general class of language models, and to provide evidence of whether a particular language model could benefit from extending it by hand to allow it to take better account of context Clustering can be useful even when there is no reason to believe the training corpus naturally divides into any particular number of clusters on any extrinsic grounds The experimental results presented show that clustering increases the (absolute) success rate of unigram and bigram language modeling for a particular ATIS task by up to about 12 and that performance improves steadily as the number of clusters climbs towards 100 (probably a reasonable upper limit, given that there are only a few thousand training sentences  The method, which is related to that of Iyer, Ostendorf and Rohlicek (1994 involves clustering the sentences in the training corpus into a number of subcorpora, each predicting a different probability distribution for linguistic objects This is important evidence for the main hypothesis of this paper: that enhancing a language model with clustering, which once the software is in place can be done largely automatically, can give us important clues about whether it is worth expending research, programming, data-collection and machine resources on hand-coded improvements to the way in which the language model in question models context, or whether those resources are best devoted to different, additional kinds of language model Secondly, it makes only modest additional demands on training data However, clustering can have two important uses I also show that, for the same task and corpus, clustering produces improvements when sentences are assessed not according to the words they contain but according to the syntax rules used in their best parse This work thus goes beyond that of Iyer et al by focusing on the methodological importance of corpus clustering, rather than just its usefulness in improving overall system performance, and by exploring in detail the way its effectiveness varies along the dimensions of language model type, language model complexity, and number of clusters used It also differs from Iyer et al's work by clustering at the utterance rather than the paragraph level, and by using a training corpus of thousands, rather than millions, of sentences; in many speech applications, available training data is likely to be quite limited, and may not always be chunked into paragraphs Most other work on clustering for language modeling (e Pereira, Tishby and Lee, 1993; Ney, Essen and Kneser, 1994) has addressed the problem of data sparseness by clustering words into classes which are then used to predict smoothed probabilities of occurrence for events which may seldom or never have been observed during training Firstly, it involves clustering whole sentences, not words Secondly, its aim is not to tackle data sparseness by grouping a large number of objects into a smaller number of classes, but to increase the precision of the model by dividing a single object (the training corpus) into some larger number of sub-objects (the clusters of sentences  There is no reason why clustering sentences for prediction should not be combined with clustering words to reduce sparseness; the two operations are orthogonal Our type of clustering, then, is based on the assumption that the utterances to be modeled, as sampled in a training corpus, fall more or less naturally into some number of clusters so that words or other objects associated with utterances have probability distributions that differ between clusters However, if the clustering reflects significant dependencies, some of the worst inaccuracies of these assumptions may be reduced, and system performance may improve as a result However, clustering may also give us significant leverage in monolingual cases Other corpora, such as Wall Street Journal articles, might also be expected to fall naturally into clusters for different subject areas, and indeed Iyer et al (1994) report positive results from corpus clustering here For some applications, though, there is no obvious extrinsic basis for dividing the training corpus into clusters Even a clustering that only partly reflects the underlying variability of the data may give us more accurate predictions of utterance likelihoods But this trade-off, and the effectiveness of different clustering algorithms, can be monitored and optimized by applying the resulting cluster-based language models to unseen test data There are many different criteria for quantifying the (dis)similarity between (analyses of) two sentences or between two clusters of sentences; Everitt (1993) provides a good overview Perhaps the most obvious measure of the similarity between two sentences or clusters is then Jaccard's coefficient (Everitt, 1993, p41 the ratio of the number of words occurring in both sentences to the number occurring in either or both This goal is analogous to that used in the work described earlier on finding word classes by clustering For our simple unigram language model without clustering, the training corpus perplexity is minimized (and its likelihood is maximized) by assigning each word wi a probability pi = fi/N, where fi is the frequency of wi and N is the total size of the corpus Present each remaining training corpus sentence in turn, initially creating an additional singleton cluster cK+1 for it It also reduces the arbitrariness introduced into the clustering process by the order in which the training sentences are presented Experiments were carried out to assess the effectiveness of clustering, and therefore the existence of unexploited contextual dependencies, for instances of two general types of language model In the second experiment, evaluation was on the basis of grammar rules used rather than word occurrences Each run was repeated for ten different random orders for presentation of the training data The unclustered (K=1) version of each language model was also evaluated The per-item entropy of the training set (i