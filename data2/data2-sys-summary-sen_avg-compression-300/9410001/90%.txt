 In speech recognition and understanding systems, many kinds of language model may be used to choose between the word and sentence hypotheses for which there is evidence in the acoustic data04 for a unigram language model, 2 Using twenty clusters for bigrams (score 43 The training corpus consisted of the 4,279 sentences in the 5,873-sentence set that were analysable and consisted of fifteen words or less I have suggested that training corpus clustering can be used both to extend the effectiveness of a very general class of language models, and to provide evidence of whether a particular language model could benefit from extending it by hand to allow it to take better account of context Clustering can be useful even when there is no reason to believe the training corpus naturally divides into any particular number of clusters on any extrinsic grounds The method, which is related to that of Iyer, Ostendorf and Rohlicek (1994 involves clustering the sentences in the training corpus into a number of subcorpora, each predicting a different probability distribution for linguistic objects Secondly, it makes only modest additional demands on training data However, clustering can have two important uses I also show that, for the same task and corpus, clustering produces improvements when sentences are assessed not according to the words they contain but according to the syntax rules used in their best parse This work thus goes beyond that of Iyer et al by focusing on the methodological importance of corpus clustering, rather than just its usefulness in improving overall system performance, and by exploring in detail the way its effectiveness varies along the dimensions of language model type, language model complexity, and number of clusters used Most other work on clustering for language modeling (e Firstly, it involves clustering whole sentences, not words There is no reason why clustering sentences for prediction should not be combined with clustering words to reduce sparseness; the two operations are orthogonal However, clustering may also give us significant leverage in monolingual cases There are many different criteria for quantifying the (dis)similarity between (analyses of) two sentences or between two clusters of sentences; Everitt (1993) provides a good overview (See e Present each remaining training corpus sentence in turn, initially creating an additional singleton cluster cK+1 for it It also reduces the arbitrariness introduced into the clustering process by the order in which the training sentences are presented In the second experiment, evaluation was on the basis of grammar rules used rather than word occurrences The unclustered (K=1) version of each language model was also evaluated