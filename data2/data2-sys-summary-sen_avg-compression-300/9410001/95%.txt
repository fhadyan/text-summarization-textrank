04 for a unigram language model, 292 respectively Using twenty clusters for bigrams (score 43 Clustering can be useful even when there is no reason to believe the training corpus naturally divides into any particular number of clusters on any extrinsic grounds The method, which is related to that of Iyer, Ostendorf and Rohlicek (1994 involves clustering the sentences in the training corpus into a number of subcorpora, each predicting a different probability distribution for linguistic objects Secondly, it makes only modest additional demands on training data However, clustering can have two important uses This work thus goes beyond that of Iyer et al by focusing on the methodological importance of corpus clustering, rather than just its usefulness in improving overall system performance, and by exploring in detail the way its effectiveness varies along the dimensions of language model type, language model complexity, and number of clusters used Most other work on clustering for language modeling (e Firstly, it involves clustering whole sentences, not words There is no reason why clustering sentences for prediction should not be combined with clustering words to reduce sparseness; the two operations are orthogonal However, clustering may also give us significant leverage in monolingual cases The unclustered (K=1) version of each language model was also evaluated