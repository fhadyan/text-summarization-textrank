 In section , we discuss our use of machine learning tools to automatically construct decision trees for segmentation from a large set of input features would be coded using the features in Fig The prosodic and cue phrase features were motivated by previous results in the literature Passonneau examined some of the few claims relating discourse anaphoric noun phrases to global discourse structure in the Pear corpus Both the hand tuned and automatically derived algorithms improve over our previous algorithms The segmentation algorithms presented in the next two sections were developed by examining only a training set of narratives The algorithms are then evaluated by examining their performance in predicting segmentation on a separate test set We currently use 10 narratives for training and 5 narratives for testing (The remaining 5 narratives are reserved for future research The 10 training narratives range in length from 51 to 162 phrases (Avg 101 The 5 test narratives range in length from 47 to 113 phrases (Avg 87 The ratios of test to training data measured in narratives, prosodic phrases and clauses, respectively, are 50 For the machine learning algorithm we also estimate performance using cross-validation , as detailed in Section  The primary benefit of the hand tuning is to identify new input features for improving performance To quantify algorithm performance, we use the information retrieval metrics shown in Fig Table shows the average human performance for both the training and test sets of narratives Machine learning tools make it convenient to perform numerous experiments, to use large feature sets, and to evaluate results using cross-validation To improve performance, we analyzed the two types of IR errors made by the original NP algorithm on the training data mis-classification of boundaries, often occurred where prosodic and cue features conflicted with NP features The original NP algorithm assigned boundaries wherever the three values coref infer global Table presents the average IR scores across the narratives in the training set for both conditions The linguistic structure of Grosz and Sidner's tri-partite discourse model consists of multi-utterance segments whose hierarchical relations are isomorphic with intentional structure Table shows the results of the hand tuned algorithm on the 5 randomly selected test narratives on both Conditions 1 and 2 Condition 1 results, the untuned algorithm with the initial feature set, are very similar to the training set except for worse precision This is strong evidence that the tuned algorithm is a better predictor of segment boundaries than the original NP algorithm Nevertheless, the test results of condition 2 are much worse than the corresponding training results, particularly for precision 44 versus  This confirms that the tuned algorithm is over calibrated to the training set5 to automatically develop segmentation algorithms from our corpus of coded narratives, where each potential boundary site has been classified and represented as a set of linguistic features5 specifies the names of the classes to be learned (boundary and non-boundary and the names and potential values of a fixed set of coding features (Fig The second input is the training data, i Our training set of 10 narratives provides 1004 examples of potential boundary sites5 is a classification algorithm expressed as a decision tree, which predicts the class of a potential boundary given its set of feature values Because machine learning makes it convenient to induce decision trees under a wide variety of conditions, we have performed numerous experiments, varying the number of features used to code the training data, the definitions used for classifying a potential boundary site as boundary or non-boundary and the options available for running the C4 This decision tree was learned under the following conditions: all of the features shown in Fig were used to code the training data, boundaries were classified as discussed in section , and C4 The decision tree predicts the class of a potential boundary site based on the features before, after, duration, cue1, word1, coref, infer, and global Note that although not all available features are used in the tree, the included features represent 3 of the 4 general types of knowledge (prosody, cue phrases and noun phrases  The performance of this learned decision tree averaged over the 10 training narratives is shown in Table , on the line labeled Learning 1  Note that Learning 1 performance is comparable to human performance (Table while Learning 2 is slightly better than humans The results obtained via machine learning are also somewhat better than the results obtained using hand tuning particularly with respect to precision Condition 2 in Table  We also use the resampling method of cross-validation to estimate performance, which averages results over multiple partitions of a sample into test versus training data We performed 10 runs of the learning program, each using 9 of the 10 training narratives for that run's training set (for learning the tree) and the remaining narrative for testing We have presented two methods for developing segmentation hypotheses using multiple linguistic features The first method hand tunes features and algorithms based on analysis of training errors Both methods rely on an enriched set of input features compared to our previous work Note that quantitatively, the machine learning results are slightly better than the hand tuning results Furthermore, note that the machine learning algorithm used the changes to the coding features that resulted from the tuning methods Moser and Moore had an expert coder assign segments and various segment features and relations based on RST Discourse structures are derived from subjects' segmentations, then statistical measures are used to characterize these structures in terms of acoustic-prosodic features Hearst presented two implemented segmentation algorithms based on term repetition, and compared the boundaries produced to the boundaries marked by at least 3 of 7 subjects, using information retrieval metrics Kozima had 16 subjects segment a simplified short story, developed an algorithm based on lexical cohesion, and qualitatively compared the results We analyzed linear segmentations of 20 narratives performed by naive subjects (7 new subjects per narrative where speaker intention was the segment criterion Subjects were given transcripts, asked to place a new segment boundary between lines (prosodic phrases) wherever the speaker had a new communicative goal, and to briefly describe the completed segment Subjects were free to assign any number of boundaries We found significant agreement among naive subjects on a discourse segmentation task, which suggests that global discourse units have some objective reality9 and the rate at which subjects assigned boundaries ranged from 5 Despite this variation, we found statistically significant agreement among subjects across all narratives on location of segment boundaries (  We used three distinct algorithms based on the distribution of referential noun phrases, cue words, and pauses, respectively Each algorithm (NP-A, CUE-A, PAUSE-A) was designed to replicate the subjects' segmentation task (break up a narrative into contiguous segments, with segment breaks falling between prosodic phrases  NP-A used three features, while CUE-A and PAUSE-A each made use of a single feature However, we also found poor correlation of three untuned algorithms (based on features of referential noun phrases, cue words, and pauses, respectively) with the subjects' segmentations No algorithm or combination of algorithms performed as well as humans We represent each narrative in our corpus as a sequence of potential boundary sites, which occur between prosodic phrases Agreement among subjects on boundaries was significant at below the  In this paper, we discuss two methods for developing segmentation algorithms using multiple knowledge sources The boxes in the figure show the subjects' responses at each potential boundary site, and the resulting boundary classification Given a narrative of n prosodic phrases, the n-1 potential boundary sites are between each pair of prosodic phrases Piand Pi+1, i from 1 to n-1 Each potential boundary site in our corpus is coded using the set of linguistic features shown in Fig The cue phrase features are also obtained by automatic analysis of the transcripts Two of the noun phrase (NP) features are hand-coded, along with functionally independent clauses (FICs following  globalpro = global illustrates how the first boundary site in Fig