 SPATTER consists of three main decision-tree models: a part-of-speech tagging model, a node-extension model, and a node-labeling model The training algorithm proceeds as follows The training corpus is divided into two sets, approximately 90% for tree growing and 10% for tree smoothing For each parsed sentence in the tree growing corpus, the correct state sequence is traversedEach event is used as a training example for the decision-tree growing process for the appropriate feature's tree (e The parsing procedure is a search for the highest probability parse tree The probability of a parse is just the product of the probability of each of the actions made in constructing the parse, according to the decision-tree models A search error occurs when the the highest probability parse found by the parser is not the highest probability parse in the space of all parses SPATTER's search procedure uses a two phase approach to identify the highest probability parse of a sentence First, the parser uses a stack decoding algorithm to quickly find a complete parse for the sentence In this second mode, it can safely discard any partial parse which has a probability lower than the probability of the highest probability completed parse Using these two search modes, SPATTER guarantees that it will find the highest probability parse Experimentally, the search algorithm guarantees the highest probability parse is found for over 96% of the sentences parsed7 words These sentences are the same test sentences used in the experiments reported for IBM's parser in  SPATTER parses word sequences, not tag sequences Instead, it uses a probabilistic model to assign tags to the words, and considers all possible tag sequences according to the probability they are assigned by the model of constituents which violate constituent boundaries with a constituent in the treebank parse I begin by describing decision-tree modeling, showing that decision-tree models are equivalent to interpolated n-gram models The decisions under consideration involve identifying constituents and constituent labels in natural language sentences The first question a decision tree might ask is: 1 What is the word being tagged? If the answer is the, then the decision tree needs to ask no more questions; it is clear that the decision tree should assign the tag with probability 1 What is the tag of the previous word? If the answer to question 2 is determiner, the decision tree might stop asking questions and assign the tag with very high probability, and the tag with much lower probability The decision tree described in this paragraph is shown in Figure  Each node defines a probability distribution on the space of possible decisions A node at which the decision tree stops asking questions is a leaf node A decision-tree model is not really very different from an interpolated n-gram model The number of parameters in this n-gram model is Using this definition, an n-gram model can be represented by a decision-tree model with n-1 questions What is the tag of the word two words back? But can a decision-tree model be represented by an n-gram model? No, but it can be represented by an interpolated n-gram model The standard approach to estimating an n-gram model is a two step process A decision-tree model can be represented by an interpolated n-gram model as follows A leaf node in a decision tree can be represented by the sequence of question answers, or history values, which leads the decision tree to that leaf These probabilities are estimated using statistical decision tree models n-gram models with very large n The probability of a complete parse tree (T) of a sentence (S) is the product of each decision (di) conditioned on all previous decisions: Each decision sequence constructs a unique parse, and the parser selects the parse whose decision sequence yields the highest cumulative probability The SPATTER parsing algorithm is based on interpreting parsing as a statistical pattern recognition process A parse tree for a sentence is constructed by starting with the sentence's words as leaves of a tree structure, and labeling and extending nodes these nodes until a single-rooted, labeled tree is constructed A parse tree can be viewed as an n-ary branching tree, with each node in a tree labeled by either a non-terminal label or a part-of-speech label If a parse tree is interpreted as a geometric pattern, a constituent is no more than a set of edges which meet at the same tree node In SPATTER, a parse tree is encoded in terms of four elementary components, or features: words, tags, labels, and extensions For an n word sentence, a parse tree has n leaf nodes, where the word feature value of the ith leaf node is the ith word in the sentence