 Parsing a natural language sentence can be viewed as making a sequence of disambiguation decisions: determining the part-of-speech of the words, choosing between possible constituent structures, and selecting labels for the constituents SPATTER consists of three main decision-tree models: a part-of-speech tagging model, a node-extension model, and a node-labeling model For the tagging model, the values of the previous two words and their tags are also asked, since they might differ from the head words of the previous two constituents The training algorithm proceeds as follows The training corpus is divided into two sets, approximately 90% for tree growing and 10% for tree smoothing For each parsed sentence in the tree growing corpus, the correct state sequence is traversedEach event is used as a training example for the decision-tree growing process for the appropriate feature's tree (e each tagging event is used for growing the tagging tree, etc  After the decision trees are grown, they are smoothed using the tree smoothing corpus using a variation of the deleted interpolation algorithm described in  The parsing procedure is a search for the highest probability parse tree The probability of a parse is just the product of the probability of each of the actions made in constructing the parse, according to the decision-tree models A search error occurs when the the highest probability parse found by the parser is not the highest probability parse in the space of all parses SPATTER's search procedure uses a two phase approach to identify the highest probability parse of a sentence First, the parser uses a stack decoding algorithm to quickly find a complete parse for the sentence In this second mode, it can safely discard any partial parse which has a probability lower than the probability of the highest probability completed parse Using these two search modes, SPATTER guarantees that it will find the highest probability parse Experimentally, the search algorithm guarantees the highest probability parse is found for over 96% of the sentences parsed7 words These sentences are the same test sentences used in the experiments reported for IBM's parser in  SPATTER parses word sequences, not tag sequences Furthermore, SPATTER does not simply pre-tag the sentences and use only the best tag sequence in parsing Instead, it uses a probabilistic model to assign tags to the words, and considers all possible tag sequences according to the probability they are assigned by the model of constituents which violate constituent boundaries with a constituent in the treebank parse Figure indicates the frequency of each sentence length in the test corpus The SPATTER parser illustrates how large amounts of contextual information can be incorporated into a statistical model for parsing by applying decision-tree learning algorithms to a large annotated corpus I begin by describing decision-tree modeling, showing that decision-tree models are equivalent to interpolated n-gram models The decisions under consideration involve identifying constituents and constituent labels in natural language sentences The first question a decision tree might ask is: 1 What is the word being tagged? If the answer is the, then the decision tree needs to ask no more questions; it is clear that the decision tree should assign the tag with probability 1 If, instead, the answer to question 1 is bear, the decision tree might next ask the question: 2 What is the tag of the previous word? If the answer to question 2 is determiner, the decision tree might stop asking questions and assign the tag with very high probability, and the tag with much lower probability However, if the answer to question 2 is noun, the decision tree would need to ask still more questions to get a good estimate of the probability of the tagging decision The decision tree described in this paragraph is shown in Figure  Each question asked by the decision tree is represented by a tree node (an oval in the figure) and the possible answers to this question are associated with branches emanating from the node Each node defines a probability distribution on the space of possible decisions A node at which the decision tree stops asking questions is a leaf node all contexts which lead to the same leaf node have the same probability distribution for the decision The candidate disambiguators are the words in the sentence, relationships among the words, and relationships among constituents already constructed in the parsing process A decision-tree model is not really very different from an interpolated n-gram model The number of parameters in this n-gram model is Using this definition, an n-gram model can be represented by a decision-tree model with n-1 questions Hence, this 4-gram tagging model is the same as a decision-tree model which always asks the sequence of 3 questions: 1 What is the tag of the word two words back? But can a decision-tree model be represented by an n-gram model? No, but it can be represented by an interpolated n-gram model The standard approach to estimating an n-gram model is a two step process A decision-tree model can be represented by an interpolated n-gram model as follows A leaf node in a decision tree can be represented by the sequence of question answers, or history values, which leads the decision tree to that leaf As ngrows, the parameter space for an n-gram model grows exponentially, and it quickly becomes computationally infeasible to estimate the smoothed model using deleted interpolation These probabilities are estimated using statistical decision tree models n-gram models with very large n Therefore, just as it is necessary to smooth empirical n-gram models, it is also necessary to smooth empirical decision-tree models The probability of a complete parse tree (T) of a sentence (S) is the product of each decision (di) conditioned on all previous decisions: Each decision sequence constructs a unique parse, and the parser selects the parse whose decision sequence yields the highest cumulative probability A question which has k values is decomposed into a sequence of binary questions using a classification tree on those k values These 30 questions are determined by growing a classification tree on the word vocabulary as described in  The SPATTER parsing algorithm is based on interpreting parsing as a statistical pattern recognition process A parse tree for a sentence is constructed by starting with the sentence's words as leaves of a tree structure, and labeling and extending nodes these nodes until a single-rooted, labeled tree is constructed This pattern recognition process is driven by the decision-tree models described in the previous section A parse tree can be viewed as an n-ary branching tree, with each node in a tree labeled by either a non-terminal label or a part-of-speech label If a parse tree is interpreted as a geometric pattern, a constituent is no more than a set of edges which meet at the same tree node In SPATTER, a parse tree is encoded in terms of four elementary components, or features: words, tags, labels, and extensions For an n word sentence, a parse tree has n leaf nodes, where the word feature value of the ith leaf node is the ith word in the sentence