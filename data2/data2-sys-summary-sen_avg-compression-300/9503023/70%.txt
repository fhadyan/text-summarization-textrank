 The pattern matching capabilities of neural networks can be used to detect syntactic constituents of natural language The sentence is first decomposed into the broad syntactic categories pre-subject - subject - predicate by locating the subject These filters or rules differ fundamentally from generative rules that produce allowable strings in a language At this stage the data is ready to present to the neural net The input to the net is derived from the candidate strings, the sequences of tags and hypertags This highly redundant code will aid the processing of sparse data typical of natural language The net that gave best results was a simple single layer net (Figure derived from the Hodyne net of wyard This is conventionally a single layer net, since there is one layer of processing nodes Multi-layer networks, which can process linearly inseparable data, were also investigated, but are not necessary for this particular processing task The net is presented with training strings whose desired classification has been manually marked Then the weights are fixed and the trained net is ready to classify unseen sentences When a string is presented to the network in training mode, it activates a set of input nodes Then a neural net selects the string with the correct placement It then outlines the neural net selection process When the trained net is run on unseen data the weights on the links are fixed Since we are working towards a hierarchical language structure, we may want the words within constituents correctly tagged, ready for the next stage of processing correct- A also requires that the words within the subject are correctly tagged When parses are postulated for a sentence negative as well as positive examples are likely to occur Now, in natural language negative correlations are an important source of information: the occurrence of some words or groups of words inhibit others from following The core process is data driven, as the parameters of the neural networks are derived from training text For example, in sentence (3) above strings 3 The neural net is trained in supervised mode on examples that have been manually marked correct and incorrect n can never be correct These should be distinguished from possibly correct parses that are not in the training data He examined the process of learning the grammar of a formal language from examples He showed that, for languages at least as high in the Chomsky hierarchy as CFGs, inference from positive data alone is strictly less powerful than inference from both positive and negative data together However, with positive data alone a problem of over generalization arises: the postulated grammar may be a superset of the real grammar, and sentences that are outside the real grammar could be accepted If both positive and negative data is used, counter examples will reduce the postulated grammar so that it is nearer the real grammar A grammar may be inferred from positive examples alone for certain subsets of regular languages , or an inference process may degenerate into a look up procedure if every possible positive example is stored In our method the required parse is found by inferring the grammar from both positive and negative information, which is effectively modelled by the neural net Future work will investigate the effect of training the networks on the positive examples alone With our current size corpus there is not enough data Any single rule prohibiting a tuple of adjacent tags could be omitted and the neural network would handle it by linking the node representing that tuple to no only However, for some processing steps we need to reduce the number of candidate tag strings presented to the neural network to manageable proportions (see Section  The data must be pre-processed by filtering through the prohibition rule constraints If the number of candidate strings is within desirable bounds, such as for the head detection task, no rules are used Our system is data driven as far as possible: the rules are invoked if they are needed to make the problem computationally tractable Since data can be represented as higher order tuples, single layer networks can be used We have also used multi-layer nets on this data: they have no advantages, and perform slightly less well  The most laborious part of this work is preparing the training data Computational tractability is further addressed by reducing data through the application of prohibitive rules as local constraints We run marked up training data through an early version of the network trained on the same data, so the results should be almost all correct Moreover, they can utilise more of the implicit information in the training data by modelling negative relationships It is necessary to locate the subject, then identify the head and determine its number in order to translate the main verb correctly in sentences like (1) below (1) This parser has been trained to find the syntactic subject head that agrees in number with the main verb In this work neural networks are used as part of a fully automated system that finds a partial parse of declarative sentences The first step in constraining the problem size is to partition an unlimited vocabulary into a restricted number of part-of-speech tags For the first processing stage we need to place the subject markers, and, as a further task, disambiguate tags It was not found necessary to use number information at this stage However, the head of the subject is then found and number agreement with the verb can be assessed At this stage the tagset, mode 2, includes number information and has 28 classes Information theoretic tools can be used to find the entropy of different tag sequence languages, and support decisions on representation Tag disambiguation is part of the parsing task, handled by the neural net and its pre-processor Other words are tagged using suffix information, or else defaults are invoked It will take a sentence, locate the subject and then find the head of the subject In the same way that tags are allocated to words, or to punctuation marks, they can represent the boundaries of syntactic constituents, such as noun phrases and verb phrases Boundary markers can be considered invisible tags, or hypertags, which have probabilistic relationships with adjacent tags in the same way that words do Using this representation a hierarchical language structure is converted to a string of tags represented by a linear vector This system generates sets of tag strings for each sentence, with the hypertags placed in all possible positions Thus, for the subject detection task: Then the performance of the pump must be monitored (3) will generate strings of tags including: [ Then ] the performance of the pump must be monitored There were arbitrary limits of a maximum of 10 words in the pre-subject and 10 words within the subject for the initial work described here However, some words will have more than one possible tag For instance, in sentence (1) above 5 words have 2 alternative tags, which will generate 2[5] possible strings before the hypertags are inserted Since there are 22 words (including punctuation ) the total number of strings would be  For instance, the subject must contain a noun-type word Applying this particular rule to sentence (3) above would eliminate candidate strings (3 A small number are excluded because the system cannot handle a co-ordinated head The grammatic framework alone does not reduce the number of candidate strings sufficiently for the subject detection stage These are adjacent tags which are not allowed, such as determiner - verb or start of subject - verb  By using these methods the number of candidate strings is drastically reduced For the technical manuals an average of 4 strings, seldom more than 15 strings, are left