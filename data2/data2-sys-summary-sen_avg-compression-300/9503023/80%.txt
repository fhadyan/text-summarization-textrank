 The pattern matching capabilities of neural networks can be used to detect syntactic constituents of natural language The sentence is first decomposed into the broad syntactic categories pre-subject - subject - predicate by locating the subject These filters or rules differ fundamentally from generative rules that produce allowable strings in a language At this stage the data is ready to present to the neural net The input to the net is derived from the candidate strings, the sequences of tags and hypertags This highly redundant code will aid the processing of sparse data typical of natural language The net that gave best results was a simple single layer net (Figure derived from the Hodyne net of wyard This is conventionally a single layer net, since there is one layer of processing nodes Multi-layer networks, which can process linearly inseparable data, were also investigated, but are not necessary for this particular processing task The net is presented with training strings whose desired classification has been manually marked Then a neural net selects the string with the correct placement The input layer potentially has a node for each possible tuple It then outlines the neural net selection process When the trained net is run on unseen data the weights on the links are fixed correct- A also requires that the words within the subject are correctly tagged Now, in natural language negative correlations are an important source of information: the occurrence of some words or groups of words inhibit others from following The core process is data driven, as the parameters of the neural networks are derived from training text For example, in sentence (3) above strings 3 The neural net is trained in supervised mode on examples that have been manually marked correct and incorrect  He examined the process of learning the grammar of a formal language from examples He showed that, for languages at least as high in the Chomsky hierarchy as CFGs, inference from positive data alone is strictly less powerful than inference from both positive and negative data together However, with positive data alone a problem of over generalization arises: the postulated grammar may be a superset of the real grammar, and sentences that are outside the real grammar could be accepted If both positive and negative data is used, counter examples will reduce the postulated grammar so that it is nearer the real grammar In our method the required parse is found by inferring the grammar from both positive and negative information, which is effectively modelled by the neural net Future work will investigate the effect of training the networks on the positive examples alone With our current size corpus there is not enough data However, for some processing steps we need to reduce the number of candidate tag strings presented to the neural network to manageable proportions (see Section  The data must be pre-processed by filtering through the prohibition rule constraints If the number of candidate strings is within desirable bounds, such as for the head detection task, no rules are used Since data can be represented as higher order tuples, single layer networks can be used The most laborious part of this work is preparing the training data We run marked up training data through an early version of the network trained on the same data, so the results should be almost all correct Moreover, they can utilise more of the implicit information in the training data by modelling negative relationships (1) This parser has been trained to find the syntactic subject head that agrees in number with the main verb For the first processing stage we need to place the subject markers, and, as a further task, disambiguate tags It was not found necessary to use number information at this stage However, the head of the subject is then found and number agreement with the verb can be assessed Tag disambiguation is part of the parsing task, handled by the neural net and its pre-processor Other words are tagged using suffix information, or else defaults are invoked It will take a sentence, locate the subject and then find the head of the subject Using this representation a hierarchical language structure is converted to a string of tags represented by a linear vector This system generates sets of tag strings for each sentence, with the hypertags placed in all possible positions Thus, for the subject detection task: Then the performance of the pump must be monitored However, some words will have more than one possible tag For instance, in sentence (1) above 5 words have 2 alternative tags, which will generate 2[5] possible strings before the hypertags are inserted Since there are 22 words (including punctuation ) the total number of strings would be  For instance, the subject must contain a noun-type word Applying this particular rule to sentence (3) above would eliminate candidate strings (3 The grammatic framework alone does not reduce the number of candidate strings sufficiently for the subject detection stage These are adjacent tags which are not allowed, such as determiner - verb or start of subject - verb  By using these methods the number of candidate strings is drastically reduced