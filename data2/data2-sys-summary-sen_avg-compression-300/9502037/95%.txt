 27 were parsed correctly, ie 50 were not parsed at all, ie \t\t  Therefore, when tuned to any particular language corpus the resulting grammar will be effectively finite-state2 N [ ] S(rel) [ t\t02 \t\tN [ ] S(np) [ t\t02 \t\tN [ ] S(rel) [ t\t0 Factor out other features which are merely passed from state to state Establish word paradigms, ie classes of words which occur with similar transitions These paradigms will correspond to a great extent to the word classes of rule-based grammars threw \t\tVP NP, X(out) \t\tprob: p1 VP X(out NP \t\tprob: p2 Even if p1 were considerably greater than p2, the cumulative negative effect of the longer states in (10) would eventually lead to the model giving the sentence with the shifted NP (11) a higher probability It would not be difficult to make a small extension to the present model to capture such information, namely by introducing an additional feature containing the lexical value of the head of a phrase