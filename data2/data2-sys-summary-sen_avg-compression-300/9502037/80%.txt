 27 were parsed correctly, ie 50 were not parsed at all, ie one or more of the transitions necessary to find a parse path was lacking, even after generalizing the transitions The formalism should be fine-grained, ie can be represented as: \t\tThe \t\tS \t\t[ ] man \t\tN \t\t[VP] gave \t\tVP \t\t[ ] (1a t\tthe \t\tNP \t\t[NP] dog \t\tN \t\t[NP] a \t\tNP \t\t[ ] bone \t\tN \t\t[ ] Intuitively, syntactic links between non-adjacent words, impossible in a standard finite-state grammar, are here established by passing categories along on the stack through the state of intervening words A perusal of the state transitions associated with individual words in (1a) reveals an obvious relationship to the types of categorial grammar \t\tFido \t\tS \t\t[ ] had \t\tVP \t\t[ ] a \t\tNP \t\t[NP(t (7a t\tbone \t\tN \t\t[NP(t yesterday \t\tNP(t) \t\t[N [NP(t and \t\tN \t\t[NP(t biscuit \t\tN \t\t[NP(t today \t\tNP(t) \t\t[ ] In this analysis instead of a regular transition for `bone' of: N [NP(t NP(t) [ ] there is instead a transition introducing coordination: N [NP(t NP(t) [N [NP(t Allowing categories on the stack to themselves have non-empty stacks moves the formalism one step further from being an indexed grammar It should be noted that an indefinite amount of centre-embedding can be described, but only at the expense of unlimited growth in the length of states: \t\tThe \t\tS \t\t[ ] fly \t\tN \t\t[VP] the \t\tS(np) \t\t[VP] dog \t\tN \t\t[VP(np VP] (8 t\tthe \t\tS(np) \t\t[VP(np VP] cat \t\tN \t\t[VP(np VP(np VP] scratched \t\tVP(np) \t\t[VP(np VP] swallowed \t\tVP(np) \t\t[VP] died \t\tVP \t\t[ ] This contrasts with unlimited right-recursion where there is no growth in state length: \t\tI \t\tS \t\t[ ] saw \t\tVP \t\t[ ] the \t\tNP \t\t[ ] cat \t\tN \t\t[ ] (9 t\tthat \t\tS(rel) \t\t[ ] scratched \t\tVP \t\t[ ] the \t\tNP \t\t[ ] dog \t\tN \t\t[ ] that \t\tS(rel) \t\t[ ] \t\t  \t\t  Therefore, when tuned to any particular language corpus the resulting grammar will be effectively finite-state2 N [NP(t S(rel) [NP(t t\t02 N [ ] S(rel) [ t\t0 However this simple-minded approach, although easy to implement, in other ways leaves much to be desired Although far from exhausting the possible methods for smoothing, the following three are those used in the implementation described at the end of the paper2 \t\tN [ ] [S(rel t\t02 \t\tN [ ] S(np) [ t\t02 \t\tN [ ] S(rel) [ t\t0 Factor out other features which are merely passed from state to state Establish word paradigms, ie classes of words which occur with similar transitions These paradigms will correspond to a great extent to the word classes of rule-based grammars The advantage would be retained however that the system is still fine-grained enough to reflect the idiosyncratic patterns of individual words and could override this paradigmatic information if sufficient data were available Although essential for effective processing, the smoothing operations may give rise to new problems Taking as an example heavy-NP shift suppose that the corpus contained two distinct transitions for the word `threw with the particle `out' both before and after the object threw \t\tVP NP, X(out) \t\tprob: p1 VP X(out NP \t\tprob: p2 Even if p1 were considerably greater than p2, the cumulative negative effect of the longer states in (10) would eventually lead to the model giving the sentence with the shifted NP (11) a higher probability It would not be difficult to make a small extension to the present model to capture such information, namely by introducing an additional feature containing the lexical value of the head of a phrase Transition probabilities were generalized in the ways discussed in the previous section