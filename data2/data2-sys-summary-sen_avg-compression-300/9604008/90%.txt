 Thus, every STSG tree would be produced by the PCFG with equal probability In the section covering previous research, we considered the most probable derivation and the most probable parse tree We call the best parse tree under this criterion the Maximum Constituents Parse Bod shows that the most probable derivation does not perform as well as the most probable parse for the DOP model, getting 65% exact match for the most probable derivation, versus 96% correct for the most probable parse This is not surprising, since each parse tree can be derived by many different derivations; the most probable parse criterion takes all possible derivations into account However, while the Inside-Outside algorithm is a grammar re-estimation algorithm, the algorithm presented here is just a parsing algorithm We use the reduction and algorithm to parse held out test data, comparing these results to a replication of Pereira and Schabes on the same data After that, put the most likely constituents together to form a parse tree, using dynamic programming The original ATIS data from the Penn Tree Bank, version 0 We also ran experiments using Bod's data, 75 sentence test sets, and no limit on sentence length Now, we note that the conditional probability of the most probable parse tree will in general decline exponentially with sentence length Since the probability of the most probable parse decreases exponentially in sentence length, the number of random samples needed to find this most probable parse increases exponentially in sentence length In the DOP model, a sentence cannot be given an exactly correct parse unless all productions in the correct parse occur in the training set Bod randomly split his corpus into test and training Examining Bod's data, we find he removed productions Now, use these trees to form a Stochastic Tree Substitution Grammar (STSG  These trees can be combined in various ways to parse sentences There are two existing ways to parse using the DOP model One could try to find the most probable parse tree For a given sentence and a given parse tree, there are many different derivations that could lead to that parse tree The probability of the parse tree is the sum of the probabilities of the derivations Given our example, there are two different ways to generate the parse tree x 1E x 1B 2S each with probability , so that the parse tree has probability  This parse tree is most probable Bod shows how to approximate this most probable parse using a Monte Carlo algorithm Parsing using the DOP model is especially difficult We call a PCFG tree isomorphic to a STSG tree if they are identical when internal non-terminals are changed to external non-terminals For every STSG subderivation, there would be an isomorphic PCFG subderivation, with equal probability Thus for every STSG derivation, there would be an isomorphic PCFG derivation, with equal probability Thus every STSG tree would be produced by the PCFG with equal probability