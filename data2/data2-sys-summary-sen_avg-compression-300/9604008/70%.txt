 Unfortunately, the number of trees is in general exponential in the size of the training corpus trees, producing an unwieldy grammar This would change the probabilities of the derivations; however the probabilities of parse trees would not change, since there would be correspondingly more derivations for each tree Now, the desired one to one relationship holds: for every derivation in the new STSG there is an isomorphic derivation in the PCFG with equal probability Thus, summing over all derivations of a tree in the STSG yields the same probability as summing over all the isomorphic derivations in the PCFG Thus, every STSG tree would be produced by the PCFG with equal probability Since the total probability of the trees produced by the STSG is 1, and the PCFG produces these trees with the same probability, no probability is left over for any other trees There are several different evaluation metrics one could use for finding the best parse In the section covering previous research, we considered the most probable derivation and the most probable parse tree If our performance evaluation were based on the number of constituents correct, using measures similar to the crossing brackets measure, we would want the parse tree that was most likely to have the largest number of correct constituents Thus, this tree has on average 2 constituents correct We call the best parse tree under this criterion the Maximum Constituents Parse Notice that this parse tree cannot even be produced by the grammar: each of its constituents is good, but it is not necessarily good when considered as a full tree Bod shows that the most probable derivation does not perform as well as the most probable parse for the DOP model, getting 65% exact match for the most probable derivation, versus 96% correct for the most probable parse Next, we present an algorithm for parsing, which returns the parse that is expected to have the largest number of correct constituents This is not surprising, since each parse tree can be derived by many different derivations; the most probable parse criterion takes all possible derivations into account Similarly, the Maximum Constituents Parse is also derived from the sum of many different derivations However, while the Inside-Outside algorithm is a grammar re-estimation algorithm, the algorithm presented here is just a parsing algorithm However, unlike in the HMM case where the algorithm produces a simple state sequence, in the PCFG case a parse tree is produced, resulting in additional constraints We use the reduction and algorithm to parse held out test data, comparing these results to a replication of Pereira and Schabes on the same data After that, put the most likely constituents together to form a parse tree, using dynamic programming The probability that a potential constituent occurs in the correct parse tree, , will be called g(s, t, X  We can compute this probability using elements of the Inside-Outside algorithm The entry maxc[1, n] contains the expected number of correct constituents, given the model The original ATIS data from the Penn Tree Bank, version 0 For this paper, we conducted two sets of experiments: one using a minimally cleaned set of data, making our results comparable to previous results; the other using the ATIS data prepared by Bod, which contained much more significant revisions Ten data sets were constructed by randomly splitting minimally edited ATIS (Hemphill et al 1990) sentences into a 700 sentence training set, and 88 sentence test set, then discarding sentences of length ] 30 We also ran experiments using Bod's data, 75 sentence test sets, and no limit on sentence length It is also noteworthy that the results are much better on Bod's data than on the minimally edited data: crossing brackets rates of 96% and 97% on Bod's data versus 90% on minimally edited data Furthermore, we believe Bod's analysis of his parsing algorithm is flawed However, for his algorithm to have some reasonable chance of finding the most probable parse, the number of times he must sample his data is at least inversely proportional to the conditional probability of that parse For instance, if the maximum probability parse had probability 1/50, then he would need to sample at least 50 times to be reasonably sure of finding that parse Now, we note that the conditional probability of the most probable parse tree will in general decline exponentially with sentence length A linear increase in ambiguity will lead to an exponential decrease in probability of the most probable parse Since the probability of the most probable parse decreases exponentially in sentence length, the number of random samples needed to find this most probable parse increases exponentially in sentence length Thus, when using the Monte Carlo algorithm, one is left with the uncomfortable choice of exponentially decreasing the probability of finding the most probable parse, or exponentially increasing the runtime In the DOP model, a sentence cannot be given an exactly correct parse unless all productions in the correct parse occur in the training set Thus, we can get an upper bound on performance by examining the test corpus and finding which parse trees could not be generated using only productions in the training corpus Bod randomly split his corpus into test and training According to his thesis , only one of his 75 test sentences had a correct parse which could not be generated from the training data An analysis of Bod's data shows that at least some of the difference in performance between his results and ours must be due to an extraordinarily fortuitous choice of test data Examining Bod's data, we find he removed productions The first number in each column is the probability that a sentence in the training data will have a production that occurs nowhere else We have given efficient techniques for parsing the DOP model Now, use these trees to form a Stochastic Tree Substitution Grammar (STSG  These trees can be combined in various ways to parse sentences There are two existing ways to parse using the DOP model First, one can find the most probable derivation Using the most probable derivation criterion, one simply finds the most probable way that a sentence could be produced Figure shows a simple example STSG For the string x x, what is the most probable derivation? The parse tree x 1A x 1CD 2S has probability of being generated by the trivial derivation containing a single tree This tree corresponds to the most probable derivation of x x One could try to find the most probable parse tree For a given sentence and a given parse tree, there are many different derivations that could lead to that parse tree The probability of the parse tree is the sum of the probabilities of the derivations Given our example, there are two different ways to generate the parse tree x 1E x 1B 2S each with probability , so that the parse tree has probability  This parse tree is most probable Bod shows how to approximate this most probable parse using a Monte Carlo algorithm The algorithm randomly samples possible derivations, then finds the tree with the most sampled derivations Bod shows that the most probable parse yields better performance than the most probable derivation on the exact match criterion Khalil Sima'an implemented a version of the DOP model, which parses efficiently by limiting the number of trees used and by using an efficient most probable derivation model We will need to create one new non-terminal for each node in the training data We will call non-terminals of this form interior non-terminals, and the original non-terminals in the parse trees exterior  We say that a PCFG derivation is isomorphic to a STSG derivation if there is a corresponding PCFG subderivation for every step in the STSG derivation Parsing using the DOP model is especially difficult Then, for trees such as AA AA 2A@j the probability of the tree is  The model can be summarized as a special kind of Stochastic Tree Substitution Grammar (STSG given a bracketed, labelled training corpus, let every subtree of that corpus be an elementary tree, with a probability proportional to the number of occurrences of that subtree in the training corpus Figure contains an example of isomorphic derivations, using two subtrees in the STSG and four productions in the PCFG We call a PCFG tree isomorphic to a STSG tree if they are identical when internal non-terminals are changed to external non-terminals Our main theorem is that this construction produces PCFG trees isomorphic to the STSG trees with equal probability For every STSG subderivation, there would be an isomorphic PCFG subderivation, with equal probability Thus for every STSG derivation, there would be an isomorphic PCFG derivation, with equal probability Thus every STSG tree would be produced by the PCFG with equal probability