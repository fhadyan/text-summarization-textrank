 The data contains about 19,000different direct object tokens, about 10,000 different verb tokens and about 140,000 different token pairs We use of the data as training and the rest as testing data The perplexity on the testing text using the clustering algorithm on the verb-object pairs is shown in Table  no predictor variable X) and the performance of the clustering algorithm on the usual bi-gram data (e The resulting data is certainly very noisy, but, as opposed to more accurate data obtained from a sophisticated parser, it would be feasible to use this method in a speech recogniser In other words, the data contains pairs like (is, chairman which would usually not be considered as a verb-direct object pair It is possible, that more accurate data (e The automaton then outputs a sequence of verb-object pairs, which constitute our training and testing data However, because of sparse training data, it is often difficult to estimate this distribution directly The conditional probability distribution is then calculated as which generally requires less training data  What is a suitable function F, also called optimisation criterion? Given a classification function G, we can estimate the probabilities pG(yl|xk) of equation using the maximum likelihood estimator, e relative frequencies: where gx=G1(xk gy=G2(yl) and N(x) denotes the number of times x occurs in the data Given these probability estimates pG(yl|xk the likelihood FMLof the training data, e the probability of the training data being generated by our probability estimates pG(yl|xk measures how well the training data is represented by the estimates and can be used as optimisation criterion (  In the following, we will derive an optimisation function FML in terms of frequency counts observed in the training data The likelihood of the training data FML is simply Assuming that the classification is unique, e However, the problem with this maximum likelihood criterion is that we first estimate the probabilities pG(yl|xk) on the training data T and then, given pG(yl|xk we evaluate the classification G on T We begin by presenting in section the process we use to obtain training and testing data from unrestricted English text In other words, both the classification G and the estimator pG(yl|xk) are trained on the same data The basic principle of cross-validation is to split the training data T into a retained part TR and a held-out part TH It divides the data into N-1 samples as retained part and only one sample as held-out part The advantage of this approach is that all samples are used in the retained and in the held-out part, thus making very efficient use of the existing data In other words, our held-out part TH to evaluate a classification G is the entire set of data points; but when we calculate the probability of the i[th] data point, we assume that our probability distribution pG(yl|xk) was estimated on all the data expect point i The data constitutes the input to a clustering algorithm and a language model, both of which are described in section  Let Ti denote the data without the pair (X[i Y[i and pG,Ti(yl|xk) the probability estimates based on a given classification G and training corpus Ti Let n0,Ti be the number of unseen pairs (gx, gy) and n Tithe number of seen pairs (gx, gy leading to the following smoothed estimate Ideally, we would make b depend on the classification, e75 during clustering the number of pairs that will be unseen when used as held-out part  Taking the logarithm, we obtain the final optimisation criterion F LO Given the F LO maximization criterion, we use the algorithm in Figure to find a good clustering function G Furthermore, since the clustering of one word affects the future clustering of other words, the order in which words are moved is important