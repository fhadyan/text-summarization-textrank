 Initial maximum Highest accuracy on the first iteration, and falling thereafter Early maximum Rising accuracy for a small number of iterations (2-4 and then falling as in initial maximum With a good lexicon but either degraded transitions or a test corpus differing from the training corpus, the pattern tends to be early maximum When the test corpus is very similar to the model, then the pattern is initial maximum If the test and training corpora are near-identical, do not use BW re-estimation; otherwise use for a small number of iterations If no such training corpus is available, but a lexicon with at least relative frequency data is available, use BW re-estimation for a small number of iterations If neither training corpus nor lexicon are available, use BW re-estimation with standard convergence tests such as perplexity 96% accuracy correct assignment of tags to word token, compared with a human annotator, is quoted, over a 500000 word corpus Instead, an approximate model is constructed by hand, which is then improved by BW re-estimation on an untagged training corpus For training from a hand-tagged corpus, the model is estimated by counting the number of transitions from each tag i to each tag j, the total occurrence of each tag i, and the total occurrence of word w with tag i Any transitions not seen in the training corpus are given a small, non-zero probability The lexicon lists, for each word, all of tags seen in the training corpus with their probabilities For words not found in the lexicon, all open-class tags are hypothesised, with equal probabilities The training and test corpora were drawn from the LOB corpus and the Penn treebank The first experiment concerned the effect of the initial conditions on the accuracy using Baum-Welch re-estimation D2 Lexical probabilities are proportional to the overall tag frequencies, and are hence independent of the actual occurrence of the word in the training corpus Perfect training is represented by case D0+T0 contains 32