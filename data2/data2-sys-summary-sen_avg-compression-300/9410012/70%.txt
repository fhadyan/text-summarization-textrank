 Part-of-speech tagging is the process of assigning grammatical categories to individual words in a corpus One widely used approach makes use of a statistical technique called a Hidden Markov Model (HMM  Alternatively, a procedure called Baum-Welch (BW) re-estimation may be used, in which an untagged corpus is passed through the FB algorithm with some initial model, and the resulting probabilities used to determine new values for the lexical and transition probabilities Initial maximum Highest accuracy on the first iteration, and falling thereafter In this case the initial model is of better quality than BW can achieve Early maximum Rising accuracy for a small number of iterations (2-4 and then falling as in initial maximum Secondly, we selected test corpora with varying degrees of similarity to the training corpus: the same text, text from a similar domain, and text which is significantly different The results appear in table , showing the best accuracy achieved (on ambiguous words  With a good lexicon but either degraded transitions or a test corpus differing from the training corpus, the pattern tends to be early maximum When the test corpus is very similar to the model, then the pattern is initial maximum If the test and training corpora are near-identical, do not use BW re-estimation; otherwise use for a small number of iterations If no such training corpus is available, but a lexicon with at least relative frequency data is available, use BW re-estimation for a small number of iterations If neither training corpus nor lexicon are available, use BW re-estimation with standard convergence tests such as perplexity Without a lexicon, some initial biasing of the transitions is needed if good results are to be obtained Similar results are presented by Merialdo (1994 who describes experiments to compare the effect of training from a hand-tagged corpora and using the Baum-Welch algorithm with various initial conditions In addition, although Merialdo does not highlight the point, BW re-estimation starting from less than 5000 words of hand-tagged text shows early maximum behaviour Perhaps what is needed is a similarity measure between two models M and M such that if a corpus were tagged with model M, M' is the model obtained by training from the output corpus from the tagger as if it were a hand-tagged corpus The model is defined by two collections of parameters: the transition probabilities, which express the probability that a tag follows the preceding one (or two for a second order model and the lexical probabilities, giving the probability that a word has a given tag without regard to words on either side of it 96% accuracy correct assignment of tags to word token, compared with a human annotator, is quoted, over a 500000 word corpus The Xerox tagger attempts to avoid the need for a hand-tagged training corpus as far as possible Instead, an approximate model is constructed by hand, which is then improved by BW re-estimation on an untagged training corpus The initial model set up so that some transitions and some tags in the lexicon are favoured, and hence having a higher initial probability Re-estimation on any of the words in a class therefore counts towards re-estimation for all of them To tag a text, the tags with non-zero probability are hypothesised for each word, and the most probable sequence of tags given the sequence of words is determined from the probabilities do not compare the success rate in their work with that achieved from a hand-tagged training text with no re-estimation The results suggest that a completely unconstrained initial model does not produce good quality results, and that one accurately trained from a hand-tagged corpus will generally do better than using an approach based on re-estimation, even when the training comes from a different source For training from a hand-tagged corpus, the model is estimated by counting the number of transitions from each tag i to each tag j, the total occurrence of each tag i, and the total occurrence of word w with tag i Any transitions not seen in the training corpus are given a small, non-zero probability The lexicon lists, for each word, all of tags seen in the training corpus with their probabilities For words not found in the lexicon, all open-class tags are hypothesised, with equal probabilities For a corpus in which a fraction a of the words are ambiguous, and p is the accuracy on ambiguous words, the overall accuracy can be recovered from 1-a+pa All of the accuracy figures quoted below are for ambiguous words only The training and test corpora were drawn from the LOB corpus and the Penn treebank The general pattern of the results presented does not vary greatly with the corpus and tagset used The first experiment concerned the effect of the initial conditions on the accuracy using Baum-Welch re-estimation A model was trained from a hand-tagged corpus in the manner described above, and then degraded in various ways to simulate the effect of poorer training, as follows: Lexicon D0 Un-degraded lexical probabilities, calculated from f(i,w f(i  D2 Lexical probabilities are proportional to the overall tag frequencies, and are hence independent of the actual occurrence of the word in the training corpus Perfect training is represented by case D0+T0 Corpus LOB-B-J was used to train the model, and LOB-B, LOB-L and LOB-B-G were passed through thirty iterations of the BW algorithm as untagged data01, and using larger corpora If a tagged corpus prepared by a human annotator is available, the transition and lexical probabilities can be estimated from the frequencies of pairs of tags and of tags associated with words Secondly, training from a hand-tagged corpus (case D0+T0) always does best, even when the test data is from a different source to the training data, as it is for LOB-L So perhaps it is worth investing effort in hand-tagging training corpora after all, rather than just building a lexicon and letting re-estimation sort out the probabilities