 Recently various methods for automatically constructing a thesaurus (hierarchically clustering words) based on corpus data have been proposed , , ,  We then constructed a number of thesauri based on these data, using our method We also evaluated our method by using a constructed thesaurus in a pp-attachment disambiguation experiment We also extracted as our test data 172 (verb,noun1,prep,noun2) patterns from the data in the same corpus, which is not used in the training data We then applied the learning method proposed in to learn case frame patterns with the constructed thesaurus as input using the same training data `Word-Based `MLE-Thesaurus and `MDL-Thesaurus' respectively stand for using word-based estimates, using a thesaurus constructed by employing MLE, and using a thesaurus constructed by our method In particular, we used this method with WordNet and using the same training data, and then conducted pp-attachment disambiguation experiment using the obtained case frame patterns A method of constructing a thesaurus based on corpus data usually consists of the following three steps: (i) Extract co-occurrence data (e We have proposed a method of clustering words based on large corpus data case frame data, adjacency data) from a corpus, (ii) Starting from a single class (or each word composing its own class divide (or merge) word classes based on the co-occurrence data using some similarity (distance) measure Our method of hierarchical clustering of words based on the MDL principle is theoretically sound Using a thesaurus constructed by our method can improve pp-attachment disambiguation results for his comments Suppose available to us are data like those in Figure , which are frequency data (co-occurrence data) between verbs and their objects extracted from a corpus (step (i  We then view the problem of clustering words as that of estimating a probabilistic model (representing probability distribution) that generates such data In this paper, we assume that the observed data are generated by a model belonging to the class of models just described, and select a model which best explains the data MDL stipulates that the best probability model for given data is that model which requires the least code length for encoding of the model itself, as well as the given data relative to it We refer to the code length for the model as the `model description length' and that for the data the `data description length We apply MDL to the problem of estimating a model consisting of a pair of partitions as described above The model description length quantifies the simplicity (complexity) of a model, and the data description length quantifies the fit to the data We will now describe how the description length for a model is calculated Given a model M and data S, its total description length L(M) is computed as the sum of the model description length Lmod(M the description length of its parameters Lpar(M and the data description length Ldat(M  (We often refer to Lmod(M) + Lpar(M) as the model description length  With the description length of a model defined in the above manner, we wish to select a model having the minimum description length and output it as the result of clustering The description lengths for the data in Figure using the two models in Figure are shown in Table  (Table shows some values needed for the calculation of the description length for Model 1 These calculations indicate that according to MDL, Model 1 should be selected over Model 2 We could in principle calculate the description length for each model and select a model with the minimum description length, if computation time were of no concern Figure shows our (divisive) clustering algorithm This is equivalent to minimizing the `data description length' as defined in Section 3, i We artificially constructed a true model of word co-occurrence, and then generated data according to its distribution We then used the data to estimate a model (clustering words and measured the KL distance between the true model and the estimated model The results indicate that MDL converges to the true model faster than MLE Also, MLE tends to select a model overfitting the data, while MDL tends to select a model which is simple and yet fits the data reasonably well