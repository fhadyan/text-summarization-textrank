 It is well known that statistical language models often suffer from a lack of training data Thus, if one starts out with an initial clustering in which no cluster occurs only once, and if one never moves words that occur only once, then one will never have a cluster which occurs only once Let C be the maximal number of clusters for G, let E be the number of elements one tries to cluster (e When one moves w from gw to g'w in the inner loop, one needs to change the counts N(gw, g2) and N(g'w, g2) for all g2 For each w, one needed to calculate the number of times w occurred with all clusters g2 It is well known that a trigram model outperforms a bigram model if there is sufficient training data Approach a which only uses one clustering function G, could produce |G V different clusterings (for each word in V, it can choose one of the |G| clusters  Approach c which uses one clustering function for the tuples wi-M wi-1 and one for wi, can produce |G1 V|M G2 V possible clusterings, including all the ones represented by approach a) and b  Similar to the derivation presented in section , one can now derive the optimisation criterion for approach c  The optimisation criterion for the extended algorithm is The corresponding clustering algorithm, which is shown in figure , is a straight forward extension of the one given in section  In order to achieve a clustered model with potentially high performance, the algorithm is then extended (section ) so that it can cluster higher order N-grams For each w, one needed to calculate the number of times w occurred with all clusters g2 This is almost identical to the complexity of the bigram clustering algorithm given in section , except that E is now the number of (M+1 grams one wishes to cluster, rather than the number of unigrams (e words of the vocabulary  If one wants to use the clustering algorithm on large corpora, the complexity of the algorithm becomes a crucial issue We therefore developed the following heuristic to speed up the algorithm The factor C[2] comes from the fact that one tries to move a word w to each of the C possible clusters (O(C and for each of these one has to calculate the difference in the optimisation function (O(C) again  If, based on some heuristic, one could select a fixed number t of target clusters, then one could only try moving w to these tclusters, rather than to all possible clusters C When such a clustering algorithm is applied to a large training corpus, e When one tries to move a word w, one also constructs a list of the h most frequent clusters that follow w(one can get this for free as part of the factor E in (E+C[2  One then simply calculates the number of clusters that are in both lists and takes this as the heuristic score H(g1  One can thus calculate the heuristic score of all C clusters in O(C  However, once one has decided to move w to a given cluster, one would have to update the lists containing the h most frequent clusters following each cluster g1(the lists might have changed due to the last moving of a word  In order to avoid this, one can make another approximation at this point One can only update the list for the original and the new cluster of w To sum up, we can say that one can select t target clusters using the heuristic in O(C  Following that, one tries moving w to each of these t clusters, which is again O(C  Moreover, several times per iteration (depending on u one updates the list of most frequent clusters which is O(C[2  The heuristic itself is parameterised by h, the number of most frequent clusters one uses to calculate the heuristic score, t, the number of best ranked target clusters one tries to move word w to and u, the number indicating after how many words a full update of the list of most frequent clusters is performed In order to evaluate the heuristic for a given set of parameters, one can simply compare the final value of the approximation function and the resulting perplexity of the heuristic algorithm with that of the full algorithm Table contains a comparison of the results using approximately one million words of training data (from the Wall Street Journal corpus) and values t=10, h=10 and u=1000 One can see that the execution time of the standard algorithm seems indeed quadratic in the number of clusters, whereas that of the heuristic version seems to be linear Moreover, the perplexity of the heuristic version is always within 4% of that of the standard algorithm, a number which does not seem to increase systematically with the number of clusters Furthermore, the speed up of the algorithm seems to be closely related to the number of clusters divided by t In table , one can see that the effect of u on the algorithm is very minor Finally, in table , one can see that the performance of the algorithm decreases with an increase in h If the suitability of a target cluster is determined by a small number of very frequently co-occurring clusters, then increasing h could make the heuristic perform worse, because the effect of the most important clusters is perturbed by a large number of less important clusters (the heuristic only counts the number of clusters in both lists and does not weigh them  Rather than trying to move each word wto all possible clusters, as the algorithm requires initially, one only tries moving w to a fixed number of clusters that have been selected from all possible clusters by a simple heuristic In the following, we will present results of clustered language models on the Wall Street Journal (WSJ) corpus The clustered models were produced with the extended heuristic version of the algorithm One frequently used approach to alleviate this problem is to construct a clustered language model First, one can see that a bigger value of C leads to a higher perplexity Even though the clustered model performs worse than the back-off model on the largest set of data, it outperforms the back-off model in almost all other cases One can see that the clustered trigram outperforms the clustered bigram, at least with sufficient training data But even with only five million words of training data, the clustered trigram is only slightly worse than the clustered bigram, showing again the robustness of the clustered language models From all the results given here, one can see that the clustered language models can still compete with unclustered models, even when a large corpus, such as the Wall Street Journal corpus, is being used Moreover, in the absence of many million words of training data, the clustered model is more robust and clearly outperforms the non-clustered models In those cases, the clustered models seem like a good alternative to back-off models and certainly one that deserves close investigation The main advantage of the clustering models, its robustness in the face of little training data, can also be seen from the results and in these situations, the clustered algorithm is preferable to the standard back-off models Following the same approach as in section , one can estimate the probabilities in equation using the maximum likelihood estimator where g1=G1(vM v1 g2=G2(w) and N(x) denotes the number of times x occurs in the data Given these probability estimates pG(w|vM v1 the likelihood FMLof the training data, e the probability of the training data being generated by our probability estimates pG(w|vM v1 measures how well the training data is represented by the estimates and can be used as optimisation criterion (  Since one is trying to optimise FML with respect to G, one can remove any term that does not depend on G, because it will not influence the optimisation In speech recognition, one is given a sequence of acoustic observations A and one tries to find the word sequence W that is most likely to correspond to A Because N(g1) does not depend on g2 and N(g2) does not depend on g1, one can simplify this again to Taking the logarithm, one obtains the equivalent optimisation criterion F ML is the maximum likelihood optimisation criterion which could be used to find a good classifications G Let Ti denote the data without the pair (wi-M wi-1, wi) and pG,Ti(w|vM v1) the probability estimates based on a given classification G and training corpus Ti Given a particular Ti, the probability of the held-out part (wi-M wi-1, wi) is pG,Ti(wi|wi-M wi-1  end{eqnarray} gt; In the case of the class bi-gram, one can again use the absolute discounting method for smoothing75 is used during clustering end{eqnarray} gt; In order to facilitate future regrouping of terms, one again expresses the counts NTi, NTi(g1) etc end{eqnarray} gt; After dropping pG,Ti(w) and substituting the expressions back into equation , one obtains: One can now substitute equations , and , using the counts of the whole corpus of equations to  the number of pairs that will be unseen when used as held-out part  Taking the logarithm, one obtains the final optimisation criterion F LO  Because it has fewer parameters, it needs less training data Even in these cases, the number of parameters that need to be estimated from training data can be quite large One way to alleviate this problem is to use class based models This way, one can be very confident that an improvement in the optimisation criterion will actually translate to an improvement of performance In the following, the optimisation criterion for a bigram based model (e relative frequencies: where N(x) denotes the number of times x occurs in the training data Given these probability estimates pG(w|v the likelihood FMLof the training data, e the probability of the training data being generated by our probability estimates pG(w|v measures how well the training data is represented by the estimates and can be used as optimisation criterion (  However, the problem with this maximum likelihood criterion is that one first estimates the probabilities pG(w|v) on the training data T and then, given pG(w|v one evaluates the classification G on T It divides the data into N-1 samples as retained part and only one sample as held-out part In other words, the held-out part TH to evaluate a classification G is the entire set of data points; but when we calculate the probability of the i[th] data point, one assumes that the probability distributions pG(w|v) were estimated on all the data expect point i Let Ti denote the data without the pair (wi-1, wi) and pG,Ti(w|v) the probability estimates based on a given classification G and training corpus Ti end{eqnarray} gt; However, in the case of the class bi-gram, one might have to predict unseen events  This leads to the following smoothed estimate Ideally, one would make b depend on the classification, e75 during clustering end{eqnarray} gt; In order to facilitate future regrouping of terms, one can now express the counts NTi, NTi(g1) etc After dropping pG,Ti(w) because it is independent of G, one arrives at One can now substitute equations , and , using the counts of the whole corpus of equations to  the number of pairs that will be unseen when used as held-out part  Taking the logarithm, we obtain the final optimisation criterion F LO Given the maximization criterion F LO, we use the algorithm in Figure to find a good clustering function G Furthermore, since the clustering of one word affects the future clustering of other words, the order in which words are moved is important As suggested in , the words are sorted by the number of times they occur such that the most frequent words, about which one knows the most, are clustered first