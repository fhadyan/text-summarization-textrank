 Let C be the maximal number of clusters for G, let E be the number of elements one tries to cluster (e For each w, one needed to calculate the number of times w occurred with all clusters g2 Approach a which only uses one clustering function G, could produce |G V different clusterings (for each word in V, it can choose one of the |G| clusters  For each w, one needed to calculate the number of times w occurred with all clusters g2 If one wants to use the clustering algorithm on large corpora, the complexity of the algorithm becomes a crucial issue If, based on some heuristic, one could select a fixed number t of target clusters, then one could only try moving w to these tclusters, rather than to all possible clusters C One can thus calculate the heuristic score of all C clusters in O(C  The heuristic itself is parameterised by h, the number of most frequent clusters one uses to calculate the heuristic score, t, the number of best ranked target clusters one tries to move word w to and u, the number indicating after how many words a full update of the list of most frequent clusters is performed One frequently used approach to alleviate this problem is to construct a clustered language model One can see that the clustered trigram outperforms the clustered bigram, at least with sufficient training data Given these probability estimates pG(w|vM v1 the likelihood FMLof the training data, e Taking the logarithm, one obtains the final optimisation criterion F LO  One way to alleviate this problem is to use class based models00002*109  Given these probability estimates pG(w|v the likelihood FMLof the training data, e However, the problem with this maximum likelihood criterion is that one first estimates the probabilities pG(w|v) on the training data T and then, given pG(w|v one evaluates the classification G on T It divides the data into N-1 samples as retained part and only one sample as held-out part75 during clustering