 The NLP tasks where SRs utility could be evaluated are diverse However, we have tested SRs on a WSS task, using the following scheme Previous knowledge used A semantic hierarchy (WordNet) where words are clustered in semantic classes, and semantic classes are organized hierarchically As evaluation measures we used coverage, abstraction ratio, and recall and precision ratios on the WSS task (section  In addition we performed some evaluation by hand comparing the SRs acquired by the different techniques Coverage for the different techniques is shown in table  The labels used for referring to the different techniques are as follows: Assoc p(c|s corresponds to the basic association measure (section Assoc Head-nouns and Assoc All nouns to the techniques introduced in section , Assoc Normalizing to the local normalization (section and finally, log-likelihood, D (relative entropy) and I (mutual information ratio) to the techniques discussed in section  Polysemous words are represented as instances of different classes The abstraction ratio for the different techniques is shown in table  In principle, the higher abstraction ratio, the better the technique succeeds in filtering out incorrect senses (less Abs  The precision and recall ratios on the noun WSS task for the different techniques are shown in table  In principle, the higher the precision and recall ratios the better the technique succeeds in inducing appropriate SRs for the disambiguation task The local normalizing technique using the uniform distribution does not help Output A set of syntactic SRs, (verb-lemma, syntactic-relationship, semantic-class, weight  However, a better informed kind of local weight (section ) should improve the technique significantly Specially the two techniques that exploit a simpler prior distribution, which seem to improve the basic technique In figure we can see the different evaluation measures of the basic technique when varying the threshold The final SRs must be mutually disjoint Both decrease when threshold increases, probably because when the rejecting threshold is low, small classes that fit the data well can be induced, learning over-general or incomplete SRs otherwise In terms of WSS, general classes may be performing better than classes that fit the data better In this paper we have presented some variations affecting the association measure and thresholding on the basic technique for learning SRs from on-line corpora We proposed some evaluation measures for the SRs learning task We can conclude that some of these variations seem to improve the results obtained using the basic technique SRs are weighted according to the statistical evidence found in the corpus Combining the different n-grams by means of smoothing techniques Learning process 3 stages: 1 Creation of the space of candidate classes Ribas reported experimental results obtained from the application of the above technique to learn SRs Most of the induced classes are due to incorrect senses The technique achieves a good coverage It makes the association score prefer incorrect classes and jump on over-generalizations The different techniques are experimentally evaluated in section  Resnik developed a method for automatically extracting class-based SRs from on-line corpora Specifically, the Assoc' takes into account the preference (selection) of syntactic positions for particular classes Local weight could be obtained using p(c|n  In this section we propose the application of other measures apart from Assoc for learning SRs: log-likelihood ratio , relative entropy , mutual information ratio ,  Different association measures use the information provided in the cross-table to different extents Evaluation of the SR learning task would provide grounds to compare different techniques that try to abstract SRs from corpus using WordNet (eg, section  It would also permit measuring the utility of the SRs obtained using WordNet in comparison with other frameworks using other kinds of knowledge SRs are useful for both lexicography and NLP Hopefully, a technique with a higher abstraction ratio learns classes that fit the set of examples better Quantification of coverage It could be measured as the proportion of triples whose correct sense belongs to one of the SRs