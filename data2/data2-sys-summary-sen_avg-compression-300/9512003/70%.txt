 The stack model doesn't predict a function for the IRUs However, according to the cache model, IRUs make information accessible that is not accessible by virtue of hierarchical recency, so that processes of content-based inferences, inference of discourse relations, and interpretation of anaphors can take place with less effort The IRU may function this way since: (1) the IRU reinstantiates the necessary information in the cache; or (2) the IRU is a retrieval cue for retrieval of information to the cache In the stack model, focus spaces for segments that have been closed are popped from the stack and entities in those focus spaces are not accessible In the cache model, popping only occurs via displacement It should also be possible to test whether entities that are in the focus spaces on the stack, according to the stack model, are more accessible than entities that have been popped off the stack In the cache model, the entities in these focus spaces would not have a privileged attentional status, unless of course they had been refreshed in the cache by being realized implicitly or explicitly in the intervening discussion Finally, consider one of the most studied predictions of the stack model: cases where a pronoun has an antecedent in a prior focus space In the stack model, any of the focus spaces on the stack can be returned to, and the antecedent for a pronoun can be in any of these focus spaces As a potential alternative to the stack model, the cache model appears to be unable to handle return pops since a previous state of the cache can't be popped to Thus, return pops are not problematic for the cache model 88,89 Thus in 17 cases, an adequate retrieval cue is constructed from processing the pronoun and the matrix verb  The occurrence of IRUs as in dialogue C is one way of doing this it was never the discourse center, suggesting that it may never compete with the other cospecifier This squib has discussed the role of limited attention in a computational model of discourse processing The cache model was proposed as a computational implemention of human working memory and operations on attentional state are formulated as operations on a cache Just as a cache can be used for processing the references and operations of a hierarchically structured program, so can a cache be used to model attentional state when discourse intentions are hierarchically structured A new focus space is pushed on the stack during the processing of dialogue A when the intention of utterance 5 is recognized The notion of processing effort for retrieval operations on main memory makes predictions that can be experimentally tested So, by the stack model, this segment is handled by the same focus stack popping mechanism as we saw for dialogue A IRUs realize propositions already established as mutually believed in the discourse only a limited number of entities in the discourse model are potential cospecifiers for a pronoun Thus, these types of IRUs show that hierarchical recency, as realized by the stack model, does not predict when information is accessible The evidence above suggests the need for a model of attentional state in discourse that reflects the limited attentional capacity of human processing Here, I propose an alternate model to the stack model, which I will call the CACHE MODEL, and discuss the evidence for this model In section , I compare a number of dimensions of the cache and stack models The notion of a cache in combination with main memory, as is standard in computational architectures, is a good basis for a computational model of human attentional capacity in processing discourse The CACHE represents working memory and MAIN MEMORY represents long-term memory The cache is a limited capacity, almost instantaneously accessible, memory store Main memory is larger than the cache, but is slower to access ,  There are three operations involving the cache and main memory Items in the cache can be preferentially RETAINED and items in main memory can be RETRIEVED to the cache Items in the cache can also be STORED to main memory When new items are retrieved from main memory to the cache, or enter the cache directly due to events in the world, other items may be DISPLACED, because the cache has limited capacity Displaced items are stored in main memory The determination of which items to displace is handled by a cache replacement policy The cache model includes specific assumptions about processing Discourse processes execute on elements that are in the cache The cospecifier of an anaphor must be in the cache for automatic interpretation, or be strategically retrieved to the cache in order to interpret the anaphor  The cache model maintains the distinction between intentional structure and attentional state first proposed by Grosz and Sidner (1986  Just as a cache can be used for processing the references and operations of a hierarchically structured program, so can a cache be used to model attentional state when discourse intentions are hierarchically structured When conversants start working towards the achievement of a new intention, that intention may utilize information that was already in the cache If so, that information will be preferentially retained in the cache because it is being used Whenever the new intention requires information that is not currently in the cache, that information must be retrieved from main memory When conversants return to a prior intention, information relevant to that intention must be retrieved from main memory if it has not been retained in the cache When an intention is completed, it is not necessary to strategically retain information relevant to the completed segment in the cache However, automatic retrieval processes can be triggered by associations between information being currently discussed and information stored in main memory  EXPECTATIONS about what will be discussed also determine operations on the cache In this section, I wish to examine evidence for the cache model, look at further predictions of the model,and then discuss evidence relevant to both stack and cache models in order to draw direct comparisons between them New intention subordinate to current intention: (1) Stack pushes new focus space; (2) Cache retrieves entities related to new intention Intention completed: (1) Stack pops focus space for intention from stack, entities in focus space are no longer accessible; (2) Cache doesn't retain entities for completed intention, but they remain accessible until displaced New intention subordinate to prior intention: (1) Stack pops focus spaces for intervening segments, focus space for prior intention accessible after pop; (2) Cache retrieves entities related to prior intention from main memory to cache, unless retained in the cache Informationally redundant utterances: (1) Stack predicts no role for IRUs when they are represented in focus space on top of stack, because information should be immediately available; (2) Cache predicts that IRUs reinstantiate or refresh known information in the cache Returning from interruption: (1) In the stack model, the length and depth of the interruption and the processing required is irrelevant; (2) In the cache model, the length of the interruption or the processing required predicts retrievals from main memory First, consider the differences in the treatment of interruptions In the cache model, an interruption may give rise to an expectation of a return to a prior intention, and each participant may attempt to retain information relevant to pursuing that intention in their cache However, it may not be possible to retain the relevant material in the cache This would mean that the processing of incoming information would be slower until all of the required information is in the cache