 Rules were then automatically learned that updated these chunk structure tags based on neighboring words and their part-of-speech and chunk tags The cross product of the 20 word and part-of-speech patterns with the 5 chunk tag patterns determined the full set of 100 templates used Applying transformation-based learning to text chunking turns out to be different in interesting ways from its use for part-of-speech tagging (The test set in all cases was 50K words Training runs were halted after the first 500 rules; rules learned after that point affect relatively few locations in the training set and have only a very slight effect for good or ill on test set performance The first line in each table gives the performance of the baseline system, which assigned a baseNP or chunk tag to each word on the basis of the POS tag assigned in the prepass The raw percentage of correct chunk tags is also given for each run, and for each performance measure, the relative error reduction compared to the baseline is listed The partitioning chunks do appear to be somewhat harder to predict than baseNP chunks In the first of the baseNP rules, adjectives (with part-of-speech tag JJ) that are currently tagged I but that are followed by words tagged O have their tags changed to O (Since the tag B is only used when baseNPs abut, the baseline system tags determiners as I Rule 3 takes words which immediately follow determiners tagged I that in turn follow something tagged O and changes their tag to also be I Rules 4-6 are similar to Rule 2, marking the initial words of baseNPs that directly follow another baseNP A similar list of the first ten rules for the chunk task can be seen in Table  To gloss a few of these, in the first rule here, determiners (with part-of-speech tag DT which usually begin N chunks and thus are assigned the baseline tag BN, have their chunk tags changed to N if they follow a word whose tag is also BN The fact that this system includes lexical rule templates that refer to actual words sets it apart from approaches that rely only on part-of-speech tags to predict chunk structure Thus lexical rules appear to be making a limited contribution in determining baseNP chunks, but a more significant one for the partitioning chunks The most frequent single confusion involved words tagged VBG and VBN, whose baseline prediction given their part-of-speech tag was O, but which also occur frequently inside baseNPs The Treebank tags the words and and frequently with the part-of-speech tag CC, which the baseline system again predicted would fall most often outside of a baseNP One such direction is to expand the template set by adding templates that are sensitive to the chunk structure For example, instead of referring to the word two to the left, a rule pattern could refer to the first word in the current chunk, or the last word of the previous chunk Another direction would be to enrich the vocabulary of chunk tags, so that they could be used during the learning process to encode contextual features for use by later rules in the sequence Voutilainen , in his impressive NPtool system, uses an approach that is in some ways similar to the one used here, in that he adds to his part-of-speech tags a new kind of tag that shows chunk structure; the chunk tag N for example, is used for determiners and premodifiers, both of which group with the following noun head He uses a lexicon that lists all the possible chunk tags for each word combined with hand-built constraint grammar patterns In this paper, we target a somewhat higher level of chunk structure using Brill's transformation-based learning mechanism, in which a sequence of transformational rules is learned from a corpus; this sequence iteratively improves upon a baseline model for some interpretive feature of the text We performed experiments using two different chunk structure targets, one that tried to bracket non-recursive baseNPs and one that partitioned sentences into non-overlapping N-type and V-type chunks, loosely following Abney's model Training and test materials with chunk tags encoding each of these kinds of structure were derived automatically from the parsed Wall Street Journal text in the Penn Treebank  The chunks in the partitioning chunk experiments were somewhat closer to Abney's model, where the prepositions in prepositional phrases are included with the object NP up to the head in a single N-type chunk Again, the possessive marker was viewed as initiating a new N-type chunk These two kinds of chunk structure derived from the Treebank data were encoded as chunk tags attached to each word and provided the targets for the transformation-based learning One of those rules whose net score (positive changes minus negative changes) is maximal is then selected, applied to the corpus, and also written out as the first rule in the learned sequence This process is iterated, leading to an ordered sequence of rules, with rules discovered first ordered before those discovered later We also note some related adaptations in the procedure for learning rules that improve its performance, taking advantage of ways in which this task differs from the learning of part-of-speech tags Applying transformational learning to text chunking requires that the system's current hypotheses about chunk structure be represented in a way that can be matched against the pattern parts of rules In this work, we have found it convenient to do so by encoding the chunking using an additional set of tags, so that each word carries both a part-of-speech tag and also a chunk tag from which the chunk structure can be derived In the baseNP experiments aimed at non-recursive NP structures, we use the chunk tag set {I, O, B where words marked I are inside some baseNP, those marked O are outside, and the B tag is used to mark the left most item of a baseNP which immediately follows another baseNP In this study, training and test sets marked with two different types of chunk structure were derived algorithmically from the parsed data in the Penn Treebank corpus of Wall Street Journal text  Punctuation marks, which are ignored in Abney's chunk grammar, but which the Treebank data treats as normal lexical items with their own part-of-speech tags, are unambiguously assigned the chunk tag P Encoding chunk structure with tags attached to words rather than non-recursive bracket markers inserted between words has the advantage that it limits the dependence between different elements of the encoded representation While brackets must be correctly paired in order to derive a chunk structure, it is easy to define a mapping that can produce a valid chunk structure from any sequence of chunk tags; the few hard cases that arise can be handled completely locally For example, in the baseNP tag set, whenever a B tag immediately follows an O, it must be treated as an I, and, in the partitioning chunk tag set, wherever a V tag immediately follows an N tag without any intervening BV, it must be treated as a BV Transformational learning begins with some initial baseline prediction, which here means a baseline assignment of chunk tags to words Reasonable suggestions for baseline heuristics after a text has been tagged for part-of-speech might include assigning to each word the chunk tag that it carried most frequently in the training set, or assigning each part-of-speech tag the chunk tag that was most frequently associated with that part-of-speech tag in the training The part-of-speech tags used by this baseline heuristic, and then later also matched against by transformational rule patterns, were derived by running the raw texts in a prepass through Brill's transformational part-of-speech tagger  The source texts were then run through Brill's part-of-speech tagger , and, as a baseline heuristic, chunk structure tags were assigned to each word based on its part-of-speech tag In transformational learning, the space of candidate rules to be searched is defined by a set of rule templates that each specify a small number of particular feature sets as the relevant factors that a rule's left-hand-side pattern should examine, for example, the part-of-speech tag of the word two to the left combined with the actual word one to the left In the preliminary scan of the corpus for each learning pass, it is these templates that are applied to each location whose current tag is not correct, generating a candidate rule that would apply at least at that one location, matching those factors and correcting the chunk tag assignment When this approach is applied to part-of-speech tagging, the possible sources of evidence for templates involve the identities of words within a neighborhood of some appropriate size and their current part-of-speech tag assignments In the text chunking application, the tags being assigned are chunk structure tags, while the part-of-speech tags are a fixed part of the environment, like the lexical identities of the words themselves The distributed version of Brill's tagger makes use of 26 templates, involving various mixes of word and part-of-speech tests on neighboring words Our tests were performed using 100 templates; these included almost all of Brill's combinations, and extended them to include references to chunk tags as well as to words and part-of-speech tags The same 10 patterns can also be used to match against part-of-speech tags, encoded as P0, P-1, etc (In other tests, we have explored mixed templates, that match against both word and part-of-speech values, but no mixed templates were used in these experiments These 20 word and part-of-speech patterns were then combined with each of the 5 different chunk tag patterns shown on the right side of the table