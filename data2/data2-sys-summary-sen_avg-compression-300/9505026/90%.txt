 There are several ways to handle unknown words The results can be slightly improved by trying only open-class tags for unknown words This constitutes an a priori distribution for unknown words, reflecting for example that most of the unknown words are nouns The probabilities could be obtained from a separate training part, or from the distribution of words that occur only once in the training corpus These words reflect the distribution of unknown words according to the formula presented in  First, we used the original tagset, consisting of 258 tags Otherwise we used the distribution of tags for words that occurred only once in the training corpus As opposed to trigram tagging, lexical tagging ignores context probabilities and is based solely on lexical probabilities Each word is assigned its most frequent tag from the training corpus Unknown words were assigned the most frequent tag of words that occurred exactly once in the training corpus The most frequent tags for single occurrence words are for the Teleman corpus NNSS (indefinite noun-noun compound) and noun (large and small tagset, resp for the Susanne corpus NN2 (plural common noun) and NN (common noun; again large and small tagset resp  The major difference (apart from corpus size and tagsets used) is the percentage of words that occur exactly once: 10% for Teleman vs38 tags for the small tagset, and 361 for the Susanne corpus, despite the fact that the tagsets for the Susanne corpus are larger than those for the Teleman corpus2% of the words in the Teleman corpus are ambiguous, and only 44 Tags in the Susanne corpus with indices are counted as separate tags Unknown words are words that occur only in the test set, but not in the training set The remaining 9,823 words of the Susanne corpus were not used in the experiments This results in poor estimates for the probabilities of new sequences of words Section describes the HMM-based tagger and Section the reductionistic statistical one Each rule applies to a current word with a set of candidate tags The tagger is reductionistic since it repeatedly removes low-probability candidate tags Coping with unknown words, i It comprises 85,408 words (tokens; here, words is a collective denotation of proper words, numbers, and punctuation 