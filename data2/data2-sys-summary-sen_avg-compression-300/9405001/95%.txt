 The similarity model was correct in 64 cases, and the back-off model in 32 In cooccurrence smoothing, as in our method, a baseline model is combined with a similarity-based model that refines some of its probability estimates suggest a class-based n-gram model in which words with similar cooccurrence distributions are clustered in word classes A more substantial variation would be to base the model on similarity between conditioned words rather than on similarity between conditioning words Cooccurrence probabilities of words are then modeled by averaged cooccurrence probabilities of word clusters Their similarity-based model avoids clustering altogether Testing on a held-out sample, the similarity model achieved a 20% reduction in perplexity for unseen bigrams Thus our application of the similarity model averages together standard back-off estimates for a set of similar conditioning words As decreases, remote words get a larger effect In commonly used models, the probability estimate for a previously unseen cooccurrence is a function of the probability estimates for the words in the cooccurrence The bigram similarity model was also tested as a language model in speech recognition