 The similarity model was correct in 64 cases, and the back-off model in 3201 level In cooccurrence smoothing, as in our method, a baseline model is combined with a similarity-based model that refines some of its probability estimates Given a baseline probability model P, which is taken to be the MLE, the confusion probability PC(w'1|w1) between conditioning words w'1 and w1 is defined as the probability that w1 is followed by the same context words as w'1 suggest a class-based n-gram model in which words with similar cooccurrence distributions are clustered in word classes Finally, while we have used our similarity model only for missing bigrams in a back-off scheme, used linear interpolation for all bigrams to combine the cooccurrence smoothing model with MLE models of bigrams and unigrams Notice, however, that the choice of back-off or interpolation is independent from the similarity model used A more substantial variation would be to base the model on similarity between conditioned words rather than on similarity between conditioning words The cooccurrence probability of a given pair of words then is estimated according to an averaged cooccurrence probability of the two corresponding classes Finally, the similarity-based model may be applied to configurations other than bigrams Cooccurrence probabilities of words are then modeled by averaged cooccurrence probabilities of word clusters Their similarity-based model avoids clustering altogether We first allocate an appropriate probability mass for unseen cooccurrences following the back-off method We applied our method to estimate unseen bigram probabilities for Wall Street Journal text and compared it to the standard back-off model Testing on a held-out sample, the similarity model achieved a 20% reduction in perplexity for unseen bigrams Thus our application of the similarity model averages together standard back-off estimates for a set of similar conditioning words In commonly used models, the probability estimate for a previously unseen cooccurrence is a function of the probability estimates for the words in the cooccurrence The interpolated model ( ) is used in the back-off scheme as Pr(w2|w1 to obtain better estimates for unseen bigrams4 to 231 The bigram similarity model was also tested as a language model in speech recognition