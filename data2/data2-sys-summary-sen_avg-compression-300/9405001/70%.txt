 The similarity model was correct in 64 cases, and the back-off model in 32 This advantage for the similarity model is statistically significant at the 001 level The hypotheses are labeled `B' for back-off and `S' for similarity, and the bold-face words are errors The cooccurrence smoothing technique , based on earlier stochastic speech modeling work by is the main previous attempt to use similarity to estimate the probability of unseen events in language modeling In those models, the relationship between given words is modeled by analogy with other words that are in some sense similar to the given ones In cooccurrence smoothing, as in our method, a baseline model is combined with a similarity-based model that refines some of its probability estimates The similarity model in cooccurrence smoothing is based on the intuition that the similarity between two words wand w' can be measured by the confusion probability PC(w w) that w' can be substituted for w in an arbitrary context in the training corpus Given a baseline probability model P, which is taken to be the MLE, the confusion probability PC(w'1|w1) between conditioning words w'1 and w1 is defined as the probability that w1 is followed by the same context words as w'1 In addition, we restrict the summation to sufficiently similar words, whereas the cooccurrence smoothing method sums over all words in the lexicon suggest a class-based n-gram model in which words with similar cooccurrence distributions are clustered in word classes These properties motivated our choice of relative entropy for similarity measure, because of the intuition that words with sharper distributions are more informative about other words than words with flat distributions Finally, while we have used our similarity model only for missing bigrams in a back-off scheme, used linear interpolation for all bigrams to combine the cooccurrence smoothing model with MLE models of bigrams and unigrams Notice, however, that the choice of back-off or interpolation is independent from the similarity model used A more substantial variation would be to base the model on similarity between conditioned words rather than on similarity between conditioning words The cooccurrence probability of a given pair of words then is estimated according to an averaged cooccurrence probability of the two corresponding classes Finally, the similarity-based model may be applied to configurations other than bigrams For trigrams, it is necessary to measure similarity between different conditioning bigrams If the configuration in question includes only two words, such as P(object|verb then it is possible to use the model we have used for bigrams In this paper we presented a new model that implements the similarity-based approach to provide estimates for the conditional probabilities of unseen word cooccurrences Our method combines similarity-based estimates with Katz's back-off scheme, which is widely used for language modeling in speech recognition Cooccurrence probabilities of words are then modeled by averaged cooccurrence probabilities of word clusters Their similarity-based model avoids clustering altogether Their model, however, is not probabilistic, that is, it does not provide a probability estimate for unobserved cooccurrences Similarity-based estimation was first used for language modeling in the cooccurrence smoothing method of , derived from work on acoustic model smoothing by  We first allocate an appropriate probability mass for unseen cooccurrences following the back-off method Then we redistribute that mass to unseen cooccurrences according to an averaged cooccurrence distribution of a set of most similar conditioning words, using relative entropy as our similarity measure This second step replaces the use of the independence assumption in the original back-off model We applied our method to estimate unseen bigram probabilities for Wall Street Journal text and compared it to the standard back-off model Testing on a held-out sample, the similarity model achieved a 20% reduction in perplexity for unseen bigrams However, this estimates the probability of any unseen bigram to be zero, which is clearly undesirable In addition, the back-off model does not require complex estimations for interpolation parameters A back-off model requires methods for (a) discounting the estimates of previously observed events to leave out some positive probability mass for unseen events, and (b) redistributing among the unseen events the probability mass freed by discounting For bigrams the resulting estimator has the general form where Pd represents the discounted estimate for seen bigrams, Pr the model for probability redistribution among the unseen bigrams, and is a normalization factor He then uses the discounted frequency in the conditional probability calculation for a bigram: In the original Good-Turing method the free probability mass is redistributed uniformly among all unseen events Instead, Katz's back-off scheme redistributes the free probability mass non-uniformly in proportion to the frequency of w2, by setting Katz thus assumes that for a given conditioning word w1 the probability of an unseen following word w2 is proportional to its unconditional probability Our scheme is based on the assumption that words that are similar to w1 can provide good predictions for the distribution of w1 in unseen bigrams Let denote a set of words which are most similar to w1, as determined by some similarity metric We use the estimates given by the standard back-off model, which satisfy that requirement Thus our application of the similarity model averages together standard back-off estimates for a set of similar conditioning words As decreases, remote words get a larger effect In commonly used models, the probability estimate for a previously unseen cooccurrence is a function of the probability estimates for the words in the cooccurrence The interpolated model ( ) is used in the back-off scheme as Pr(w2|w1 to obtain better estimates for unseen bigrams The baseline back-off model follows closely the Katz design, except that for compactness all frequency one bigrams are ignored5 million words of WSJ text from the years 1987-89 For perplexity evaluation, we tuned the similarity model parameters by minimizing perplexity on an additional sample of 575 thousand words of WSJ text, drawn from the ARPA HLT development test set This improvement on unseen bigrams corresponds to an overall test set perplexity improvement of 24 to 231 From equation ( it is clear that the computational cost of applying the similarity model to an unseen bigram is O(k  The bigram similarity model was also tested as a language model in speech recognition From the given lattices, we constructed new lattices in which the arc scores were modified to use the similarity model instead of the baseline model