 The similarity model was correct in 64 cases, and the back-off model in 32 This advantage for the similarity model is statistically significant at the 0 The cooccurrence smoothing technique , based on earlier stochastic speech modeling work by is the main previous attempt to use similarity to estimate the probability of unseen events in language modeling In cooccurrence smoothing, as in our method, a baseline model is combined with a similarity-based model that refines some of its probability estimates Given a baseline probability model P, which is taken to be the MLE, the confusion probability PC(w'1|w1) between conditioning words w'1 and w1 is defined as the probability that w1 is followed by the same context words as w'1 In addition, we restrict the summation to sufficiently similar words, whereas the cooccurrence smoothing method sums over all words in the lexicon suggest a class-based n-gram model in which words with similar cooccurrence distributions are clustered in word classes These properties motivated our choice of relative entropy for similarity measure, because of the intuition that words with sharper distributions are more informative about other words than words with flat distributions Finally, while we have used our similarity model only for missing bigrams in a back-off scheme, used linear interpolation for all bigrams to combine the cooccurrence smoothing model with MLE models of bigrams and unigrams Notice, however, that the choice of back-off or interpolation is independent from the similarity model used A more substantial variation would be to base the model on similarity between conditioned words rather than on similarity between conditioning words The cooccurrence probability of a given pair of words then is estimated according to an averaged cooccurrence probability of the two corresponding classes Finally, the similarity-based model may be applied to configurations other than bigrams For trigrams, it is necessary to measure similarity between different conditioning bigrams In this paper we presented a new model that implements the similarity-based approach to provide estimates for the conditional probabilities of unseen word cooccurrences Cooccurrence probabilities of words are then modeled by averaged cooccurrence probabilities of word clusters Their similarity-based model avoids clustering altogether Their model, however, is not probabilistic, that is, it does not provide a probability estimate for unobserved cooccurrences We first allocate an appropriate probability mass for unseen cooccurrences following the back-off method Then we redistribute that mass to unseen cooccurrences according to an averaged cooccurrence distribution of a set of most similar conditioning words, using relative entropy as our similarity measure We applied our method to estimate unseen bigram probabilities for Wall Street Journal text and compared it to the standard back-off model Testing on a held-out sample, the similarity model achieved a 20% reduction in perplexity for unseen bigrams A back-off model requires methods for (a) discounting the estimates of previously observed events to leave out some positive probability mass for unseen events, and (b) redistributing among the unseen events the probability mass freed by discounting For bigrams the resulting estimator has the general form where Pd represents the discounted estimate for seen bigrams, Pr the model for probability redistribution among the unseen bigrams, and is a normalization factor Instead, Katz's back-off scheme redistributes the free probability mass non-uniformly in proportion to the frequency of w2, by setting Katz thus assumes that for a given conditioning word w1 the probability of an unseen following word w2 is proportional to its unconditional probability We use the estimates given by the standard back-off model, which satisfy that requirement Thus our application of the similarity model averages together standard back-off estimates for a set of similar conditioning words As decreases, remote words get a larger effect In commonly used models, the probability estimate for a previously unseen cooccurrence is a function of the probability estimates for the words in the cooccurrence The interpolated model ( ) is used in the back-off scheme as Pr(w2|w1 to obtain better estimates for unseen bigrams The baseline back-off model follows closely the Katz design, except that for compactness all frequency one bigrams are ignored The bigram similarity model was also tested as a language model in speech recognition From the given lattices, we constructed new lattices in which the arc scores were modified to use the similarity model instead of the baseline model