 Language models used in the context of speech recognition are normally some variety of finite-state grammar A bigram language model is used By construction, the specialized grammar has strictly less coverage on the domain than the original one Preference functions are of three types Firstly, there is a speech function which simply returns the acoustic score for the sentence hypothesis that gave rise to the QLF (or a default low score if the hypothesis was suggested by the repair algorithm  The scaling factors used to derive a single summed score for a QLF from the scores returned for that QLF by the various preference functions are also trained automatically in order to maximize of the chances of the highest-scoring QLF being correct Scaling factor training has two phases The first phase makes use of a measure of the similarity between each QLF for a sentence and the correct QLF (selected in advance by interaction with a developer) for that sentence The preference functions used were: The speech function, returning the recognizer score Two combining functions: one for grammar rules used in the best QLF for the string, and one for the semantic triples for that QLF In Section 44 above we gave performance details for speech and language analysis Sentence recognition accuracy using optimized speech (DECIPHER) and language (CLE and N-gram) information on unseen ATIS data is 73 by syntactic, semantic and preference processing The speech recognizer used is a fast version of SRI's DECIPHER [TM] speaker-independent continuous speech recognition system (Murveit et al, 1991  On the main training corpus of 4615 reference sentences used during the project, the repair mechanism suggested corrections for 135 sentences Correct decisions are shown in bold type