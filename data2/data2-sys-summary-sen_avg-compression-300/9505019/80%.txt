 We arrive at a surprising conclusion that a set of idiomatic expressions with complicated meanings and a trivial construction about time can have the same semantic complexity (From the perspective of answering yes/no questions  Let U be a finite set of tokens Then, the size of MU is the measure of what is -complexity of U Thus, the semantic what is -complexity of S is greater than the what is -complexity of T The set Q consists of questions about parameters of calendar events: event_time event_name alarm_on event_topic event_participants  Similarly, we can compute the semantic complexity of ELIZA as Q-complexity What would happen if one would like to play the game of asking what is questions with a machine How complex such a machine would have to be? Again, using the results of and about the roughly 10:1 ratio of the number of words to the number of facts necessary to understand sentences with them, we get that for the set T we need about 20 facts for two rounds of questions However for S we would need about 250 for two rounds of questions And these numbers are closer to our intuitive understanding of the semantic complexity of the two sets We have shown how these two kinds of complexity measures distinguish between the two sets of sentences with at  In other words, the real complexity of discussing a narrative is at least the complexity of iterated-what-is combined with iterated-why (and might as well include alternative questions  yes/no and what-is complexities  What is -complexity: A natural language understanding system has to deal with a set of basic objects It can be argued that what is -complexity is a reasonable measure of how complex is the set of those basic objects Namely, what is -complexity and twice-iterated-what-is -complexity measures the size of the database of background knowledge facts Intuitively, this is a reasonable measure of their semantic complexity Complexity of grammatical constructions: In many cases the complexity of a new construction is not much greater than the complexity of the subconstructions they are built from Thus, roughly, the complexity of the grammar can be estimated by the number of grammatical constructions, defaults and filters The number of semantic/ontological categories is small The second advantage of a limited domain lies in the relatively small number of semantic categories One can of course argue about the adequacy of either model, but the fact remains that for simple tasks dialog complexity is limited by a small number of basic states We have defined semantic complexity by connecting the concept of Kolmogorov complexity with the types of questions that can apply to a sentence (a string  an abstract machine for answering questions of interest We have analyzed semantic complexities of simple examples involving prepositional phrases and of larger NLU programs We have introduced a new concept of meaning of a string, identifying it with the set of values for a fixed set of questions For instance: (1) How useful is the new concept of meaning? What about compositional semantics? Notice that the appeal of compositionality at least partly lies in reducing the complexity of the meaning automaton at a price of high what-is -complexity (i the complex semantic descriptions of words) we get a very simple automaton whose only move is functional application (2) Can we estimate semantic complexities by statistical means? This is possible for some cases of what-is -complexity, e (3) Can we express semantic complexity of a NLU task as a function of the complexity of an automaton partially solving the task and the description (or a corpus) of the whole task For instance, for what is -complexity such a result is trivially holds: the size of the corpus determines the size of the explanation table But when? (5) If we measure the semantic complexity by the number of pairs in the functions, the yes-no complexities of the two sets T and S were the same and equal to 24[2 similarly if we use Turing machines with the complexity of 25, not 30) it behaves almost like the yes-no machine of Section 3 Acknowledgments Kanevsky for our discussions of semantic complexity, and W Secondly, they have a similar number of background knowledge facts For simplicity, we will consider the two constructions simply as sets of sentences One of the tools for measuring complexity widely used in theoretical computer science is Kolmogorov complexity Kolmogorov complexity of a string x is defined as as the size of the shortest string y from which a certain universal Turing machine produces x We could define semantic complexity of a set of sentences S as its Kolmogorov complexity, i It is also problematic because the function K assigning its Kolmogorov complexity to a string is not computable In this paper we think of M as a Turing machine that computes the semantics of the sentences in S, and measure its size by the number of states Let Q be a set of questions We can measure the semantic complexity of a set of sentences by the size of the smallest model that answers all relevant questions about those sentences (in practice, the simplest we are able to construct  A simple classification of questions given by (pp We now want to examine a few measures of semantic complexity: yes/no-complexity, and what is -complexity We also analyze the complexity of ELIZA as Q-complexity, and argue that defining semantic complexity of NL interfaces as Q-complexity makes sense If we measure the semantic complexity by the number of pairs in the functions, the yes-no complexities of both sets are the same and equal 24[2  If we measure it by the number of states of their respective Turing machines, because the two problems are isomorphic, their yes-no complexity will again be identical