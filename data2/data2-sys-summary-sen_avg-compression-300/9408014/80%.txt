 In recent years there has been a resurgence of interest in statistical approaches to natural language processing This is followed by discussion of the logic-based model in section , the overall quantitative model in section , monolingual models in section , translation models in section , and some conclusions in section  For the qualitative model, the operable notion of correspondence is based on logical equivalence and the constants are source word sense predicates and target sense predicates  A' is a set of assumptions that includes the assumptions A which supported  A typical bilingual postulate for translating between p1 and q1 might be of the form:  This can be alleviated by placing restrictions on the form of meaning postulates and input formulas and using heuristic search methods We concentrate throughout on what information about language and translation is coded and how it is expressed as logical constraints or statistical parameters Monolingual models that can be used for both analysis and generation Hierarchical phrases capturing recursive linguistic structure Hudson 1984  In our case, the dependency representation is monostratal in that the relations may include ones normally classified as belonging to syntax, semantics or pragmatics This lexical anchoring facilitates statistical training and sensitivity to lexical variation and collocations The model associates phrases with relation graphs A relation graph is a directed labeled graph consisting of a set of relation edges Ignoring algorithmic issues relating to compactly representing and efficiently searching the space of alternative hypotheses, the overall design of the quantitative system is as follows The speech recognizer produces a set of word-position hypotheses (perhaps in the form of a word lattice) corresponding to a set of string hypotheses for the input The source language model is used to compute a set of possible relation graphs, with associated probabilities, for each string hypothesis These target graphs include all the words of possible translations of the utterance hypotheses but do not specify the surface order of these words This word sequence can then be handed to the speech synthesizer The probabilities associated with phrases in the above description are computed according to the statistical models for analysis, translation, and generation We are not considering training issues in this paper, though a number of now familiar techniques ranging from methods for maximum likelihood estimation to direct estimation using fully annotated data are applicable We now apply some simplifying independence assumptions concerning relation graphs Since As is given, 1/P(As) is a constant which can be ignored in finding the maximum of P(Wt | As  Such scores are normally computed by speech recognition systems, although they are usually also multiplied by word-based language model probabilities P(Ws) which we do not require in this application context In some respects this is similar to Dagan and Itai's (1994) approach to word sense disambiguation using statistical associations in a second language Chang, Luo, and Su 1992; Hindle and Rooth 1993  The language model factors the statistical derivation of a sentence with word string W as follows: where C ranges over relation graphs The content model, P(C and generation model, P(W | C are components of the overall statistical model for spoken language translation given earlier language production in context exactly one for each relation  The set of relation edges for the entire derivation is the union of these local edge sets We now return to the generation model P(W | C  Most language processing labeled as statistical involves associating real-number valued parameters to configurations of symbols One possibility is to use `bi-relation' parameters for the probability that an ri-dependent immediately follows an rj-dependent We let the identity relation e stand for the head itself We can thus use these sequencing parameters directly in our overall model However, we may also want a model for P(W for example for pruning speech recognition hypotheses Combining our content and ordering models we get: P(W) = _C P(C) P(W | C) = _C P(Top(h_C _h W P(s_WCh|h) _r(h,w) E_C(h) P(r(h,w h,r) The parameters P(s|h) can be derived by combining sequencing parameters with the detail parameters for h As already mentioned, the translation model defines mappings between relation graphs Cs for the source language and Ct for the target language Thus nominals and their modifiers pick out entities in a (real or imaginary) world, verbs and their modifiers refer to actions or events in which the entities participate in roles indicated by the edge relations We call this approximating referential equivalence (1990) in their surface translation model The translation probability is then the sum of probabilities over different alignments f:  There are different ways to model P(Ct,f|Cs) corresponding to different kinds of alignment relations and different independence assumptions about the translation mapping The inverse relation f 1] need not be a function, allowing different numbers of words in the source and target sentences That is, the probability that f maps exactly the (possibly empty) subset of Nt to wi These subgraphs give rise to disjoint sets of relation edges which together form Et For simplicity, we assume here that the source graph Cs is a tree For the simple model it remains to specify derivation step probabilities This last condition ensures that the target graph partitions join up in a way that is compatible with the node alignment f The factoring of the translation model into these lexical and structural components means that it will overgenerate because these aspects are not independent in translation between real natural languages This can be done in a parallel fashion to the forward direction described above 1985) in the qualitative model Computationally, the quantitative model lets us escape from the undecidability of logic-based reasoning In particular, we preserved the notion of hierarchical phrase structure The quantitative model also reduced overall complexity in terms of the sets of symbols used In addition to words, it only required symbols for dependency relations, whereas the qualitative model required symbol sets for linguistic categories and features, and a set of word sense symbols Despite their apparent importance to translation, the quantitative system can avoid the use of word sense symbols (and the problems of granularity they give rise to) by exploiting statistical associations between words in the target language to filter implicit sense choices A speech translation system, which by necessity combines speech and language technology, is a natural place to consider combining the statistical and conventional approaches and much of this paper describes probabilistic models of structural language analysis and translation 1992  For translation, a very direct approach using parameters based on surface positions of words in source and target sentences was adopted in the Candide system (Brown et al However, this does not capture important structural properties of natural language Nor does it take into account generalizations about translation that are independent of the exact word order in source and target sentences 1992  The aim of the quantitative language and translation models presented in sections and is to employ probabilistic parameters that reflect linguistic structure without discarding rich lexical information or making the models too complex to train automatically We now consider a hypothetical speech translation system in which the language processing components follow a conventional qualitative transfer design The translation relation is expressed by a set of first order axioms which are used by a theorem prover to derive a target language logical form that is equivalent (in some context) to the source logical form A grammar for the target language is then applied to the target form, generating a syntax tree whose fringe is passed to a speech synthesizer The relation form is many-to-many, associating a string with linguistically possible logical form interpretations cm(w  There is also the problem of requiring increasingly complex feature sets to describe idiosyncrasies in the lexicon We can state this as follows:  We are thus forced to under-filter and make an arbitrary choice between remaining alternatives In both the quantitative and qualitative models we take a transfer approach to translation We do not depend on interlingual symbols, but instead map a representation with constants associated with the source language into a corresponding expression with constants from the target language