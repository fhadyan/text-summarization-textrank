 In recent years there has been a resurgence of interest in statistical approaches to natural language processing Such approaches are not new, witness the statistical approach to machine translation suggested by Weaver (1955 but the current level of interest is largely due to the success of applying hidden Markov models and N-gram language models in speech recognition This is followed by discussion of the logic-based model in section , the overall quantitative model in section , monolingual models in section , translation models in section , and some conclusions in section  For the qualitative model, the operable notion of correspondence is based on logical equivalence and the constants are source word sense predicates and target sense predicates  A' is a set of assumptions that includes the assumptions A which supported  A typical bilingual postulate for translating between p1 and q1 might be of the form:  This can be alleviated by placing restrictions on the form of meaning postulates and input formulas and using heuristic search methods We concentrate throughout on what information about language and translation is coded and how it is expressed as logical constraints or statistical parameters Monolingual models that can be used for both analysis and generation Hierarchical phrases capturing recursive linguistic structure Hudson 1984  Dependency representations have been used in large scale qualitative machine translation systems, notably by McCord (1988  In our case, the dependency representation is monostratal in that the relations may include ones normally classified as belonging to syntax, semantics or pragmatics This lexical anchoring facilitates statistical training and sensitivity to lexical variation and collocations Pereira, Tishby and Lee 1993, Dagan, Marcus and Markovitch 1993) to improve parameter estimation from sparse data The model associates phrases with relation graphs A relation graph is a directed labeled graph consisting of a set of relation edges The nodes wi and wj are word occurrences representable by a word and an index, the indices uniquely identifying particular occurrences of the words in a discourse or corpus The relations in the models for the source and target languages need not be the same, or even overlap Ignoring algorithmic issues relating to compactly representing and efficiently searching the space of alternative hypotheses, the overall design of the quantitative system is as follows The speech recognizer produces a set of word-position hypotheses (perhaps in the form of a word lattice) corresponding to a set of string hypotheses for the input The source language model is used to compute a set of possible relation graphs, with associated probabilities, for each string hypothesis These target graphs include all the words of possible translations of the utterance hypotheses but do not specify the surface order of these words Probabilities for different possible word orderings are computed according to ordering parameters which form part of the target language model This word sequence can then be handed to the speech synthesizer For tighter integration between generation and synthesis, information about the derivation of the target utterance can also be passed to the synthesizer The probabilities associated with phrases in the above description are computed according to the statistical models for analysis, translation, and generation We are not considering training issues in this paper, though a number of now familiar techniques ranging from methods for maximum likelihood estimation to direct estimation using fully annotated data are applicable We now apply some simplifying independence assumptions concerning relation graphs Since As is given, 1/P(As) is a constant which can be ignored in finding the maximum of P(Wt | As  Such scores are normally computed by speech recognition systems, although they are usually also multiplied by word-based language model probabilities P(Ws) which we do not require in this application context This latter formulation allows us to apply constraints imposed by the target language model to filter inappropriate possibilities suggested by analysis and transfer In some respects this is similar to Dagan and Itai's (1994) approach to word sense disambiguation using statistical associations in a second language Chang, Luo, and Su 1992; Hindle and Rooth 1993  The language model factors the statistical derivation of a sentence with word string W as follows: where C ranges over relation graphs The content model, P(C and generation model, P(W | C are components of the overall statistical model for spoken language translation given earlier language production in context This is less important, however, in the translation setting since we produce Ct in the context of a source relation graph Cs and we assume the availability of a model for P(Ct | Cs  exactly one for each relation  There will be a set of edges corresponding to the local tree rooted at h with dependent nodes  The set of relation edges for the entire derivation is the union of these local edge sets Our previous assumption amounted to stating that this was always 1 for n=1 or for n=0 Conversely, it is possible to imagine building a language model in which all probabilities are estimated according to intuition without reference to any real data, giving a probabilistic model that is not empirical there are nr! permutations of the r-dependents of h if these dependents are all distinct  We now return to the generation model P(W | C  This success was directly measurable in terms of word recognition error rates, prompting language processing researchers to seek corresponding improvements in performance and robustness Most language processing labeled as statistical involves associating real-number valued parameters to configurations of symbols One possibility is to use `bi-relation' parameters for the probability that an ri-dependent immediately follows an rj-dependent We let the identity relation e stand for the head itself We can thus use these sequencing parameters directly in our overall model However, we may also want a model for P(W for example for pruning speech recognition hypotheses This is not surprising given that natural language, at least in written form, is explicitly symbolic Combining our content and ordering models we get: P(W) = _C P(C) P(W | C) = _C P(Top(h_C _h W P(s_WCh|h) _r(h,w) E_C(h) P(r(h,w h,r) The parameters P(s|h) can be derived by combining sequencing parameters with the detail parameters for h As already mentioned, the translation model defines mappings between relation graphs Cs for the source language and Ct for the target language Thus nominals and their modifiers pick out entities in a (real or imaginary) world, verbs and their modifiers refer to actions or events in which the entities participate in roles indicated by the edge relations We call this approximating referential equivalence This means it cannot capture some of the subtleties that a theory based on logical equivalence might be expected to Thus using the more impoverished lexical relations representation may not be costing us much in practice Although we have not provided a denotational semantics for sets of relation edges, we anticipate that this will be possible along the lines developed in monotonic semantics (Alshawi and Crouch 1992  (1990) in their surface translation model The translation probability is then the sum of probabilities over different alignments f:  There are different ways to model P(Ct,f|Cs) corresponding to different kinds of alignment relations and different independence assumptions about the translation mapping For our quantitative design, we adopt a simple model in which lexical and relation (structural) probabilities are assumed to be independent In this model the alignment relations are functions from the word occurrence nodes of Ct to the word occurrences of Cs The idea is that f(vj wi means that the source word occurrence wi `gave rise' to the target word occurrence vj The inverse relation f 1] need not be a function, allowing different numbers of words in the source and target sentences That is, the probability that f maps exactly the (possibly empty) subset of Nt to wi These subgraphs give rise to disjoint sets of relation edges which together form Et For simplicity, we assume here that the source graph Cs is a tree This is consistent with our earlier assumptions about the source language model For the simple model it remains to specify derivation step probabilities This last condition ensures that the target graph partitions join up in a way that is compatible with the node alignment f The factoring of the translation model into these lexical and structural components means that it will overgenerate because these aspects are not independent in translation between real natural languages This can be done in a parallel fashion to the forward direction described above 1985) in the qualitative model Computationally, the quantitative model lets us escape from the undecidability of logic-based reasoning In particular, we preserved the notion of hierarchical phrase structure The quantitative model also reduced overall complexity in terms of the sets of symbols used In addition to words, it only required symbols for dependency relations, whereas the qualitative model required symbol sets for linguistic categories and features, and a set of word sense symbols Despite their apparent importance to translation, the quantitative system can avoid the use of word sense symbols (and the problems of granularity they give rise to) by exploiting statistical associations between words in the target language to filter implicit sense choices It seems unlikely that these continuously variable aspects of fluent natural language can be captured by a purely combinatoric model A speech translation system, which by necessity combines speech and language technology, is a natural place to consider combining the statistical and conventional approaches and much of this paper describes probabilistic models of structural language analysis and translation This naturally leads to the question of how best to introduce quantitative modeling into language processing Nevertheless, probability theory does offer a coherent and relatively well understood framework for selecting between uncertain alternatives, making it a natural choice for quantitative language processing 1992  Even if probability theory remains, as it currently is, the method of choice in making language processing quantitative, this still leaves the field wide open in terms of carving up language processing into an appropriate set of events for probability theory to work with Our aim will be to provide an overall model for translation with the best of both worlds For translation, a very direct approach using parameters based on surface positions of words in source and target sentences was adopted in the Candide system (Brown et al 1990  However, this does not capture important structural properties of natural language Nor does it take into account generalizations about translation that are independent of the exact word order in source and target sentences 1992  The aim of the quantitative language and translation models presented in sections and is to employ probabilistic parameters that reflect linguistic structure without discarding rich lexical information or making the models too complex to train automatically We now consider a hypothetical speech translation system in which the language processing components follow a conventional qualitative transfer design The translation relation is expressed by a set of first order axioms which are used by a theorem prover to derive a target language logical form that is equivalent (in some context) to the source logical form A grammar for the target language is then applied to the target form, generating a syntax tree whose fringe is passed to a speech synthesizer A grammar, expressed as a set of syntactic rules (axioms) Gsyn and a set of semantic rules (axioms) Gsem is used to support a relation form holding between strings s and logical forms expressed in first order logic:  The relation form is many-to-many, associating a string with linguistically possible logical form interpretations cm(w  There is also the problem of requiring increasingly complex feature sets to describe idiosyncrasies in the lexicon An example of this is explicit scoping which leads (again) to large numbers of alternatives which the qualitative model has difficulty choosing between Qualitative approaches to grammar tend to emphasize the ability to capture generalizations as the main measure of success in linguistic modeling We can state this as follows:  We are thus forced to under-filter and make an arbitrary choice between remaining alternatives In both the quantitative and qualitative models we take a transfer approach to translation We do not depend on interlingual symbols, but instead map a representation with constants associated with the source language into a corresponding expression with constants from the target language