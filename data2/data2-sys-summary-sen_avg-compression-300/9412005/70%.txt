 Infants must learn to recognize certain sound sequences as being words; this is a difficult problem because normal speech contains no obvious acoustic divisions between words However, the problem we examined is different we want to know how infants segment speech before knowing which phonemic sequences form words Finally, two other simulations were run on each sample to measure chance performance: (1) RAND-FREE inserted random segmentation points and reported the resulting hypothesis, (2) RAND-PHONO inserted random segmentation points where permitted by the phonotactic constraints Since the RAND simulations were given the number of segmentation points to add (equal to the number of segmentation points needed to produce the natural English segmentation their performance is an upper bound on chance performance In contrast, the DIST simulations must determine the number of segmentation points to add using MDL evaluations Each simulation was scored for the number of correct segmentation points inserted, as compared to the natural English segmentation Note that there is a trade-off between recall and accuracy if all possible segmentation points were added, recall would be 100% but accuracy would be low; likewise, if only one segmentation point was added between two words, accuracy would be 100% but recall would be low Primarily, two sources have been examined: prosody and word stress Note in all the results of DIST-FREE that using distributional information alone favors recall over accuracy; in fact, the segmentation hypotheses produced by DIST-FREE have most words broken into single phoneme units with only a handful of words remaining intact The former comparison shows that the sources combined are more useful than phonotactic information alone Data on discovered word types helps make this comparison: DIST-FREE found 12% of the words with 30% accuracy and DIST-PHONO found 33% of the words with 50% accuracy Whereas the segmentation point data are inconclusive, word type data demonstrate that combining information sources is more useful than using distributional information alone This difference is again supported by word type data: 14% recall with 30% accuracy for adult-directed speech, 56% recall with 65% accuracy for child-directed speech Our technique segments continuous speech into words using only distributional and phonotactic information more effectively than one might expect up to 66% recall of segmentation points with 92% accuracy on one sample, which yields 58% recall of word types with 67% accuracy (the relatively low type accuracy is mitigated by the fact that most incorrect words are meaningful concatenations of correct words eg `thekitty  This finding confirms the idea that distribution and phonotactics are useful sources of information that infants might use in discovering words (eg Jusczyk et al 1993b  In fact, it helps explain infants' ability to learn words from parental speech: these two sources alone are useful and infants have several others, like prosody and word stress patterns, available as wellg Jusczyk, 1993, downplayed the utility of words in isolation  Word stress in English fairly accurately predicts the location of word beginnings (Cutler Norris, 1988; Cutler Butterfield, 1992 Jusczyk, Cutler and Redanz (1993) demonstrated that 9-month-olds (but not 6-month-olds) are sensitive to the common strong/weak word stress pattern in English However, Fisher and Tokura (in press) found no evidence that prosody can accurately predict word boundaries, so the task of finding words remains To gain an intuitive understanding of our model, consider the following speech sample (transcription is in IPA particular segmentation is called a segmentation hypothesis  Two such hypotheses are: lexicons: frequent words whereas Segmentation 2 yields a much larger lexicon of infrequent words Also note that a lexicon contains only the words used in the sample no words are known to the system a priori, nor are any carried over from one hypothesis to the next Given a lexicon, the sample can be encoded by replacing words with their respective indices into the lexicon: sizes of the lexicon and encoded sample We found that each source provided some useful information for speech segmentation, but the combination of sources provided substantial information We also found that child-directed speech was much easier to segment than adult-directed speech when using both sources 74-75  The lexicon lists words (represented as phoneme sequences) paired with their code after the other; the first column is called the word inventory column; the second column is called the code word inventory column In the word inventory column (see Figure for a schematic the list of lexical items is represented as a continuous string of phonemes, without separators between words (e Each length is represented as a fixed-length, zero-padded binary number So, the length of the representation of a word wiin the lexicon is the number of phonemes in the word times the length of a phoneme:  In each field is an integer between one and the number of phonemes in the longest word The code word inventory column of the lexicon (see Figure for a schematic) has a nearly identical representation as the previous column except that code words are listed instead of phonemic words the length fields and unary prefix serve the same purpose of marking the divisions between code words The sample can be represented most compactly by assigning short code words to frequent words, reserving longer code words for infrequent words To satisfy this property, code words are assigned so that their lengths are frequency-based; the length of the code word for a word of frequency f(w) will not be greater than: The total length of the code word list is the sum of the code word lengths over all lexical entries: As in the word inventory column (described above the length of each code word is represented in a fixed-length field Finally, the sequence of words which form the sample (see Figure for a schematic) is represented as the number of words in the sample (m) followed by the list of code words Since code words are used as compact indices into the lexicon, the original sample could be reconstructed completely by looking up each code word in this list and replacing it with its phoneme sequence from the lexicon The code words we assigned to lexical items are self-delimiting (once the set of codes is known so there is no need to represent the boundaries between code words The length of the representation of the integer m is given by the function The length of the representation of the sample is computed by summing the lengths of the code words used to represent the sample We can simplify this description by noting that the combined length of all occurrences of a particular code word [wi] is since there are f(wi) occurrences of the code word in the sample The total length of the representation of the entire hypothesis is the sum of the representation lengths of the word inventory column, the code word inventory column and the sample This system of computing hypothesis sizes is efficient in the sense that elements are thought of as being represented compactly and that code words are assigned based on the relative frequencies of words In those simulations which used the phonotactic knowledge, a word boundary could not be inserted when doing so would create a word initial or final consonant cluster not on the list or would create a word without a vowel Note that using the phonotactic constraints reduces the number of potential word boundaries from fifteen to six in this example Each sample was checked for consistent word spellings (e