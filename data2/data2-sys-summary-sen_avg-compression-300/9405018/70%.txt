 In a lazy learning approach, on the other hand, knowledge acquisition is automatic Information-theoretic metrics like information gain basically modify this feature space automatically by assigning more or less weight to particular features (dimensions  In constructive induction, completely new feature dimensions may be introduced for separating the different category areas better in feature space The effect of this is that the priorities between the three goals discussed earlier are changed: the representation of the acquired knowledge depends on the acquisition technique used, and the knowledge acquired depends on what the learning algorithm has induced as being relevant in solving the task Lazy Learning is fundamentally a classification paradigm Identification Instances of this include part of speech tagging, grapheme-to-phoneme conversion, lexical selection in generation, morphological synthesis, word sense disambiguation, term translation, stress assignment, etc Segmentationg shift and different types of reduce as categories in a shift-reduce parser (see for such an approach outside the context of Machine Learning  It also has as a consequence that it is no longer a priori evident that there should be different components for lexical and non-lexical knowledge in the internal representation of an NLP system solving a task, except when the task learned is specifically lexical In the lazy learning approach ( ; we used the windowing approach referred to earlier to formulate the task as a classification problem (more specifically, a segmentation problem  The lazy learning approach produced results which were more accurate than both a connectionist approach (backpropagation learning in a recurrent multi-layer perceptron) and a knowledge-based approach Again, in the knowledge-based approach, the lexical requirements for such a system are extensive In a typical knowledge-based system solving the problem, morphological analysis (with lexicon phonotactic knowledge, and syllable structure determination modules are designed and implemented In a lazy learning approach ( ; again a windowing approach was used to formulate the task as a classification problem (identification this time: given a set of possible phonemes, determine which phoneme should be used to translate a target spelling symbol taking into account its context  Results were highly similar to the syllable boundary prediction task: the lazy learning approach resulted in systems which were more accurate than both a connectionist approach and a linguistically motivated approach The results were replicated for English, French, and Dutch, using the same lazy learning algorithm, which shows its reusability Another task we applied the lazy learning algorithm to, was stress assignment in Dutch monomorphematic, polysyllabic words ( ,  There were three categories: final stress, penultimate stress, and antepenultimate stress (an identification problem  For the actual tagging problem, a moving window approach was again used, using patterns of ambiguous categories (a target and a left and right context  Section 3 introduces lazy learning, the symbolic machine learning paradigm which we have used in experiments in lexical acquisition First, as far as linguistic engineering is concerned, a new approach to the reusability problem was proposed Instead of concentrating on linguistic engineering of theory-neutral, poly-theoretic, multi-applicable lexical representations combined with semi-automatic migration of lexical knowledge between different formats, we propose an approach in which a single inductive learning method is reused on different corpora representing useful linguistic mappings, acquiring the necessary lexical information automatically and implicitly Secondly, the theoretical claim underlying this proposal is that language acquisition and use (and a fortiori lexical knowledge acquisition and use) are behaviour-based processes rather than knowledge-based processes These processes implement lexical performance Each lexical process is represented by a set of exemplars (solved cases) in memory, which act as models to new input In a broader context, the results described here argue for an empiricist approach to language acquisition, and for exemplars rather than rules in linguistic knowledge representation (see and Gillis et al Also, information gain or other feature weighting techniques can be used to automatically reduce the dimensionality of the problem, sometimes effectively solving the sparse data problem Section 5 gives an overview of research results in applying lazy learning to the acquisition of lexical knowledge, and Section 6 concludes with a discussion of advantages and limitations of the approach The methods described often make use of a moving window approach in which only a local part of an input representation is used A possible solution for this problem is the cascading of different lazy learning systems, one working on the output of the other For example, a learning system for part of speech tagging could be combined with a learning system taking patterns of disambiguated tags as input, and producing constituent types as output Taking patterns of constituent types as input, a third learning system should have no problem assigning long-distance dependencies: given the right representation, all dependencies are local One of the central intuitions in current knowledge-based NLP research is that in solving a linguistic task (like text-to-speech conversion, parsing, or translation the more linguistic knowledge is explicitly modeled in terms of rules and knowledge bases, the better the performance Current lexical research in language technology is eminently knowledge-based in this respect As far as lexical knowledge is concerned, this knowledge is represented in a lexical knowledge base, introduced either by hand or semi-automatically using machine-readable dictionaries The problem of reusability is dealt with by imposing standards on the representation of the knowledge, or by applying filters or translators to the lexical knowledge In this paper, we will claim that regardless of the state of theory-formation about some linguistic task, simple data-driven learning techniques, containing very little a priori linguistic knowledge, can lead to performance systems solving the task with an accuracy higher than state-of-the art knowledge-based systems Consistency Heuristic It is also generally acknowledged that there exists a natural order of dependencies between these three research questions: acquisition techniques depend on the type of knowledge representation used and the type of knowledge that should be acquired, and the type of knowledge representation used depends on what should be represented In this approach, emphasis shifts from knowledge representation (competence) to induction of systems exposing useful behaviour (performance and from knowledge engineering to the simpler process of data collection It is useful in Machine Learning to make a distinction between a learning component and a performance componentg a word and its context) using some kind of representation (decision trees, classification hierarchies, rules, exemplars,  The learning component implements a learning method There are several ways in which domain bias (a priori knowledge about the task to be learned) can be used to optimize learning Other evaluation criteria include learning and performance speed, memory requirements, clarity of learned representations, etc Recently, there has been an increased interest in Machine Learning for lazy learning methods In this type of similarity-based learning, classifiers keep in memory (a selection of) examples without creating abstractions in the form of rules or decision trees (hence lazy learning  Instances of this form of nearest neighbour method include instance-based learning ( exemplar-based learning ( , memory-based reasoning ( and case-based reasoning (  Different tasks require different lexical information Examples are represented as a vector of feature values with an associated category label Also, different theoretical formalisms, domains, and languages require different types of lexical information and therefore possibly also different types of lexical knowledge representation and different acquisition methods In lazy learning, performance crucially depends on the distance metric used Elsewhere ( ; ) we introduced the concept of information gain (also used in decision tree learning, ) into lazy learning to weigh the importance of different features in a domain-independent way The information entropy of such an information source can be compared in turn for each feature to the average information entropy of the information source when the value of that feature is known Database information entropy is equal to the number of bits of information needed to know the category given a pattern V is the set of possible values for feature f Linguistic tasks (including lexical tasks) are context-sensitive mappings from one representation to another (e To illustrate the difference between the traditional knowledge-based approach with the lazy learning approach, consider Fig