 The grammar induction algorithms most successful in language modeling include the Inside-Outside algorithm , , , a special case of the Expectation-Maximization algorithm , and work by  To evaluate our algorithm, we compare the performance of our algorithm to that of n-gram models and the Inside-Outside algorithm For n-gram models, we tried for each domain The smoothing parameters are trained through the Forward-Backward algorithm on held-out data The parameter is trained through the Inside-Outside algorithm on held-out data The objective function is taken to be some measure dependent on the training data; one generally wants to find a grammar that in some sense accurately models the training data In the first two domains, we created the training and test data artificially so as to have an ideal grammar in hand to benchmark results In particular, we used a probabilistic grammar to generate the data In the first domain, we created this grammar by hand; the grammar was a small English-like probabilistic context-free grammar consisting of roughly 10 nonterminal symbols, 20 terminal symbols, and 30 rules In the second domain, we derived the grammar from manually parsed text Most work in language modeling, including n-gram models and the Inside-Outside algorithm, falls under the maximum-likelihood paradigm, where one takes the objective function to be the likelihood of the training data given the grammar The ideal grammar denotes the grammar used to generate the training and test data We achieve a moderate but significant improvement in performance over n-gram models and the Inside-Outside algorithm in the first two domains, while in the part-of-speech domain we are outperformed by n-gram models but we vastly outperform the Inside-Outside algorithm However, the optimal grammar under this objective function is one which generates only strings in the training data and no other strings The first pass row refers to the main grammar induction phase of our algorithm, and the post-pass row refers to the Inside-Outside post-pass Notice that our algorithm produces a significantly more compact model than the n-gram model, while running significantly faster than the Inside-Outside algorithm even though we use an Inside-Outside post-pass Our algorithm consistently outperformed the Inside-Outside algorithm in these experiments In the Inside-Outside algorithm, the gradient descent search discovers the nearest local minimum in the search landscape to the initial grammar Such grammars are poor language models, as they overfit the training data and do not model the language at large Hence, our algorithm should be less prone to suboptimal local minima than the Inside-Outside algorithm In n-gram models and the Inside-Outside algorithm, this issue is evaded by bounding the size and form of the grammars considered, so that the optimal grammar cannot be expressed A factor in the objective function that favors smaller grammars over large can prevent the objective function from preferring grammars that overfit the training data The goal of grammar induction is taken to be finding the grammar with the largest a posteriori probability given the training data, that is, finding the grammar G' where and where we denote the training data as O, for observations As described above, we take grammar induction to be the search for the grammar G' that optimizes the objective function p(O|G)p(G  We maintain a single hypothesis grammar which is initialized to a small, trivial grammar We then try to find a modification to the hypothesis grammar, such as the addition of a grammar rule, that results in a grammar with a higher score on the objective function When we find a superior grammar, we make this the new hypothesis grammar For our initial grammar, we choose a grammar that can generate any string, to assure that the grammar can cover the training data The initial grammar is listed in Table  Notice that this grammar models a sentence as a sequence of independently generated nonterminal symbols An appealing alternative is grammar-based language models We use the term move set to describe the set of modifications we consider to the current hypothesis grammar to hopefully produce a superior grammar Our move set includes the following moves: Move 1: Create a rule of the form Move 2: Create a rule of the form For any context-free grammar, it is possible to express a weakly equivalent grammar using only rules of these forms Consider the task of calculating the objective function p(O|G)p(G) for some grammar G We cannot afford to parse the training data for each grammar considered; indeed, to ever be practical for data sets of millions of words, it seems likely that we can only afford to parse the data once Language models expressed as a probabilistic grammar tend to be more compact than n-gram language models, and have the ability to model long-distance dependencies , ,  To make this possible, we approximate the probability of the training data p(O|G) by the probability of the single most probable parse, or Viterbi parse, of the training data For example, consider the case where the training data consists of the two sentences In Figure , we display the Viterbi parse of this data under the initial hypothesis grammar used in our algorithm Now, let us consider the move of adding the rule to the initial grammar (as well as the concomitant rule  To minimize these effects, we process the training data incrementally Using our initial hypothesis grammar, we parse the first sentence of the training data and search for the optimal grammar over just that one sentence using the described search framework We use the resulting grammar to parse the second sentence, and then search for the optimal grammar over the first two sentences using the last grammar as the starting point In this section, we describe how the parameters of our grammar, the probabilities associated with each grammar rule, are set Ideally, in evaluating the objective function for a particular grammar we should use its optimal parameter settings given the training data, as this is the full score that the given grammar can achieve In this paper, we describe a corpus-based induction algorithm for probabilistic context-free grammars that outperforms n-gram models and the Inside-Outside algorithm in medium-sized domains To address this issue, we use an Inside-Outside algorithm post-pass We create n new nonterminal symbols , and create all rules of the form: denotes the set of nonterminal symbols acquired in the initial grammar induction phase, and X1 is taken to be the new sentential symbol The algorithm employs a greedy heuristic search within a Bayesian framework, and a post-pass using the Inside-Outside algorithm Using this grammar as the starting point, we run the Inside-Outside algorithm on the training data until convergence As mentioned, this work employs the Bayesian grammar induction framework described by Solomonoff 