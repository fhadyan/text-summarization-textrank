 In particular, we used a probabilistic grammar to generate the data In the first domain, we created this grammar by hand; the grammar was a small English-like probabilistic context-free grammar consisting of roughly 10 nonterminal symbols, 20 terminal symbols, and 30 rules The ideal grammar denotes the grammar used to generate the training and test data We maintain a single hypothesis grammar which is initialized to a small, trivial grammar When we find a superior grammar, we make this the new hypothesis grammar For our initial grammar, we choose a grammar that can generate any string, to assure that the grammar can cover the training data Using our initial hypothesis grammar, we parse the first sentence of the training data and search for the optimal grammar over just that one sentence using the described search framework We use the resulting grammar to parse the second sentence, and then search for the optimal grammar over the first two sentences using the last grammar as the starting point Using this grammar as the starting point, we run the Inside-Outside algorithm on the training data until convergence