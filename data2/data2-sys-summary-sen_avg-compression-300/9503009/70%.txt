 One way to do this is to let the right context vector record which classes of left context vectors occur to the right of a word The rationale is that words with similar left context characterize words to their right in a similar way Rather than having separate entries in its right context vector for seemed would and likes a word like he can now be characterized by a generalized entry for inflected verb form occurs frequently to my right  This proposal was implemented by applying a singular value decomposition to the 47025-by-250 matrix of left context vectors and clustering the resulting context vectors into 250 classes A generalized right context vector v for word w was then formed by counting how often words from these 250 classes occurred to the right of w Generalized left context vectors were derived by an analogous procedure using word-based right context vectors Note that the information about left and right is kept separate in this computation This differs from previous approaches , in which left and right context vectors of a word are always used in one concatenated vector There are arguably fewer different types of right syntactic contexts than types of syntactic categories This generalization could not be exploited if left and right context were not treated separately Another argument for the two-step derivation is that many words don't have any of the 250 most frequent words as their left or right neighbor The generalized context vectors were input to the tag induction procedure described above for word-based context vectors: 20,000 word triplets were selected from the corpus, encoded as 1,000-dimensional vectors (consisting of four generalized context vectors decomposed by a singular value decomposition and clustered into 200 classes Tables present results for word type-based induction and induction based on word type and context Table shows that performance for generalized context vectors is better than for word-based context vectors (0 However, since the number of tags with better and worse performance is about the same (7 and 5 one cannot conclude with certainty that generalized context vectors induce tags of higher quality Several researchers have worked on learning grammatical properties of words Apparently, the 250 most frequent words capture most of the relevant distributional information so that the additional information from less frequent words available from generalized vectors only has a small effect those not containing punctuation marks and rare words Rare words are difficult because of lack of distributional evidence Both occurrences are miscategorized, since its context vectors do not provide enough evidence for the verbal use and use vector models in which words are clustered according to the similarity of their close neighbors in a corpus Finally, our procedure induces a hard part-of-speech classification of occurrences in context, i applies factor analysis to collocations of two target words certain and right with their immediate neighbors Given the widespread part-of-speech ambiguity of words this is problematic will is needed in combination with context for correct classification In this paper, we will compare two tagging algorithms, one based on classifying word types, and one based on classifying words-plus-context We start by constructing representations of the syntactic behavior of a word with respect to its left and right context Therefore, we will measure the similarity between two words with respect to their syntactic behavior to, say, their left side by the degree to which they share the same neighbors on the left0 if two words share many neighbors, and 0 We refer to the vector of left neighbors of a word as its left context vector, and to the vector of right neighbors as its right context vector The unreduced context vectors in the experiment described here have 250 entries, corresponding to the 250 most frequent words in the Brown corpus Their right similarity according to the cosine measure would be zero But even with high-frequency words, the simple vector model can yield misleading similarity measurements Yet intuitively, they are similar with respect to their right syntactic context despite the lack of common right neighbors We can represent the left vectors of all words in the corpus as a matrix C with n rows, one for each word whose left neighbors are to be represented, and kcolumns, one for each of the possible neighbors Table shows the nearest neighbors of two words (ordered according to closeness to the head word) after the dimensionality reduction Neighbors with highest similarity according to both left and right context are listed For seemed left-context neighbors are words that have similar types of noun phrases in subject position (mainly auxiliaries  An adjective like likely is very similar to seemed in this respect although its left context is quite different from that of seemed  These examples demonstrate the importance of representing generalizations about left and right context separately The left and right context vectors are the basis for four different tag induction experiments, which are described in detail below: induction based on word type only induction based on word type and context induction based on word type and context, restricted to natural contexts induction based on word type and context, using generalized left and right context vectors The two context vectors of a word characterize the distribution of neighboring words to its left and right The concatenation of left and right context vector can therefore serve as a representation of a word's distributional behavior ,  We formed such concatenated vectors for all 47,025 words (surface forms) in the Brown corpus Here, we use the raw 250-dimensional context vectors and apply the SVD to the 47,025-by-500 matrix (47,025 words with two 250-dimensional context vectors each  This classification constitutes the baseline performance for distributional part-of-speech tagging All occurrences of a word are assigned to one class In order to exploit contextual information in the classification of a token, we simply use context vectors of the two words occurring next to the token An occurrence of word w is represented by a concatenation of four context vectors: The right context vector of the preceding word The left context vector of w The right context vector of w The left context vector of the following word We randomly selected 20,000 word triplets from the corpus and formed concatenations of four context vectors as described above The singular value decomposition of the resulting 20,000-by-1,000 matrix defines a mapping from the 1,000-dimensional space of concatenated context vectors to a 50-dimensional reduced space Distributional tagging of an occurrence of a word w proceeds then by retrieving the four relevant context vectors (right context vector of previous word, left context vector of following word, both context vectors of w) concatenating them to one 1000-component vector, mapping this vector to 50 dimensions, computing the correlations with the 200 cluster centroids and, finally, assigning the occurrence to the closest cluster The context vectors of punctuation marks contribute little information about syntactic categorization since there are no grammatical dependencies between words and punctuation marks, in contrast to strong dependencies between neighboring words For this reason, a second induction on the basis of word type and context was performed, but only for those tokens with informative contexts Tokens next to punctuation marks and tokens with rare words as neighbors were not included Contexts with rare words (less than ten occurrences) were also excluded for similar reasons: If a word only occurs nine or fewer times its left and right context vectors capture little information for syntactic categorization The context vectors used so far only capture information about distributional interactions with the 250 most frequent words