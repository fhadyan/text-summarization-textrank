33 and 000 respectively This is basically the entropy used in  Repeat i i+1; ; N[i] N N 3 Until for some appropriate set metric (e If C(N) [ C0 then Shigh Smid else Slow Smid; Nc N; 5 Goto ; 6 The resulting rules will be the set of cut-up training examples A threshold value of say 1 Note that they not correspond to any training example The likelihood for this to happen by chance decreases drastically with increased rule length A second reason for this is that the number of states visited will decrease with increasing reduction length This is in sharp contrast to what the above scheme accomplishes; the corresponding figures are about 20 or 30 percent each for lengths one and two An attempted solution to this problem is to impose restrictions on neighbouring cutnodes The table below summarizes the results for some grammars of different coverage extracted using: 1 Hand-coded tree-cutting criteria Entropy is a measure of disorder Figure shows five examples of implicit parse trees Perplexity is related to entropy as follows We now turn to the task of calculating the entropy of a node in a parse tree10 + 133 = 2 Let us call these nodes cutnodes  Calculate the entropy of each or-node where it is difficult to predict which rule will be resolved on next This corresponds exactly to the nodes in the and-or tree that exhibit high entropy values Determine a threshold entropy that yields the desired coverage This can be done using for example interval bisection First, the treebank is partitioned into a training set and a test set The test set will be used to check the coverage of the set of extracted rules Then, the set of implicit parse trees is stored in an and-or tree Each such arc leads to an or-node This means that if we cut at a node corresponding to e an NP, i Repeat i i+1; N[i] N(N[i-1 3 Until N[i] = N[i-1]4 In the simplest scheme for calculating the entropy of an or-node, only the RHS phrase of the parent rule, i the dominating and-node, contributes to the entropy, and there is in fact no need to employ an and-or tree at all, since the tree-cutting criterion becomes local to the parse tree being cut up In a slightly more elaborate scheme, we sum over the entropies of the nodes of the parse trees that match this node of the and-or tree