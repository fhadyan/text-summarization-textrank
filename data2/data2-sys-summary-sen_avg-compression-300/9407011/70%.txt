g , ,  This provides an elegant account in the simple case, but requires a strong assumption of co-operativeness Below we describe parts of the discourse model in more detail and then show how it is used to account for aspects of this dialogue The TRAINS System is a large integrated natural language conversation and plan reasoning system Figure illustrates the system from the viewpoint of the dialogue manager The dialogue manager is responsible for maintaining the flow of conversation and making sure that the conversational goals are met For this system, the main goals are that an executable plan which meets the user's goals is constructed and agreed upon by both the system and the user and then that the plan is executed Agent A must adopt agent B's goals as her own Each utterance will generally contain acts (or partial acts) at each of these levels The stack structure is appropriate because, in general, one must respond to the most recently imposed obligation first When the intentions are formed, the obligations are removed from the stack, although they have not yet actually been met The over-riding goal for the TRAINS domain is to construct and execute a plan that is shared between the two participants This leads to other goals such as accepting proposals that the other agent has suggested, performing domain plan synthesis, proposing to the other agent plans that the domain plan reasoner has constructed, or executing a completed plan As shown above, though, new obligations will need to be addressed before performing intended actions Obligations might also lead directly to immediate action If there are no obligations, then the agent will consider its intentions and perform any actions which it can to satisfy these intentions For the discourse actor, special consideration must be given to the extra constraints that participation in a conversation imposes Discourse Obligations from Table 2 Several approaches have been suggested to account for this behavior Intended Speech Acts 4 Weak Obl: Grounding (coordinate mutual beliefs) 5 Discourse Goals: Domain Plan Negotiation 6 High-level Discourse Goals The implemented actor serializes consideration of these sources into the algorithm in Figure g the observance of a new utterance from the user If there are any, then the actor will do what it thinks best to meet those obligationsg the acceptance, rejection, or clarification  When this intention is acted upon and the utterance produced, the obligation will be discharged Other obligation types are to repair an uninterpretable utterance or one in which the presuppositions are violated, or to answer a question In certain cases, though, such as a repair, the system will actually try to take control of the turn and produce an utterance immediately Others have tried to account for this kind of behavior using social intentional constructs such as Joint intentions or Shared Plans  When the system does not have the turn, the conversational state will still be updated, but the actor will not try to deliberate or act This might, of course, end up in releasing the turn This will involve acknowledging or repairing user utterances, as well as repairing and requesting acknowledgement of the system's own utterances If all accessible utterances are grounded, the actor then considers the negotiation of domain beliefs and intentions (lines 9-10  The actor will try to work towards a shared domain plan, adding intentions to perform the appropriate speech acts to work towards this goal If none of the more local conversational structure constraints described above require attention, then the actor will concern itself with its actual high-level goals It is also at this point that the actor will take control of the conversation, pursuing its own objectives rather than responding to those of the user Utterance 1 is interpreted as performing two Core Speech Acts It is interpreted (literally) as the initiation of an inform about an obligation to perform a domain action (shipping the oranges  In addition, this utterance releases the turn to the system Figure shows the relevant parts of the discourse state after interpretation of this utterance Finally, the system acts on the intentions produced by these deliberations (lines 5-6) and produces the combined acknowledgement/acceptance of utterance 2 Utterances 3-3=6 and 3-7 are interpreted, but not responded to yet since the user keeps the turn (in this case by following up with subsequent utterances before the system has a chance to act  The resulting discourse context (after the system decides to acknowledge) is shown in Figure  Yet, typically agents will still respond in such situations The reasoning leading up to utterance 14 is similar to that leading to utterance 2 Utterances 15-2=4, 15-5=7, and 15-8=10 are interpreted as requests because of the imperative surface structure The discourse obligation to address the request is incurred only when the system decides to acknowledge the utterances and ground them Utterance 17 is interpreted as a request for evaluation of the plan When the system decided to acknowledge, this creates a discourse obligation to address the request An agent has certain goals, and communication results from a planning process to achieve these goals As another example, consider a case in which the agent's goals are such that it prefers that an interrogating agent not find out the requested information This is then generated as 18-3 The discourse state after the decision to acknowledge is shown in Figure  This example illustrates only a small fraction of the capabilities of the dialogue model However this architecture can handle varying degrees of initiative, while remaining responsive The default behavior is to allow the user to maintain the initiative through the plan construction phase of the dialogue We can illustrate the system behaving more on the basis of goals than obligations with a modification of the previous example Here, the user releases the turn back to the system after utterance 2, and the deliberation proceeds as follows: the system has no obligations, no communicative intentions, nothing is ungrounded, and there are no unaccepted proposals, so the system starts on its high-level goalsg a choice of the particular engine or boxcar to use  Assuming that the user still has not taken the turn back, the system can now propose these new items to the user The user will now be expected to acknowledge and react to these proposals If the proposal is not accepted or rejected, the system can request an acceptance If a proposal is rejected, the system can negotiate and offer a counterproposal or accept a counter proposal from the user Obligations do not replace the plan-based model, but augment it The resulting model more readily accounts for discourse behavior in adversarial situations and other situations where it is implausible that the agents adopt each others goals Clearly, the more two agents agree on the rules, the smoother the interaction becomes, and some rules are clearly virtually universal Some researchers, e In particular, much of local discourse behavior can arise in a reactive manner without the need for complex planning Following the initiative of the other can be seen as an obligation driven process, while leading the conversation will be goal driven Representing both obligations and goals explicitly allows the system to naturally shift from one mode to the other It is left unexplained what goals motivate conversational co-operation We are developing an alternate approach that takes a step back from the strong plan-based approach We believe that people have a much more complex set of motivations for action Social interactions are enabled by their being a sufficient compatibility between the rules affecting the interacting agents One responds to a question because this is a social behavior that is strongly encouraged as one grows up, and becomes instilled in the agent When planning, an agent considers both its goals and obligations in order to determine an action that addresses both to the extent possible Returning to the example about questions, when an agent is asked a question, this creates an obligation to respond Rather it is a constraint on the actions that the agent may plan to dog consider most politicians at press conferences  The planning task then is to satisfy the obligation of responding to the question, without revealing the answer if at all possible An action that might occur or not-occur according to R is neither obligatory nor forbidden If an obligation is not satisfied, then this means that one of the rules must have been broken But when they directly conflict with the agent's personal goals, the agent may choose to violate them Obligations are quite different from and can not be reduced to intentions and goals The intentional story account of this goes as follows Obligations also cannot be reduced to simple expectations, although obligations may act as a source of expectations The interpretation of an utterance will often be clear even without coherence with prior expectations We need to allow for the possibility that an agent has performed an action even when this violates expectations In a conversational setting, an accepted offer or a promise will incur an obligation Our model of obligation is very simple We use a set of rules that encode discourse conventions We use a simple forward chaining technique to introduce obligations Some obligation rules based on the performance of conversation acts are summarized in Table  A question establishes an obligation to answer the question If an utterance has not been understood, or is believed to be deficient in some way, this brings about an obligation to repair the utteranceg the architecture proposed by  There are a large number of strategies which may be used to incorporate obligations into the deliberative process, based on how much weight they are given compared to the agents goals For instance, consider an agent with an intention to do something as soon as possible We have built a system that explicitly uses discourse obligations and communicative intentions to partake in natural dialogue This system plays the role of the dialogue manager in the TRAINS dialogue system, which acts as an intelligent planning assistant in a transportation domain