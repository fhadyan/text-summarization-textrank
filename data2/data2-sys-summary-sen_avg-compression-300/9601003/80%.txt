 assessment task has connections with SU 1 One might wish e independent evaluations for spoken and written language engineering Then there's handwriting as well  at the level of user-significant components only 2 Accuracy and robustness are often also attributes of components or systems implementing tasks But evaluation requires modularity to be enforced everywhere, and that will just block innovation 2: We need to bear in mind that finer granularity may differ substantially between systems, so that in may cases comparative evaluation may not be possible generation ? I don't know what 'willing' is supposed to mean here - this is a very resource-dependent thing word sense identification ? 7 phone lattice production ? 3 prosodic marking ? many spoken language systems wont produce any of these language ? 2 native-non-native speakers ? 26 Implementation of Comparative TA SE Two possible scenarios for developing an evaluation programme are the following In this scenario the type of evaluation evolves out of actual funded pilot applications This would be done in the 2nd and final calls the bottom-up scenario ? 2 bottom up  homogeneous What does are required, where appropriate, to participate I am afraid that this will not lead anywhere 2: I don't think either of theses is an effective way to achieve this goal, but 2 comes closest Task Decomposition: There may be more than one way of decomposing a task into constituent sub-tasks7 Other Comments If you have any other comments you would like to make about the way you think TA SE should be carried out in the EC, or about aspects of this questionnaire (omissions or commissions) then please make them here general business letter dictation Different task decompositions may be more or less appropriate to different constraints on the linguistic features, the depth and accuracy, and the environment within which a task is to be performed - My answers are very much those of someone interested in developing, rather than using, written language technology Strategic Marked and Innovation Watch, as well as with sector-based tasks concerned with assessment and evaluation you set up false dichotomies, where one actually has a gradation eg for 2 Clearly some mix is required Proper performance of these tasks may be as essential to the success of the system as carrying out its linguistic tasks Not all respondents replied to all questions Not all respondents answered all questions5 Content of Comparative TA SE a translation system for weather reports and a translation system for airline enquiries Environments thus display a concentric, onion-like organisation, where what constitutes a system, task or component at one level may correspond to part of an environment at a lower level One can consider a computational system within the environment constituted by its end-users One can look at a major task components in a system (e translation, document retrieval, speech recognition) in the context of other major task components in the system One can view lower level tasks (parsing, coreference resolution) within the environment of the broader linguistic tasks they contribute to (e Any task or component (including non-linguistic ones) within a system also has an environment It is therefore a mistake to draw a completely hard line between user-centered / system assessment on the one hand, and technology / individual component assessment on the other To match a task to its environment, we need to compare task attributes and environment attributes A grid evaluation methodology naturally emerges This involves carrying out different runs of a system or component, varying environmental attributes (e language, subject domain, text type) against different settings of task parameters if one wishes to investigate how a system of component would fare outside its intended environment Thus a system environment attribute concerning speed of processing will typically surface as a similar environmental attribute for components or sub-tasks within the system There may be two reasons why environmental attributes are not matched by system attributes First, the components implementing the system task have not been coded with sufficient generality or flexibility; in this case the system may be doing the right thing, but not well enough Users are the prime determinants of system environment variables It is therefore necessary to obtain user profile data, and in this regard we should distinguish further categories of user present users, accessible for collection of user data prospective users, from whom such data may or may not be obtainable idealised users, to who future users will hopefully correspond individual users: if a system is being tailored to a specific individual their idiosyncratic needs should be taken into account collective users (e Assessment comprises verification and validation The difference between verification and validation is succinctly summarised as Verification: are we building the system right? Validation: are we building the right system? Verification is thus a form of assessment that gauges how far a system or system component fulfills its functional specification, i Validation is a form of assessment that checks that a system actually meets genuine needs and requirements Validation ultimately comes down to user validation; namely that a system, the tasks it aims to perform, and the components implementing those tasks all contribute to meeting the needs of its intended users Having determined these, one way of subjecting a system to user validation is by carrying out a grid evaluation, varying environment variables alongside system parameters So one cannot have `pure' task validation, independent of any user requirements A technology has user validity if it can be deployed within some system meeting genuine user needs Shared tasks within project clusters could include, amongst others, market research and user requirement definitions, the establishment of user forums, data gathering, the testing and assessment of generic technologies, and the development of user validation guidelines What is usually meant by technology assessment here is the evaluation of mainly user-transparent tasks or components of a system, in a way that either abstracts away from (or more likely ignores) user-centered factors Not all sub-tasks or components within a system meet the criteria above for being a technology; some can be quite specific to a particular application or system Therefore, the term technology assessment can be somewhat misleading: (user-transparent) task assessment might be better terminology A task or technology has to be assessed in the light of environmental factors, and these will include factors ultimately stemming from system users To be sure, many previous efforts at technology assessment have proposed abstract measures (such as matching labelled bracketings, predicate argument structure, word error rates etc) and applied them to system components without regard to user issues For the assessment to enjoy proper validity, one also needs to match the results of these measures up against identified environment attributes for the task Technology validation is just as important as technology verification Internal assessment refers to user and technology verification and validation that is carried out solely with regard to the particular needs of an individual project Internal assessment will normally make use of evaluation data specific to the needs of the project: either the user needs, or the technological needs Assessment issues can be divided into those concerning user-centered assessment, and those concerning technology assessment Given that applications will vary from project to project, this user-specific data may not be directly amenable for use as comparative evaluation data Technological evaluation data will depend on the precise instances of technology used within the project So again, the technological data may not be directly amenable for use as comparative evaluation data Changes in environment might result from: different user groups, different domains, addition of further task requirements, and so on Assessment of these properties gauges how the present system compares to slight variants of itself, and is a form of comparative assessment Comparative assessment involves taking different systems and comparing either their system-wide performance, and/or the performance of individual components / task competencies User-centered assessment involves the testing the operational feasibility and functional adequacy of system, with regard to its intended user population, and to gauge user acceptance and the socio-economic impact of the system in a real-life situation More generally Environment: This has several sub-variables, applicable in different cases: User profile (system assessment) Overall surrounding task (component/sub-task assessment) Domain, text type, etc Channel conditions, speaker accent, etc Other non-linguistic factors Task: One can vary the task or system goals Task Attributes: These include Depth, accuracy, robustness and efficiency Grammars, lexicons, language models etc Task decomposition Implementation: Which covers such things as hardware platforms, operating systems, programming language, code, etc One can fix, vary or ignore combinations of these factors to get different kinds of evaluation, not all of which are strictly comparative Varying things at the system level, one can have, e ARPA: The ARPA MUC and TREC evaluations are characterised by having the system environment and task fixed, while varying task attributes and implementation Flexibility: For an individual system one can vary environment factors such as user profile, domain, etc to see how flexible a system is Usefulness: By fixing just the user profile, one can assess how useful various systems are for a particular user group Portability: e by varying the hardware platform, operating system A second question is what benefits such a comparative evaluation will bring in the near and/or long term to system users Briefly, the benefits of comparative evaluation are Cross-Fertilization: By comparing different systems, technologies and implementations, projects can learn from and exploit best practices adopted in other projects The core evaluation activity involves providing a system or system component with some input and comparing its output with the results expected But for all these modes of comparison, evaluation data is required Evaluation data comes in three forms 1 Test, or input data 2 Answer, or output data 3 Training data Training data may not always be required, but for any system employing statistical methods it is likely to be essential Very often, old input and output data can be used as training data Requirements on evaluation data are that it be realistic, and representative (Sparck Jones 1994  For test data to be realistic, it must be the kind of input data that the system or component would actually receive in real use A distinction should also be drawn between diagnostic data and adequacy data Diagnostic data may be set up very carefully to pinpoint particular points at which a system is failing In collecting evaluation data for assessment, a decision must be made about the level of: 1g do we evaluate the system against data from different languages, different domains etc  This section addresses user-centered assessment As such it is primarily, though not exclusively, concerned with project internal evaluation; the next section on technology assessment takes up the issue of comparative assessment One of the primary aims of technology assessment is to provide the means to abstract away from individual users, and to gauge the applicability of systems or system components to other types of user One can look at it on the basis of different types of data: Factual, objective data about users These determine the implied needs of the users Two very broad system attributes are usability and integratability The trigger papers, and one reaction paper, are Sparck Jones Crouch: General Technology Assessment Netter: Technology Assessment for Written NL Applications Steeneken: Speech Technology Assessment Adriaens: User-Centered Assessment for RTD in Language Engineering King: Reactions to G is an important attribute that pervades all stages of the development cycle of a system or system increment integration in a complex user environment alpha test (system or system increment test by other people than the developers, typically prospective end users, possibly still in a controlled environment) 4 acceptance test (a formal test initiated by the prospective procurer/buyer of a system or system increment, typically at the installation site; for custom-built solutions, this is a final test in the test chain for a system or system increment) 5 Regression testing presupposes a rigid approach in which test data are well-defined, and evaluation tools exist to determine changes from one test run to another However good the underlying system, a poor user interface can make it practically unusable But most of the reasons for pursuing comparative evaluation do persist at the user level: e One possible mode of user-centered comparative evaluation would be to try the user group from one project out on a system from another project having a broadly similar, though not necessarily identical, functionality Another mode of evaluation would be to exchange subject domains between projects in the cluster (assuming that suitable evaluation data can be made available to assess the flexibility and portability of systems The problem with user-centered comparative evaluation comes down to the fact that user groups may simply be too heterogeneous to permit much meaningful comparison This being so, there is a danger that user groups within projects will be resistant to this form of comparative evaluation What could be distilled from these data are abstractions from comparable evaluation scenarios, evaluation metrics etc In the medium and long run this could result in a kind of evaluation library containing user models, abstract scenarios, evaluation methods, test suites, tools, metrics etc which have been successfully applied in user validation However, if one looks at the structure of different systems, one usually finds that they have tasks and components in common Of course the same task/component in two different systems might have quite different attributes in terms of user-visibility, depth, accuracy, robustness, efficiency, language, text-type, etc etc task and sub-task performance) is a natural way of pursuing comparative evaluation The second discusses user-centered assessment, followed by an extensive discussion of technology assessment `Pure' technology evaluation metrics require user-centered validation User-centered validation of technology evaluation metrics has received scant attention In section 5 some recommendations are given about the kind of organisation and infrastructure required for carrying out user-centered and technology assessment, both at a project internal level and for the purposes of comparative evaluation If user-centered validation is to be taken seriously, this means that the initial stages of any comparative technology evaluation exercise would need to be devoted to it On the assumption that technology assessment can be carried out using standardised test data, users and systems designers can identify appropriate technological tools without first having to perform user-specific evaluation, where the collection of test data is likely to be expensive Comparative assessment is of potential value to user groups because it promotes: Cross-fertilization One of the bye-products of the ARPA MUC and TREC comparative evaluations has been cross-fertilization between different systems Comparative evaluation means that funding user groups can be more confident that the most appropriate components have been employed to perform various sub-tasks A properly set up comparative evaluation can do this, since components of a given system will be run on data from different domains, and where the data arise in response to different sets of user requirements Additionally Comparative assessment involves a degree of abstraction away from specific tasks, domains and user groups Some kind of general technology assessment is required to achieve this abstraction Performing meaningful technology assessment entails the identification of relevant environment and user attributes This involves proposing general technology metrics, applying them to individual systems, and comparing the results with user-centered assessment of the systems One possibility would be to set up a small ARPA-like evaluation project under FP-4 However, this would (a) exclude remaining FP-4 projects from comparative evaluation, and (b) does not sit well with emphasis on users in FP-4 the evaluation task would doubtless involve a large degree of artificiality Another way of tackling things would be to build an evaluation exercise bottom-up from existing FP-4 projects, hopefully building on the work done for internal project evaluation The evaluation should address tasks with multilingual and multimodal (spoken and written language) aspects, falling within the areas covered by approved LE projects A language engineering application system or just LE system is a set of software components constructed to permit a user to carry out some language-related task or function in a specific real-world environment corpora and, more importantly, the evaluation data with `answers' for chosen tasks The evaluation structure should allow both technological and user-centered evaluation As far as possible the comparative evaluation exercise should sit on top of, and make use of, project internal evaluation Different overall tasks will often have overlapping sub-tasks These evaluation points allow for a variety of different kinds of evaluation When a sub-task is user significant, then user-centered evaluation metrics can be applied Generally, we can talk of a system or system component as implementing a task The tasks define competences to be evaluated and not system components; it is possible for a task to be distributed across several components, and also for several tasks to make use of the same system component A braided evaluation structure allows for comparative and individual evaluation of different systems at different levels (user-centered, task-specific, general technology  While every attempt should be made to bring project internal and comparative evaluation as close together as possible, it would be unrealistic to expect a comparative evaluation to run itself, with the only impetus coming from within individual projects Task specific assessment of technologies will thus be feasible only if there is a large enough number of systems performing the same tasks Evaluation data has to be chosen that is sufficiently representative for the task or application to be assessed To our knowledge, none of these test suites have been applied to evaluation as opposed to diagnosis; test suites are generally designed to meet criteria other than user-centered evaluation From one point of view, say that of someone designing a modular language analyser, the system still performs these separate tasks Parallel test corpora in different languages with comparable annotations could become relevant, if portability of systems from one language to another is to be assessed But one could well imagine the connectionist system designer arguing that these do not correspond to any genuine tasks, as witnessed by the lack of any separate system components But in any case, more than corpora will be required to support comparative evaluation using common data Any project that tackles internal evaluation seriously is likely to build up evaluation data in the form of layered corpora, lexicons, terminological databases, databases, etc, as described above If at all possible, it makes sense to build on this kind of test data to provide material for comparative assessment For example, a mono-lingual information access system for airline reservations simply will not provide the kind of test data required for evaluating the multi-lingual aspects of a multi-lingual access system for airline reservations Test data provided by individual projects is thus a starting point for building up shared evaluation resources, but it has to be built upon It needs to be extended to cover tasks not relevant to the system from which the data originated Thus, identifying task structure rather than system architecture is the first step towards defining an evaluation framework One needs to retain flexibility and room for development in what should constitute common test data Standards for technology assessment of written NL applications are only gradually emerging System architecture will of course normally provide much useful information about the appropriate task structures to employ What appear to be the main limitations of current evaluation methods, if viewed from the point of technology assessment are the following aspects Many of the performance evaluation methods, such as employed in MUC and TREC, are task specific However, it will provide material for a task specific evaluation, which e This task-specific evaluation needs to be treated with care What counts as the correct index terms may very well depend on the nature of the retrieval system carrying out Task 2 So direct, task-specific comparisons between systems employing different retrieval systems may very well not be possible The pair L-OBJ 1 and L-OBJ 2 provides material for evaluation of the retrieval system One could envisage user, task and general technology measures being applied to Task 3 The coarse task decomposition above provides little space for technology assessment Since most of the tasks on the path (1 5) correspond to standard linguistic functionalities, one can apply general technology assessment measures, e However, different systems may perform these tasks to different depths of analysis and levels of detail Own-language retrieval would be done only to provide evaluation data In relation to speech processing evaluation in particular, various speech input conditions are possible for Task 1, and there are also speech input and output possibilities at Task 3 Since the illustrative evaluation scenario above is somewhat biassed towards text processing, this section raises some issues more specific to the evaluation of spoken language systems Three topics of particular interest in the evaluation of the present state-of-the-art speech technology: user appreciation of spoken language systems; assessment of technology modules; and the interaction between spoken language and natural language systems This requires a continuous stream of new test data The major issue is total system performance and error correction Whether a task is user-visible or transparent can depend on the user as much as on the task This adaptation requires specific assessment methods Total system performance (does the user get the required information efficiently user appreciation and the performance of different technologies are main issues for assessment The assessment is very much application related and generally requires specific data bases Multilingual use requires a variety of identical (language specific) data bases Total system performance : benefit vs human performance successful trials handling time error analysis ease of use Total system performance quantifies the technical success of a system (does it work well  Additionally the benefit of a system versus human performance (do we really need such a system) are items to include Human performance also offers a bench mark for system evaluation Technology assessment: Progress during development, competitive comparison and progress estimation are normally quantified within a specific application or technology (e Interaction between a human and a system may fail Specific requirements of a certain application may lead to a different assessment methods, and at least to different metrics and criteria Therefore, it is impossible to give detailed examples of possible assessment projects In general a robust evaluation experiment should include experiments on various items of user appreciation, system performance or technology Three examples are given: Application oriented: The evaluation of a system using speech input and speech output in a dialogue concept, such as used for a travel information system, allows for the assessment of the total system (user appreciation and system performance  Also the assessment of individual modules may be relevant, consider the robustness of the recognizer for telephone quality speech and the intelligibility of the (text-to speech output system However, for application oriented assessments one can use representative evaluation data where variables are uncontrolled but more or less representative for the application Also selective analysis of the results (individual module responses) may produce diagnostic information on the performance of the system A competitive project in which various systems are compared will push the technology Combination of Spoken Language and Natural Language Projects: In general SL and NLP use different assessment methods and scoring metrics However, the use of common data bases for testing of a combined system (eg a translation system with speech input and/or output, a spelling checker with voice control, or a information system with an interpreter) may be a useful first step The role of project internal technology assessment is likely to be primarily one of producing diagnostic information This can be used to facilitate progress in system development, by identifying gaps in the technology that need to be filled in order to improve system performance Internal technology assessment may or may not form part of progress evaluation, which would in any case require a large element of user-centered assessment To a large degree, internal technology assessment is a matter of internal project policy Technological assessment is appropriate where a project envisages either (i) refining or developing speech and/or language processing techniques, or (ii) adapting data such as grammars, language models, lexicons for a particular domain, task or language Namely, (a) that the form of project internal technology evaluation can vary from project to project, and (b) that technology evaluation provides the core of comparative assessment The second claim would seem to demand uniformity between projects in technology evaluation, while the first denies it This suggests that internal and comparative technology assessment are very different kinds of animal But what one would like is for comparative technology assessment to build on the back of project internal assessment But if the standard components can be identified and kept separable, then project internal test data may, with appropriate extra effort, be re-usable for comparative purposes Another obvious point to make is that if an evolutionary system development cycle is followed, then snapshots of a system at different times can profitably be regarded, for the purposes of comparative evaluation as different systems for the same domain, task and user group Thus, in order to facilitate comparison and reusability of evaluation data, a limited degree of standardisation is required for project internal technology assessment The overwhelming majority of respondents expressed keenness to conduct both sorts of technology assessment (some comments indicated a confusion between project-internal technology assessment, and other forms of project internal assessment, such as simple progress evaluation - the questionnaire may have failed to make this distinction sufficiently clearly  Again, the overwhelming response was in favour of quantitative technology assessment, independently of what user assessment might or might not take place The final subpart asked whether in project-internal assessment systems should be assessed against test data that differed in various linguistic features (e The third multipart question asked about implementing project-internal technology assessment However, there was almost an even split for and against the evaluation data collectors being members of projects The first subpart asked for preferences concerning an integrated speech and language comparative evaluation exercise versus separate speech and language evaluations projects themselves should carry out project internal technology assessment, though there may be a case for having evaluation data assembled by independent `experts 3 While technology evaluation must form the core of the exercise, user-centered issues may readily be catered for by paying due attention to the environmental aspects of technology assessment A flexible evaluation structure capable of accommodating a variety of tasks and systems, and with an emphasis on environmental / user-centered validation of technology measures should be employed The profile of expertise of this group should reflect both the needs of technology assessment and of user validation To a large degree, methods of project internal evaluation will be a matter for negotiation between users and system developers within individual projects However, there are a number of points that are not only desirable for internal evaluation, but which would also facilitate comparative evaluation: Projects should be encouraged to develop a well defined, evolutionary evaluation strategy Early acquisition (and use) of evaluation data should be encouraged Wherever possible, projects should be encouraged to make their internal evaluation strategies, test data, user profiles etc Section amplifies on some of these points, and suggests some practices for project internal evaluation The comparative evaluation exercise needs to be set up in a way that is part top-down and part bottom-up For LE tasks, linguistic features that can vary (for different tasks) include: The language of the input or output (e Natural evaluation points in the braided structure should be identified Appropriate corpus annotations for the selected evaluation points should be specified Projects whose task structure includes a particular evaluation point are expected to employ the annotation relevant to that point as part of their internal assessment regime (They may of course also apply additional assessment measures  One should aim to exploit existing tools where possible, and otherwise distribute development effort over different projects and the evaluation coordinators Using these tools, individual projects should produce annotated answer data from their own evaluation corpora Individual projects should make available as much additional linguistic and non-linguistic data pertinent to evaluation as possible This involves correlating metric scores with the adequacy or inadequacy of task performance under the different environmental attributes imposed by different systems This would allow comparison of technology measures and system adequacy under relatively fixed environmental constraints Even a small comparative evaluation exercise cannot be expected to be self-running and self-regulating QUESTIONNAIRE ON ASSESSMENT AND EVALUATION IN LANGUAGE ENGINEERING 1uni-sb user validation; and 20 Questionnaire 21 Introduction and Terminology Terms later used in the specific sense introduced here are capitalised spontaneous speech channel conditions (telephone vs Example To make these distinctions clearer consider this example The linguistic functions of the overall system, and those which define user-significant components, may be described as transcription and translation Relevant linguistic features of the input data include: English language, single speaker, read speech, limited domain, short passages, formulaic style; relevant linguistic features of the output are French language, formulaic style, limited domain seeing if the techniques employed may be generalised other languages, other sorts of brief report, etc  Would be willing to conduct it 4 Maybe you should have an option 5 For more theoretical work its more problematic since you might not know what to measure, and it may change as the project progresses Would be willing to participate 4 4: We already actively participate in common comparative evaluations Not convinced how useful it would be, but that's not what you asked comparative assessment is not going to divert substantial resources from internal assessment and development; 2 that comparative assessment is likely to bring about project internal improvements, in the same way that internal evaluation should; 3 4: Concern that too much research time would be directed towards setting up evaluations (but this had good results in the ARPA community) 2 whatever validation the users deem sufficient) ? 2 2: qualitative assessment is not enough Once you can do that, the considerations change I actually think it may be dangerous to place too much emphasis on UV, because depending on users' existing preconceptions introduces a huge inertia into technological development Test data at least provides some moves in this direction 2: User validation alone won't promote generality and reusability of components at the level of user-significant components only 2 Therefore technological assessment levels are undetournable For semi-objective UV it only makes to look a user-significant components 2: User validation alone won't promote generality and reusability of components only on unseen input-output pairings whose linguistic features match those of the project application 2 2: This answer needs to be qualified This reflects a system's maintainability, portability and flexibility internally by project participants (developers and users) ? 2 2: but these specialists must do so in close coordination with the partners to ensure compatibility/applicability 1: See previous comment3 above Depends on cases again 2 will involve a lot of organising and doesn't make sense for every project