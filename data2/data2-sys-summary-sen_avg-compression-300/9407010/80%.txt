 Recognizer score + linguistic KSs 3 Two variants of the highest-in-coverage method provided a lower limit: the straight method, and one in which the hypotheses were first rescored using the optimized combination of recognizer score and N-gram discriminant KSs Table shows the sentence error rates for different preference methods and utterance lengths, using 10-best lists; Table shows the word error rates for each method on the full set The word error rate decreased from 7 It is apparent that nearly all of the improvement is coming from the linguistic KSs; the difference between the lines recognizer + linguistic KSs and all available KSs is not significant On either type of evaluation, the difference between all available KSs and any other method except recognizer + linguistic KSs is significant at the 5% level according to the McNemar sign test  This led to results which were both better and also qualitatively different; the N-gram KSs made a much larger contribution, and appeared to dominate the linguistic KSs More commonly, however, the KS will produce a list of one or more linguistic items associated with H, for example surface N-grams in H or the grammar rules occurring in the best linguistic analysis of H, if there was one A given linguistic item L is associated with a numerical score through a discrimination function (one function for each type of linguistic item that summarizes the relative frequencies of occurrence of L in correct and incorrect hypotheses respectively The intent is that linguistic items which tend to occur more frequently in correct hypotheses than incorrect ones will get positive scores; those which occur more frequently in incorrect hypotheses than correct ones will get negative scores We now define formally the discrimination function dT for a given type T of linguistic item We start by defining dT as a function on linguistic items dT(L) for a given linguistic item L is computed as follows The language processor brings more sophisticated linguistic knowledge sources to bear, typically some form of syntactic and/or semantic analysis, and uses them to choose the most plausible member of the N-best list The training corpus is analyzed, and each hypothesis is tagged with its set of associated linguistic items The recognizer used a class bigram language model Each N-best hypothesis received a numerical plausibility score; only the top 10 hypotheses were retained The 1-best sentence error rate was about 34 the 5-best error rate (i The CLE normally assigns a hypothesis several different possible linguistic analyses, scoring each one with a plausibility measure Only the most plausible linguistic analysis was used2% of all hypotheses timed out during linguistic analysis; the average analysis time required per hypothesis was 2 The following knowledge sources were used in the experiments: Recognizer score: The numerical score assigned to each hypothesis by the DECIPHER (TM) recognizer In coverage: Whether or not the CLE assigned the hypothesis a linguistic analysis (1 or 0  Unlikely grammar construction: 1 if the most plausible linguistic analysis assigned to the hypothesis by the CLE was unlikely 0 otherwise Class N-gram discriminants (four distinct knowledge sources Discrimination scores for 1 2 3- and 4-grams of classes of surface linguistic items Grammar rule discriminants: Discrimination scores for the grammar rules used in the most plausible linguistic analysis of the hypothesis, if there was one Semantic triple discriminants: Discrimination scores for semantic triples in the most plausible linguistic analysis of the hypothesis, if there was one The first is the singleton consisting of the recognizer score KS; the second contains the four class N-gram discriminant KSs; the third consists of the remaining linguistic KSs Recognizer score + class N-gram discriminant KSs 2