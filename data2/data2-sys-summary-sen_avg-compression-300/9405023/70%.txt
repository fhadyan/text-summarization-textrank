 Thus, although somewhat overly harsh, the heuristic is quite effective in identifying bad parses With our integrated heuristic scheme, good/close parses were returned in 30 sentences (6 additional sentences  In cases where the complete input sentence is not covered by the grammar, the parser attempts to find a maximal subset of the input that is parsable In many cases, such a parse can serve as a good approximation to the true parse of the sentence Since the parser is LR(0 previous reduce operations remain valid even when words further along in the input are skipped Information about skipped words is maintained in the symbol nodes that represent parse sub-trees To guarantee runtime feasibility, the GLR* parser is coupled with a beam search heuristic, that dynamically restricts the skipping capability of the parser, so as to focus on parses of maximal and close to maximal substrings of the input A second data structure uses pointers to keep track of all possible parse trees throughout the parsing of the input, while sharing common subtrees of these different parses At the end of the process of parsing a sentence, the GLR* parser returns with a set of possible parses, each corresponding to some grammatical subset of words of the input sentence However, in many cases there are several distinct maximal parses, each consisting of a different subset of words of the original sentence We use features of both the candidate parse and the ignored parts of the original input sentence For each parse, the heuristic computes a penalty score for each of the features The parser then selects the parse ranked best (i the parse of lowest overall score  The number and position of skipped words 2 The number of substituted words 3 The fragmentation of the parse analysis 4 The statistical score of the disambiguated parse tree The penalty scheme for skipped words is designed to prefer parses that correspond to fewer skipped words95 - 105) for each word of the original sentence that was skipped This preference was designed to handle the phenomena of false starts, which is common in spontaneous speech The GLR* parser has a capability for handling common word substitutions when the parser's input string is the output of a speech recognition system We have recently augmented the parser with a statistical disambiguation module Training of the probabilities is performed on a set of disambiguated parses The probabilities of the parse actions induce statistical scores on alternative parse trees, which are used for disambiguation However, additionally, we use the statistical score of the disambiguated parse as an additional evaluation feature across parses However, we plan on investigating the possibility of using some known optimization techniques for this task The utility of a parser such as GLR* obviously depends on the semantic coherency of the parse results that it returns Similar to the penalty score scheme, the precise thresholds are currently fine tuned to try and optimize the classification results on a training set of data A list of common appearing substitutions was constructed from the development set95 and 105, depending on the word's position in the sentence The result of parsing an input sentence consists of both a parse tree and the computed feature structure associated with the non-terminal at the root of the tree Our goal was three-fold Second, we wished to test the effectiveness of our evaluation heuristics in selecting the best parse The first run was with skipping disabled The second run was conducted with skipping enabled and full heuristics The third run was conducted with skipping enabled, and with a simple heuristic that prefers parses based only on the number of words skipped The results of the experiment can be seen in Table  The results indicate that using the GLR* parser results in a significant improvement in performance Fully 96% of the test sentences (all but 5) are parsable by GLR 