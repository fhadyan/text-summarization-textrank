 There is a probability distribution across the bins representing how instances fall into bins Second, since training data is limited, the learner may not have sufficient data available to acquire accurate rules We are interested in estimating the accuracy for various volumes of training data The most severe result of insufficient training data is that some bins can go without any training instances For each bin, the probability that no training instances fall into it is: I will call such bins EMPTY BINS The inequality holds for all  When bins are uniformly probable, the expected number of training instances in the same bin as a random test instance is (  When this is true the expected number of training instances in the same bin as a random test instance is approximately (  These simulations use a fixed number of bins (10,000 allocating m training instances to the bins according to either a uniform or logarithmic distribution Figure shows five traces of accuracy as the volume of training data is varied In particular, an average of four training instances per bin can be expected to yield an error rate only 50% worse than the optimal error rate Error rates only 50% worse than optimal result from only three training instances per bin