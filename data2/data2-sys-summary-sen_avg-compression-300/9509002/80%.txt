 There is a probability distribution across the bins representing how instances fall into bins Also, for each bin, there is a probability distribution across the set of values representing how instances in that bin take on values Second, since training data is limited, the learner may not have sufficient data available to acquire accurate rules We are interested in estimating the accuracy for various volumes of training data Since the optimal error rate is independent of the amount of training data, it will always exist no matter how much data is used As the amount of training data increases we expect the accuracy to get closer to this optimal If we denote the most likely value in each bin as , then the expected value of the optimal accuracy is determined by the likelihood of this value occurring in each bin The most severe result of insufficient training data is that some bins can go without any training instances For each bin, the probability that no training instances fall into it is: I will call such bins EMPTY BINS In Lauer (1995) it is shown that for any bin b: Lauer (1995) also bounds the expected accuracy of the mode-based learner when all bins are guaranteed to have at least one training instance Over non-empty bins, we know that the error rate is no worse than twice the optimal error rate for those bins The third step follows from equation (  The assumption that bin probabilities are uniform is problematic When bins are uniformly probable, the expected number of training instances in the same bin as a random test instance is (  When this is true the expected number of training instances in the same bin as a random test instance is approximately (  These simulations use a fixed number of bins (10,000 allocating m training instances to the bins according to either a uniform or logarithmic distribution Figure shows five traces of accuracy as the volume of training data is varied However, when the bins are logarithmically distributed learning converges significantly more quickly, as suggested by the reasoning about expected number of relevant training instances (see section  In this paper I have explored the dependence of the expected accuracy of a simple statistical learner on the volume of training data In particular, an average of four training instances per bin can be expected to yield an error rate only 50% worse than the optimal error rate Error rates only 50% worse than optimal result from only three training instances per bin