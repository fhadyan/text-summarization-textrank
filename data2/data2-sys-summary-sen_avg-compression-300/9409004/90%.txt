 In our case, we define precision as the proportion of triples appearing in syntactic positions with acquired SRs, which effectively fulfill one of those SRs8% triples didn't belong to any of the classes induced for their syntactic positions Noise The class accumulates enough evidence provided by erroneously extracted triples Almost one correct semantic class for each syntactic position in the sample is acquired Since different verb senses occur in the corpus, the SRs acquired appear mixed In this way, the syntactic position also would provide information (statistical evidence) for measuring the most appropriate classes Therefore, once disambiguated the verb senses it would be possible to split the set of SRs acquired Output The result of the learning process is a set of syntactic SRs, (verb, syntactic relationship, semantic class  Semantic classes are represented extensionally as sets of nouns SRs are only acquired if there are enough cases in the corpus as to gather statistical evidence Previous knowledge used In the process of learning SRs, the system needs to know how words are clustered in semantic classes, and how semantic classes are hierarchically organized Ambiguous words must be represented as having different hyperonym classes Learning process The computational process is divided in three stages: (1) Guessing the possible semantic classes, i Furthermore, SRs may help the parser when deciding the semantic role played by a syntactic complement Let be the sets of all verbs, nouns, syntactic positions, and possible noun classes, respectively In different iterations over these candidate classes, two operations are performed: first, the class, c, having the best Assoc (best class is extracted for the final result; and second, the remaining candidate classes are filtered from classes being hyper/hyponyms to the best class We used Wordnet as the verb and noun lexicons for the lemmatizer, and also as the semantic taxonomy for clustering nouns in semantic classes 113,583 (93