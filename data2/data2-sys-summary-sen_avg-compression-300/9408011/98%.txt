 Unsurprisingly, the training set relative entropy decreases monotonically To cluster nouns n according to their conditional verb distributions pn, we need a measure of similarity between distributions First, for fixed average distortion between the cluster centroid distributions p(v|c) and the data p(v|n we find the cluster membership probabilities, which are the Bayes's inverses of the p(n|c that maximize the entropy of the cluster distributions We say then that the original cluster has split into the two new clusters