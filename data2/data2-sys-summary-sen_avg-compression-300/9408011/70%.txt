 As explained there, our clustering procedure yields for each value of a set of clusters minimizing the free energy F, and the asymmetric model for estimates the conditional verb distribution for a noun n by where p(c|n) also depends on  Figure shows the five words most similar to the each cluster centroid for the four clusters resulting from the first two cluster splits It can be seen that first split separates the objects corresponding to the weaponry sense of fire (cluster 1) from the ones corresponding to the personnel action (cluster 2  The second split then further refines the weaponry sense into a projectile sense (cluster 3) and a gun sense (cluster 4  So, far, we have looked at two kinds of measurements of model quality: (i) relative entropy between held-out data and the asymmetric model, and (ii) performance on the task of deciding which of two verbs is more likely to take a given noun as direct object when the data relating one of the verbs to the noun has been witheld from the training data We selected then the subset involving the 1000 most frequent nouns in the corpus for clustering, and randomly divided it into a training set of 756721 pairs and a test set of 81240 pairs Figure plots the average relative entropy of several data sets to asymmetric clustered models of different sizes, given by where tn is the relative frequency distribution of verbs taking nas direct object in the test set For each critical value of , we show the relative entropy with respect to the asymmetric model based on of the training set (set train of randomly selected held-out test set (set test and of held-out data for a further 1000 nouns that were not clustered (set new  Unsurprisingly, the training set relative entropy decreases monotonically More specifically, we model senses as probabilistic concepts or clusters c with corresponding cluster membership probabilities p(c|w) for each word w The test set relative entropy decreases to a minimum at 206 clusters, and then starts increasing, suggesting that larger models are overtrained The new noun test set is intended to test whether clusters based on the 1000 most frequent nouns are useful classifiers for the selectional properties of nouns in general As the figure shows, the cluster model provides over one bit of information about the selectional properties of the new nouns, but the overtraining effect is even sharper than for the held-out data involving the 1000 clustered nouns We also evaluated asymmetric cluster models on a verb decision task closer to possible applications to disambiguation in language analysis Thus this test evaluates how well the models reconstruct missing data in the verb distribution for n from the cluster centroids close to n The resulting training set was used to build a sequence of cluster models as before Each model was used to decide which of two verbs v and v' are more likely to appear with a noun n where the (v,n) data was deleted from the training set, and the decisions compared with the corresponding ones derived from the original event frequencies in the initial data set We would like to thank Don Hindle for making available the 1988 Associated Press verb-object data set, the Fidditch parser and a verb-object structure filter, Mats Rooth for selecting the objects of fire data set and many discussions, David Yarowsky for help with his stemming and concordancing tools, and Ido Dagan for suggesting ways of testing cluster models Our approach avoids both problems We will consider here only the problem of classifying nouns according to their distribution as direct objects of verbs; the converse problem is formally similar Our classification method will construct a set of clusters and cluster membership probabilities p(c|n  Each cluster c is associated to a cluster centroid pc, which is discrete density over obtained by averaging appropriately the pn To cluster nouns n according to their conditional verb distributions pn, we need a measure of similarity between distributions We can thus use the relative entropy between the context distributions for two words to measure how likely they are to be instances of the same cluster centroid Finally, relative entropy is a natural measure of similarity between distributions for clustering because its minimization leads to cluster centroids that are a simple weighted average of member distributions It turns out that the problem is avoided by our clustering technique, since it does not need to compute the KL distance between individual word distributions, but only between a word distribution and average distributions, the current cluster centroids, which are guaranteed to be nonzero whenever the word distributions are In particular, we would like to find a set of clusters such that each conditional distribution pn(v) can be approximately decomposed as where p(c | n ) is the membership probability of n in c and pc(v) = p(v|c) is v's conditional probability given by the centroid distribution for cluster c To determine this decomposition we need to solve the two connected problems of finding find suitable forms for the cluster membership and centroid distributions p(v|c and of maximizing the goodness of fit between the model distribution and the observed data Goodness of fit is determined by the model's likelihood of the observations As for the membership probabilities, they must be determined solely by the relevant measure of object-to-cluster similarity, which in the present work is the relative entropy between object and cluster centroid distributions Since no other information is available, the membership is determined by maximizing the configuration entropy subject for a fixed average distortion With the maximum entropy (ME) membership distribution, ML estimation is equivalent to the minimization of the average distortion of the data For instance, one may estimate the likelihood of a particular direct object for a verb from the likelihoods of that direct object for similar verbs The first stage of an iteration is a maximum likelihood, or minimum distortion, estimation of the cluster centroids given fixed membership probabilities In the second iteration stage, the entropy of the membership distribution is maximized with a fixed average distortion In principle such a symmetric model may be more accurate, but in this paper we will concentrate on asymmetric models in which cluster memberships are associated to just one of the components of the joint distribution and the cluster centroids are specified only by the other component In particular, the model we use in our experiments has noun clusters with cluster memberships determined by p(n|c) and centroid distributions determined by p(v|c  First, for fixed average distortion between the cluster centroid distributions p(v|c) and the data p(v|n we find the cluster membership probabilities, which are the Bayes's inverses of the p(n|c that maximize the entropy of the cluster distributions It turns out that this will also be the values of p(v|c) that minimize the average distortion between the asymmetric cluster model and the data Given any similarity measure d(n,c) between nouns and cluster centroids, the average cluster distortion is If we maximize the cluster membership entropy subject to normalization of p(n|c) and fixed ( we obtain the following standard exponential forms for the class and membership distributions where the normalization sums (partition functions) are and  We say then that the original cluster has split into the two new clusters We start with very low and a single cluster whose centroid is the average of all noun distributions For any given , we have a current set of leaf clusters corresponding to the current free energy (local) minimum