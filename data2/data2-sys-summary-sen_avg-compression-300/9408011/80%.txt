 Figure shows the five words most similar to the each cluster centroid for the four clusters resulting from the first two cluster splits So, far, we have looked at two kinds of measurements of model quality: (i) relative entropy between held-out data and the asymmetric model, and (ii) performance on the task of deciding which of two verbs is more likely to take a given noun as direct object when the data relating one of the verbs to the noun has been witheld from the training data We selected then the subset involving the 1000 most frequent nouns in the corpus for clustering, and randomly divided it into a training set of 756721 pairs and a test set of 81240 pairs Figure plots the average relative entropy of several data sets to asymmetric clustered models of different sizes, given by where tn is the relative frequency distribution of verbs taking nas direct object in the test set For each critical value of , we show the relative entropy with respect to the asymmetric model based on of the training set (set train of randomly selected held-out test set (set test and of held-out data for a further 1000 nouns that were not clustered (set new  Unsurprisingly, the training set relative entropy decreases monotonically More specifically, we model senses as probabilistic concepts or clusters c with corresponding cluster membership probabilities p(c|w) for each word w The test set relative entropy decreases to a minimum at 206 clusters, and then starts increasing, suggesting that larger models are overtrained The new noun test set is intended to test whether clusters based on the 1000 most frequent nouns are useful classifiers for the selectional properties of nouns in general We also evaluated asymmetric cluster models on a verb decision task closer to possible applications to disambiguation in language analysis Thus this test evaluates how well the models reconstruct missing data in the verb distribution for n from the cluster centroids close to n The resulting training set was used to build a sequence of cluster models as before Our approach avoids both problems Our classification method will construct a set of clusters and cluster membership probabilities p(c|n  Each cluster c is associated to a cluster centroid pc, which is discrete density over obtained by averaging appropriately the pn To cluster nouns n according to their conditional verb distributions pn, we need a measure of similarity between distributions We can thus use the relative entropy between the context distributions for two words to measure how likely they are to be instances of the same cluster centroid Finally, relative entropy is a natural measure of similarity between distributions for clustering because its minimization leads to cluster centroids that are a simple weighted average of member distributions It turns out that the problem is avoided by our clustering technique, since it does not need to compute the KL distance between individual word distributions, but only between a word distribution and average distributions, the current cluster centroids, which are guaranteed to be nonzero whenever the word distributions are To determine this decomposition we need to solve the two connected problems of finding find suitable forms for the cluster membership and centroid distributions p(v|c and of maximizing the goodness of fit between the model distribution and the observed data Goodness of fit is determined by the model's likelihood of the observations As for the membership probabilities, they must be determined solely by the relevant measure of object-to-cluster similarity, which in the present work is the relative entropy between object and cluster centroid distributions With the maximum entropy (ME) membership distribution, ML estimation is equivalent to the minimization of the average distortion of the data The first stage of an iteration is a maximum likelihood, or minimum distortion, estimation of the cluster centroids given fixed membership probabilities In the second iteration stage, the entropy of the membership distribution is maximized with a fixed average distortion In particular, the model we use in our experiments has noun clusters with cluster memberships determined by p(n|c) and centroid distributions determined by p(v|c  First, for fixed average distortion between the cluster centroid distributions p(v|c) and the data p(v|n we find the cluster membership probabilities, which are the Bayes's inverses of the p(n|c that maximize the entropy of the cluster distributions It turns out that this will also be the values of p(v|c) that minimize the average distortion between the asymmetric cluster model and the data Given any similarity measure d(n,c) between nouns and cluster centroids, the average cluster distortion is If we maximize the cluster membership entropy subject to normalization of p(n|c) and fixed ( we obtain the following standard exponential forms for the class and membership distributions where the normalization sums (partition functions) are and  The free energy determines both the distortion and the membership entropy through with temperature  We say then that the original cluster has split into the two new clusters We start with very low and a single cluster whose centroid is the average of all noun distributions