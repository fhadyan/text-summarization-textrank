 The Xerox Tagger HREF= 9505035 Thus, the Lancaster-Oslo/Bergen (LOB) Corpus distinguishes about 135 tags, and the Lancaster UCREL group uses a set of 166 tags (for CLAWS2 HREF= 9505035 Other tagsets are even larger, as the one used in the London-Lund Corpus of Spoken English, which contains 197 tags As a consequence of this combination of techniques, present excelent results tagging an English corpus with the Xerox Tagger and the constraint-based system ENGCG HREF= 9505035 It contains 479 POS tags (there are also special tags for punctuation signs  HREF= 9505035html#Cutting92 &gt;Cutting et al 1992 for instance, report on tagging results on even numbered sentences of the Brown corpus using a 50,000 forms lexicon With this lexicon and the suffix file, no unknown forms were encountered in the training process, thus providing no training data for forms assigned the open class However, a greater lexicon does not necessarily guarantee a better tagging accuracy Hence, the whole set of tags for each word has been taken into account during lexicon building This lexicon is being used in the actual tagging of the ITU corpus, since it provides a more accurate model to lexical ambiguity than that provided by suffix information alone Training on hidden Markov models of language is performed without a tagged corpus Two such ways implemented in the Xerox Tagger, concerning ambiguity classes and state transitions, are described below: The biasing facts on ambiguity classes are called symbol biases This way, ambiguity classes are annotated with favoured tags The Xerox Tagger uses a statistical method for text tagging The choice of the training corpus affects the result Finally, the choice of the training corpus has consequences on the accuracy of the system The full 1M word subset of the corpus being postedited has been used as the training corpus, leaving file SP_itu_corpus_000 as the test corpus This corpus contains 9,366 tagged tokens The corpus has been used in an incremental way, testing results with each partial model obtained Two types of training have been used with this model1% better accuracy than the full tagset Table 1 shows the behaviour of the system when tagging the test corpus with the full tagset The first one makes use of a tagged training corpus continuous invariant multi-word units  This model is then used to tag more text; the tags are manually corrected and subsequently used to retrain the model The second method does not require a tagged training corpus present, using a corpus different to that used for training, results sensibly better for French (96 While use a tagset consisting of 88 tags and a tagset with 42 tags, our tagset has 466 different tags uses this method for training a text tagger The number of equivalence classes (referred to as ambiguity classes in HREF= 9505035 It uses ambiguity classes and a first-order model to reduce the number of parameters to be estimated without significant reduction in accuracy Besides, relatively few ambiguity classes are sufficient for wide coverage, so it is unlikely that adding new words to the lexicon requires retraining, as their ambiguity classes are accomodated Words not found in the lexicon are assigned an ambiguity class according to both context and suffix information The set of tags identifies an ambiguity class, which is also delivered by the lexicon The training module takes long sequences of ambiguity classes as input The tagging module buffers sequences of ambiguity classes between sentence boundaries A language-specific method can be employed to guess ambiguity classes for these unknown words This function also operates on an untagged training corpus As a final stage, words not found in the lexicon and ending in a suffix not recognized are assigned a default ambiguity class (open class  As already mentioned in the previous section, in case a word is unknown to the system, `suffix' information can be used in order to approximate its possible ambiguity class The ambiguity class to be assigned to each suffix is selected from the set of classes computed during normal training, which is written to a classes file This file contains (i) every tag observed in the lexicon (which is, obviously, unambiguous (ii) every set of ambiguously assigned tags for every form in the lexicon, and (iii) the ambiguity class for the open class (a default class  The above-mentioned function, after computing a suffix, observes words in the lexicon ending in the proposed suffix and the set of tags assigned to them It then eliminates those tags not included in the ambiguity class for the open class and, afterwards, tries to match the remaining tags with one of the existing ambiguity classes If it succeeds, this ambiguity class is assigned to the suffix Conversely, if it fails, the suffix will receive the default ambiguity class If we establish an open class including all nominal, adjectival, and verbal tags, the classes file will contain, along with this open class, the list of individual tags of the tagset, the default ambiguity class, several ambiguity classes formed by 2-tuples, 3-tuples, 4-tuples and a few 5-tuples and 6-tuples This means that computed suffixes must be accomodated into these latter ambiguity classes in order to maximize accuracy in the assignment of tags (the use of the default ambiguity class in these cases will produce incorrect results in most cases  Moreover, in inflectional languages, the selection of the training corpus is also crucial to the issue of suffix calculation The parameter to be considered in this respect is not the actual size of this lexicon (which, nevertheless, is important in order to accurately assign ambiguity classes to word tokens from a corpus but the set of ambiguity classes represented in that lexicon and this set would not increase with the addition of new words Some systems, like the Xerox Tagger, compute probabilistically both the suffixes and the ambiguity classes associated to them; but others, like the one described in HREF= 9505035 Hence, a new approach could include both manually-computed suffix tables and ambiguity classes, specially for inflectional languages where this information can be straightforwardly obtained, thus improving system accuracy The selection of the training corpus and the results obtained are also discussed