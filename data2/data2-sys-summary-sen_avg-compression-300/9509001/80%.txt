 The training corpus is Grolier's encyclopedia which contains on the order of 10 million words Each of these provides 100 training instances (every other word in the window so billion They cyclically apply this technique, adding disambiguated attachments into the training set, until all the training data (ambiguous or not) has been used The training set consists of 754,000 noun attachments and 468 thousand verb ones giving m = 122 million If the latter figure reflects the optimal error rate, it appears there is still room for improvement by adding training data or changing the statistical measures The training corpus consists of about 35,000 two-word compounds, giving m = 35,000 and million WordNet groups words into synsets, categories of synonymous words Their training corpus is an order of magnitude smaller than Hindle and Rooth's, so m is around 100,000 It is possible that insufficient training data is the cause of this shortfall The model formulated above and the empirical data presented support a number of qualitative inferences about the potential of systems given a fixed training set size Because training data will always be limited, such reasoning is an important part of system design Consider, for instance, the effect on data requirements of incorporating new indicators The situation is worse still if the training set is not hand annotated In this case, introducing the new indicator creates additional ambiguity in the training set since the value of the new indicator must be determined for each training example Thus, linguistic sophistication presents a trade-off between accuracy and data sparseness It is possible to acquire this understanding by computing statistics over a large corpus, a process called training Let , the training instances in a corpus c that fall into bin b Let , the frequency of the value v in the set of training instances, t This paper is concerned with the dependence of a system's accuracy on the size of the training corpus Either the corpus contains no occurrences of (b, v) for any value (Case A) or there is some training data which falls into the bin (Case B  Case A arises when none of the training instances fall into the bin In the following section, the notions of indicators, choices and training data will be made more formal A statistical NLP system deals with a certain linguistic universe Finally, almost all statistical NLP systems deal with some noise in the training data The mathematical results need to be extended to reflect noisy training data and to support reasoning about the sensitivity of data requirements to noise Formally, there is a set of linguistic events from which every training example and every test instance will be drawn Nonetheless, all these tasks share some important characteristics, not the least of which is the requirement for a sizable corpus of training data In the predictor, the set of bins is the set of words plus a start of sentence symbol Regardless of the learning algorithm used, each possible training corpus, c, results in the acquisition of some function, Pc One question which has largely been ignored is how much data is enough? For example, given a limited body of training data, it is essential to know which statistical NLP methods are likely to be accurate before pursuing any one Surprisingly, it is not always obvious how many training instances have been used to train a statistical method A system which collects word associations using a window of cooccurrence 10 words wide will find 819 instances in a 100 word corpus, while one collecting the objects of the preposition on from the same corpus, would most likely find only a few instances Therefore, before any conclusions can be drawn about data requirements, the training corpus must be measured in terms of instances The statistical processor will treat every instance in a bin identically Also, given a particular method, when will acquiring further training data cease to improve the system accuracy? Currently, the field is conspicuously lacking a general theory of data requirements for statistical NLP Often multiple indicators are used However, because there are more bins, there are fewer training instances in each bin Thus, statistical estimation will be less accurate In practice, high accuracy requires at least a few training instances per bin Thus increasing the number of indicators may actually decrease the overall accuracy Probabilistic analysers always select just one value for each bin, the one which maximises p I begin by formulating a framework for statistical NLP systems designed to capture some of the elements crucial to data requirements analysis Any probabilistic analyser which achieves an accuracy close to this is unlikely to benefit from further training data Unless large volumes of manually annotated data exist, measuring the size of in any given statistical processor presents a difficult challenge Of course, if there is insufficient training data the system may do considerably worse The training data is four million words of the University of Pennsylvania Treebank, tagged with a set of 47 different tags Since every word in the corpus (bar the first two) is used for training, we have m=4 million and The accuracy is reported to be around 97 which is approximately the accuracy of human taggers using the whole context