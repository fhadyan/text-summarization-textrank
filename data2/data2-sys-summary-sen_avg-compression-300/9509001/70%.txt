 Finally, I present some preliminary work on establishing a closed form upper bound on data requirements for a class of statistical NLP systems The training corpus is Grolier's encyclopedia which contains on the order of 10 million words Each of these provides 100 training instances (every other word in the window so billion The average accuracy reported is 92  They cyclically apply this technique, adding disambiguated attachments into the training set, until all the training data (ambiguous or not) has been used Thus B is a product of two indicator spaces: the set of verbs and nouns and the set of prepositions The training set consists of 754,000 noun attachments and 468 thousand verb ones giving m = 122 million The accuracy reported is close to 80 while human subjects given the same indications could achieve 85-88% accuracy If the latter figure reflects the optimal error rate, it appears there is still room for improvement by adding training data or changing the statistical measures Statistical NLP systems are designed to make choices; hopefully in an informed manner The training corpus consists of about 35,000 two-word compounds, giving m = 35,000 and million The accuracy reported is 75  Thus B is now a product of three spaces: the set of nouns and verbs, the set of prepositions and the set of nouns WordNet groups words into synsets, categories of synonymous words V is still binary Their training corpus is an order of magnitude smaller than Hindle and Rooth's, so m is around 100,000 It is possible that insufficient training data is the cause of this shortfall The model formulated above and the empirical data presented support a number of qualitative inferences about the potential of systems given a fixed training set size Because training data will always be limited, such reasoning is an important part of system design Consider, for instance, the effect on data requirements of incorporating new indicators The situation is worse still if the training set is not hand annotated In this case, introducing the new indicator creates additional ambiguity in the training set since the value of the new indicator must be determined for each training example This effectively decreases the number of training instances resulting in a further decrease in m:L Thus, linguistic sophistication presents a trade-off between accuracy and data sparseness If we are to strike a satisfactory compromise, we need a strong theory of data requirements and ways to make more economic use of data By collecting statistics based on concepts rather than individual words, the number of bins is usually reduced It is possible to acquire this understanding by computing statistics over a large corpus, a process called training In this section I shall establish some lower bounds on the accuracy of a simple training scheme within the framework developed Let , the training instances in a corpus c that fall into bin b Let , the frequency of the value v in the set of training instances, t This paper is concerned with the dependence of a system's accuracy on the size of the training corpus Either the corpus contains no occurrences of (b, v) for any value (Case A) or there is some training data which falls into the bin (Case B  Case A arises when none of the training instances fall into the bin Let pb denote In Case B, we have at least one instance in the corpus for the given bin One possible answer is that different bins have widely differing frequencies In the following section, the notions of indicators, choices and training data will be made more formal A statistical NLP system deals with a certain linguistic universe Finally, almost all statistical NLP systems deal with some noise in the training data This is especially important in systems like Yarowsky (1992) where training is unsupervised The mathematical results need to be extended to reflect noisy training data and to support reasoning about the sensitivity of data requirements to noise In this paper I have indicated the lack of a general theory of data requirements within the field of statistical NLP Formally, there is a set of linguistic events from which every training example and every test instance will be drawn In the next word predictor, this need only be the set of all pairs of words which may be adjacent in text In the predictor, this is the set of words plus an end of sentence symbol We also require a set of indicators, B, to use in selecting a value for a given linguistic event Nonetheless, all these tasks share some important characteristics, not the least of which is the requirement for a sizable corpus of training data In the predictor, the set of bins is the set of words plus a start of sentence symbol Therefore, it is a function The task of the learning algorithm is to acquire this function by computing statistics on the training set Regardless of the learning algorithm used, each possible training corpus, c, results in the acquisition of some function, Pc One question which has largely been ignored is how much data is enough? For example, given a limited body of training data, it is essential to know which statistical NLP methods are likely to be accurate before pursuing any one Surprisingly, it is not always obvious how many training instances have been used to train a statistical method A system which collects word associations using a window of cooccurrence 10 words wide will find 819 instances in a 100 word corpus, while one collecting the objects of the preposition on from the same corpus, would most likely find only a few instances Therefore, before any conclusions can be drawn about data requirements, the training corpus must be measured in terms of instances The statistical processor will treat every instance in a bin identically Further, once the bins are chosen, the greater the number of training instances that fall into a bin, the greater our confidence in the statistical inference made by the processor for test cases in that bin Also, given a particular method, when will acquiring further training data cease to improve the system accuracy? Currently, the field is conspicuously lacking a general theory of data requirements for statistical NLP It is not always obvious how many bins a given statistical method employs Often multiple indicators are used For instance, a trigram tagger uses the tags of the two preceding words and the current word to choose a new tag However, because there are more bins, there are fewer training instances in each bin Thus, statistical estimation will be less accurate In practice, high accuracy requires at least a few training instances per bin Thus increasing the number of indicators may actually decrease the overall accuracy For any non-trivial general statistical processor the indicators used cannot perfectly represent the entire linguistic event space Probabilistic analysers always select just one value for each bin, the one which maximises p I begin by formulating a framework for statistical NLP systems designed to capture some of the elements crucial to data requirements analysis Any probabilistic analyser which achieves an accuracy close to this is unlikely to benefit from further training data Unless large volumes of manually annotated data exist, measuring the size of in any given statistical processor presents a difficult challenge Of course, if there is insufficient training data the system may do considerably worse A worthwhile goal for future research is to establish a statistical method for estimating or bounding using language data Weischedel et al The training data is four million words of the University of Pennsylvania Treebank, tagged with a set of 47 different tags Since every word in the corpus (bar the first two) is used for training, we have m=4 million and The accuracy is reported to be around 97 which is approximately the accuracy of human taggers using the whole context