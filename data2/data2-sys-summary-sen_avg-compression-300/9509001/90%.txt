 The training corpus is Grolier's encyclopedia which contains on the order of 10 million words They cyclically apply this technique, adding disambiguated attachments into the training set, until all the training data (ambiguous or not) has been used22 million The training corpus consists of about 35,000 two-word compounds, giving m = 35,000 and million WordNet groups words into synsets, categories of synonymous words V is still binary It is possible that insufficient training data is the cause of this shortfall The model formulated above and the empirical data presented support a number of qualitative inferences about the potential of systems given a fixed training set size Because training data will always be limited, such reasoning is an important part of system design Consider, for instance, the effect on data requirements of incorporating new indicators In this case, introducing the new indicator creates additional ambiguity in the training set since the value of the new indicator must be determined for each training example Thus, linguistic sophistication presents a trade-off between accuracy and data sparseness Let , the training instances in a corpus c that fall into bin b Let , the frequency of the value v in the set of training instances, t Case A arises when none of the training instances fall into the bin A statistical NLP system deals with a certain linguistic universe Finally, almost all statistical NLP systems deal with some noise in the training data The mathematical results need to be extended to reflect noisy training data and to support reasoning about the sensitivity of data requirements to noise One question which has largely been ignored is how much data is enough? For example, given a limited body of training data, it is essential to know which statistical NLP methods are likely to be accurate before pursuing any one Surprisingly, it is not always obvious how many training instances have been used to train a statistical method Therefore, before any conclusions can be drawn about data requirements, the training corpus must be measured in terms of instances Also, given a particular method, when will acquiring further training data cease to improve the system accuracy? Currently, the field is conspicuously lacking a general theory of data requirements for statistical NLP In practice, high accuracy requires at least a few training instances per bin Any probabilistic analyser which achieves an accuracy close to this is unlikely to benefit from further training data Of course, if there is insufficient training data the system may do considerably worse The training data is four million words of the University of Pennsylvania Treebank, tagged with a set of 47 different tags