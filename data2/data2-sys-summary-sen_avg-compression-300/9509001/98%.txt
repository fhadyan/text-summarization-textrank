22 million V is still binary Finally, almost all statistical NLP systems deal with some noise in the training data Also, given a particular method, when will acquiring further training data cease to improve the system accuracy? Currently, the field is conspicuously lacking a general theory of data requirements for statistical NLP In practice, high accuracy requires at least a few training instances per bin The training data is four million words of the University of Pennsylvania Treebank, tagged with a set of 47 different tags