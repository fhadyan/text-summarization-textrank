 Word vectors reflecting word meanings are expected to enable numerical approaches to semantics A reference network of the words in a dictionary (Fig For the word sense disambiguation based on the context similarity, co-occurrence vectors from the 1987 Wall Street Journal (20M total words) was advantageous over distance vectors from the Collins English Dictionary ( head words + definition words  For learning or meanings from example words, distance vectors gave remarkably higher precision than co-occurrence vectors This suggests, though further investigation is required, that distance vectors contain some different semantic information from co-occurrence vectors ) is used to measure the distance between words The word dictionary is thus linked to the words book, word, language, and alphabetical The words in Fig In principle, origin words can be freely chosen In our experiments we used middle frequency words: the 51st to 1050th most frequent words in the reference Collins English Dictionary (CED  If word A is used in the definition of word B, these words are expected to be strongly related Dependence on Dictionaries As a semantic representation of words, distance vectors are expected to depend very weakly on the particular source dictionary Because a path through low-frequency words (rare words) implies a strong relation, it should be measured as a shorter path using co-occurrence statistics A co-occurence vector of a word is defined as the list of co-occurrence likelihood of the word with a certain set of origin words We used the same set of origin words as for the distance vectors Co-occurrence Vector Since then, there have been some promising results from using co-occurrence vectors, such as word sense disambiguation , and word clustering  The first is word sense disambiguation (WSD) based on the similarity of context vectors; the second is the learning of or meanings from example words With WSD, the precision by using co-occurrence vectors from a 20M words corpus was higher than by using distance vectors from the CED Word sense disambiguation is a serious semantic problem used simulated annealing for quick parallel disambiguation, and Yarowsky used co-occurrence statistics between words and thesaurus categories However, using the co-occurrence statistics requires a huge corpus that covers even most rare words In this method, a context vector is the sum of its constituent word vectors (except the target word itself  Figure (next page) shows the disambiguation precision for 9 words The results using distance vectors are shown by dots ( and using co-occurrence vectors from the 1987 WSJ (20M words) by circles (  A context size (x-axis) of, for example, 10 means 10 words before the target word and 10 words after the target word show that the precision by using co-occurrence vectors are higher than that by using distance vectors except two cases, interest and customs Therefore we conclude that co-occurrence vectors are advantageous over distance vectors to WSD based on the context similarity The sparseness problem for co-occurrence vectors is not serious in this case because each context consists of plural words The x-axis indicates the number of example words for each or pair In this case, the distance vectors were advantageous In the experiments discussed above, the corpus size for co-occurrence vectors was set to 20M words 87 WSJ) and the vector dimension for both co-occurrence and distance vectors was set to 1000 Corpus size (for co-occurrence vectors) Figure shows the change in disambiguation precision as the corpus size for co-occurrence statistics increases from 200 words to 20M words Therefore, a corpus size of 20M words is not too small Vector Dimension Figure (next page) shows the dependence of disambiguation precision on the vector dimension for (i) co-occurrence and (ii) distance vectors A comparison was made of co-occurrence vectors from large text corpora and of distance vectors from dictionary definitions