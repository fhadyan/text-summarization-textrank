 Word vectors reflecting word meanings are expected to enable numerical approaches to semantics A reference network of the words in a dictionary (Fig For the word sense disambiguation based on the context similarity, co-occurrence vectors from the 1987 Wall Street Journal (20M total words) was advantageous over distance vectors from the Collins English Dictionary ( head words + definition words  For learning or meanings from example words, distance vectors gave remarkably higher precision than co-occurrence vectors This suggests, though further investigation is required, that distance vectors contain some different semantic information from co-occurrence vectors ) is used to measure the distance between words The word dictionary is thus linked to the words book, word, language, and alphabetical The words in Fig In principle, origin words can be freely chosen If word A is used in the definition of word B, these words are expected to be strongly related using co-occurrence statistics A co-occurence vector of a word is defined as the list of co-occurrence likelihood of the word with a certain set of origin words We used the same set of origin words as for the distance vectors Co-occurrence Vector The first is word sense disambiguation (WSD) based on the similarity of context vectors; the second is the learning of or meanings from example words With WSD, the precision by using co-occurrence vectors from a 20M words corpus was higher than by using distance vectors from the CED Figure (next page) shows the disambiguation precision for 9 words The results using distance vectors are shown by dots ( and using co-occurrence vectors from the 1987 WSJ (20M words) by circles (  A context size (x-axis) of, for example, 10 means 10 words before the target word and 10 words after the target word show that the precision by using co-occurrence vectors are higher than that by using distance vectors except two cases, interest and customs Therefore we conclude that co-occurrence vectors are advantageous over distance vectors to WSD based on the context similarity In this case, the distance vectors were advantageous In the experiments discussed above, the corpus size for co-occurrence vectors was set to 20M words 87 WSJ) and the vector dimension for both co-occurrence and distance vectors was set to 1000 Corpus size (for co-occurrence vectors) Figure shows the change in disambiguation precision as the corpus size for co-occurrence statistics increases from 200 words to 20M words A comparison was made of co-occurrence vectors from large text corpora and of distance vectors from dictionary definitions